<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="reading">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Interesting research papers I have read (and my notes):">
<meta property="og:description" content="Interesting research papers I have read (and my notes):">
<link rel="canonical" href="https://samratsahoo.com/reading">
<meta property="og:url" content="https://samratsahoo.com/reading">
<meta property="og:site_name" content="samratâ€™s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-10-19T19:04:13+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="reading">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-10-19T19:04:13+00:00","datePublished":"2025-10-19T19:04:13+00:00","description":"Interesting research papers I have read (and my notes):","headline":"reading","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/reading"},"url":"https://samratsahoo.com/reading"}</script><title> reading - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="posts"><p><u><i>Interesting research papers I have read (and my notes):<u></u><i></i></i></u></p>
<p><br></p>
<ul>
<li>
<a href="/2025/10/17/hermes">HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation </a><time datetime="">10-17-2025</time>
</li>
<li>
<a href="/2025/10/14/resmimic">ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning </a><time datetime="">10-14-2025</time>
</li>
<li>
<a href="/2025/10/13/deep-mimic">DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills </a><time datetime="">10-13-2025</time>
</li>
<li>
<a href="/2025/10/03/symbolic-world-models">From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models </a><time datetime="">10-03-2025</time>
</li>
<li>
<a href="/2025/09/30/pdp">PDP: Physics-Based Character Animation via Diffusion Policy </a><time datetime="">09-30-2025</time>
</li>
<li>
<a href="/2025/09/07/open-x-embodiment">Open X-Embodiment: Robotic Learning Datasets and RT-X Models </a><time datetime="">09-07-2025</time>
</li>
<li>
<a href="/2025/08/28/pi-zero">$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control </a><time datetime="">08-28-2025</time>
</li>
<li>
<a href="/2025/06/29/iu-agent">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously </a><time datetime="">06-29-2025</time>
</li>
<li>
<a href="/2025/06/13/unreal">Reinforcement Learning with Unsupervised Auxiliary Tasks </a><time datetime="">06-13-2025</time>
</li>
<li>
<a href="/2025/06/12/uvfa">Universal Value Function Approximators </a><time datetime="">06-12-2025</time>
</li>
<li>
<a href="/2025/06/09/progressive-networks">Progressive Neural Networks </a><time datetime="">06-09-2025</time>
</li>
<li>
<a href="/2025/06/05/valor">Variational Option Discovery Algorithms </a><time datetime="">06-05-2025</time>
</li>
<li>
<a href="/2025/06/04/diayn">Diversity is All You Need: Learning Skills without a Reward Function </a><time datetime="">06-04-2025</time>
</li>
<li>
<a href="/2025/06/02/vic">Variational Intrinsic Control </a><time datetime="">06-02-2025</time>
</li>
<li>
<a href="/2025/06/01/rnd">Exploration by Random Network Distillation </a><time datetime="">06-01-2025</time>
</li>
<li>
<a href="/2025/05/31/icm">Curiosity-driven Exploration by Self-supervised Prediction </a><time datetime="">05-31-2025</time>
</li>
<li>
<a href="/2025/05/30/ex2">EX2: Exploration with Exemplar Models for Deep Reinforcement Learning </a><time datetime="">05-30-2025</time>
</li>
<li>
<a href="/2025/05/29/hash-based-counts">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning </a><time datetime="">05-29-2025</time>
</li>
<li>
<a href="/2025/05/28/pixelcnn-pseudocounts">Count-Based Exploration with Neural Density Models </a><time datetime="">05-28-2025</time>
</li>
<li>
<a href="/2025/05/26/recurrent-world-models">Recurrent World Models Facilitate Policy Evolution </a><time datetime="">05-26-2025</time>
</li>
<li>
<a href="/2025/05/23/cts-based-pseudocounts">Unifying Count-Based Exploration and Intrinsic Motivation </a><time datetime="">05-23-2025</time>
</li>
<li>
<a href="/2025/05/22/vime">VIME: Variational Information Maximizing Exploration </a><time datetime="">05-22-2025</time>
</li>
<li>
<a href="/2025/05/22/es">Evolution Strategies as a Scalable Alternative to Reinforcement Learning </a><time datetime="">05-22-2025</time>
</li>
<li>
<a href="/2025/05/21/ipg">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning </a><time datetime="">05-21-2025</time>
</li>
<li>
<a href="/2025/05/20/reactor">The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning </a><time datetime="">05-20-2025</time>
</li>
<li>
<a href="/2025/05/18/pgql">Combining Policy Gradient and Q-learning </a><time datetime="">05-18-2025</time>
</li>
<li>
<a href="/2025/05/15/trust-pcl">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control </a><time datetime="">05-15-2025</time>
</li>
<li>
<a href="/2025/05/12/decision-transformer">Decision Transformer: Reinforcement Learning via Sequence Modeling </a><time datetime="">05-12-2025</time>
</li>
<li>
<a href="/2025/05/11/pcl">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a><time datetime="">05-11-2025</time>
</li>
<li>
<a href="/2025/05/10/stein-control-variates">Action-dependent Control Variates for Policy Optimization via Steinâ€™s Identity</a><time datetime="">05-10-2025</time>
</li>
<li>
<a href="/2025/05/08/q-prop">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic </a><time datetime="">05-08-2025</time>
</li>
<li>
<a href="/2025/05/07/iqn">Implicit Quantile Networks for Distributional Reinforcement Learning</a><time datetime="">05-07-2025</time>
</li>
<li>
<a href="/2025/05/05/qr-dqn">Distributional Reinforcement Learning with Quantile Regression</a><time datetime="">05-05-2025</time>
</li>
<li>
<a href="/2025/04/27/c51">A Distributional Perspective on Reinforcement Learning</a><time datetime="">04-27-2025</time>
</li>
<li>
<a href="/2025/04/26/td3">Addressing Function Approximation Error in Actor-Critic Methods</a><time datetime="">04-26-2025</time>
</li>
<li>
<a href="/2025/04/26/ddpg">Continuous Control with Deep Reinforcement Learning</a><time datetime="">04-26-2025</time>
</li>
<li>
<a href="/2025/04/24/dpg">Deterministic Policy Gradient Algorithms</a><time datetime="">04-24-2025</time>
</li>
<li>
<a href="/2025/04/23/sac">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor </a><time datetime="">04-23-2025</time>
</li>
<li>
<a href="/2025/04/20/acer">Sample Efficient Actor-Critic with Experience Replay</a><time datetime="">04-20-2025</time>
</li>
<li>
<a href="/2025/04/18/acktr">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a><time datetime="">04-18-2025</time>
</li>
<li>
<a href="/2025/04/17/ppo">Proximal Policy Optimization Algorithms</a><time datetime="">04-17-2025</time>
</li>
<li>
<a href="/2025/04/17/emergence-of-locomotion-behaviors-in-rich-environments">Emergence of Locomotion Behaviours in Rich Environments</a><time datetime="">04-17-2025</time>
</li>
<li>
<a href="/2025/04/15/gae">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a><time datetime="">04-15-2025</time>
</li>
<li>
<a href="/2025/04/07/trpo">Trust Region Policy Optimization</a><time datetime="">04-07-2025</time>
</li>
<li>
<a href="/2025/04/06/asynchronous-deep-rl">Asynchronous Methods for Deep Reinforcement Learning</a><time datetime="">04-06-2025</time>
</li>
<li>
<a href="/2025/04/05/rainbow">Rainbow - Combining Improvements in Deep Reinforcement Learning</a><time datetime="">04-05-2025</time>
</li>
<li>
<a href="/2025/04/04/prioritized-experience-replay">Prioritized Experience Replay</a><time datetime="">04-04-2025</time>
</li>
<li>
<a href="/2025/04/03/double-dqn">Deep Reinforcement Learning with Double Q-learning</a><time datetime="">04-03-2025</time>
</li>
<li>
<a href="/2025/04/01/dueling-dqn">Dueling Network Architectures for Deep Reinforcement Learning</a><time datetime="">04-01-2025</time>
</li>
<li>
<a href="/2025/04/01/deep-recurrent-q-learning">Deep Recurrent Q-Learning for Partially Observable MDPs</a><time datetime="">04-01-2025</time>
</li>
<li>
<a href="/2024/11/04/playing-atari-with-deep-reinforcement-learning">Playing Atari With Deep Reinforcement Learning</a><time datetime="">11-04-2024</time>
</li>
<li>
<a href="/2024/09/05/extensibility-safety-and-performance-in-the-spin-operating-system">Extensibility, Safety, and Performance in the SPIN Operating System</a><time datetime="">09-05-2024</time>
</li>
<li>
<a href="/2024/08/31/on-microkernel-construction">On Micro-Kernel Construction</a><time datetime="">08-31-2024</time>
</li>
<li>
<a href="/2024/08/31/exokernel-an-operating-system-architecture-for-application-level-resource-management">Exokernel - An Operating System Architecture for Application-Level Resource Management</a><time datetime="">08-31-2024</time>
</li>
</ul></section></main></body>
</html>
