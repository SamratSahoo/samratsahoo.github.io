<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="lecture 8 - policy gradient I">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Policy Based Reinforcement Learning Previously, we approximated the value or action-value function using parameters ($\theta$) $V _{\theta}(s) \approx V^\pi(s)$ $Q _{\theta}(s, a) \approx Q^\pi(s, a)$ We then used these approximated functions to derive a policy Now we will directly parameterize the policy: $\pi _{\theta}(s, a) = \mathbb{P}[a \vert s;\theta]$ We want to find a policy with the highest value function, $V^\pi$">
<meta property="og:description" content="Policy Based Reinforcement Learning Previously, we approximated the value or action-value function using parameters ($\theta$) $V _{\theta}(s) \approx V^\pi(s)$ $Q _{\theta}(s, a) \approx Q^\pi(s, a)$ We then used these approximated functions to derive a policy Now we will directly parameterize the policy: $\pi _{\theta}(s, a) = \mathbb{P}[a \vert s;\theta]$ We want to find a policy with the highest value function, $V^\pi$">
<link rel="canonical" href="https://samratsahoo.com/brain/cs234/policy-gradient-I">
<meta property="og:url" content="https://samratsahoo.com/brain/cs234/policy-gradient-I">
<meta property="og:site_name" content="samratâ€™s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-06-15T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="lecture 8 - policy gradient I">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2024-06-15T00:00:00+00:00","datePublished":"2024-06-15T00:00:00+00:00","description":"Policy Based Reinforcement Learning Previously, we approximated the value or action-value function using parameters ($\\theta$) $V _{\\theta}(s) \\approx V^\\pi(s)$ $Q _{\\theta}(s, a) \\approx Q^\\pi(s, a)$ We then used these approximated functions to derive a policy Now we will directly parameterize the policy: $\\pi _{\\theta}(s, a) = \\mathbb{P}[a \\vert s;\\theta]$ We want to find a policy with the highest value function, $V^\\pi$","headline":"lecture 8 - policy gradient I","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/brain/cs234/policy-gradient-I"},"url":"https://samratsahoo.com/brain/cs234/policy-gradient-I"}</script><title> lecture 8 - policy gradient I - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>
<a href="/brain/cs234" class="url">cs234</a> / lecture 8 - policy gradient I</h2>
<h3 id="policy-based-reinforcement-learning">Policy Based Reinforcement Learning</h3>
<ul>
<li>Previously, we approximated the value or action-value function using parameters ($\theta$)<ul>
<li>$V _{\theta}(s) \approx V^\pi(s)$</li>
<li>$Q _{\theta}(s, a) \approx Q^\pi(s, a)$</li>
<li>We then used these approximated functions to derive a policy</li>
</ul>
</li>
<li>Now we will directly parameterize the policy: $\pi _{\theta}(s, a) = \mathbb{P}[a \vert s;\theta]$<ul><li>We want to find a policy with the highest value function, $V^\pi$</li></ul>
</li>
</ul>
<h3 id="value-based-vs-policy-based-rl">Value-Based vs Policy-Based RL</h3>
<ul>
<li>Value Based<ul>
<li>Learnt value function</li>
<li>Implicit policy (i.e., $\epsilon-\text{greedy}$)</li>
</ul>
</li>
<li>Policy Based<ul>
<li>No value function</li>
<li>Learnt policy</li>
</ul>
</li>
<li>Actor Critic<ul>
<li>Learnt value function</li>
<li>Learnt policy</li>
</ul>
</li>
</ul>
<h3 id="advantages--disadvantages-of-policy-based-rl">Advantages + Disadvantages of Policy Based RL</h3>
<ul>
<li>Advantages<ul>
<li>Better convergence properties</li>
<li>Effective in higher-dimensional or continuous action spaces</li>
<li>Can learn stochastic policies<ul>
<li>Useful where deterministic policies are exploitable (i.e., in Rock Paper Scissors)<ul><li>Value based RL leads to near-deterministic policies</li></ul>
</li>
<li>Also useful when partial aliasing occurs (differents states are indistinguishable) due to partial observability<ul><li>Allows for stochastic polcies in aliased states</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Disadvantages<ul>
<li>Converge to local rather than global optimum</li>
<li>Evaluating a policy is inefficient and high variance</li>
</ul>
</li>
</ul>
<h3 id="policy-objective-functions">Policy Objective Functions</h3>
<ul>
<li>Goal: given a policy $\pi (s,a)$ with parameters $\theta$, find the best $\theta$<ul><li>Maximize J (see below)</li></ul>
</li>
<li>Start value of a policy:<ul>
<li>Episodic: $J_1(\theta) = V^{\pi \theta}(s_1)$<ul><li>Find parameterized policy that results in highest value after H steps</li></ul>
</li>
<li>Continuing environments (infinite horizon): $J_ {avV}(\theta) = \sum_s d^{\pi\theta}(s)V^{\pi \theta}(s)$<ul>
<li>$d^{\pi\theta}$ is the stationary distribution of the Markov chain for $\pi\theta$</li>
<li>Calculates average value reached under a specific policy for infinite horizon cases</li>
</ul>
</li>
</ul>
</li>
<li>Average reward per time step: $J_ {avR}(\theta) = \sum_s d^{\pi\theta}(s) \sum_a \pi_\theta(s, a)R(s,a)$</li>
</ul>
<h3 id="policy-optimization">Policy Optimization</h3>
<ul>
<li>Find the policy paramters that maximize $V^{\pi\theta}$</li>
<li>Gradient Free Optimization Methods (for non-differentiable functions)<ul>
<li>Hill Climbing</li>
<li>Simplex / Amoeba / Nelder Mead</li>
<li>Genetic Algorithms</li>
<li>Cross Entropy Methods</li>
<li>Covariance Matrix Adaption</li>
<li>Benefits: Works with any policy parameterizations including non-differentiable</li>
<li>Limitation: Not very sample efficient because it ignores temporal structure</li>
</ul>
</li>
<li>Gradient Based Methods<ul>
<li>Methods<ul>
<li>Gradient Descent</li>
<li>Conjugate Gradient</li>
<li>Quasi-Newton</li>
</ul>
</li>
<li>Exploits the sequential structure of MDPs</li>
</ul>
</li>
</ul>
<h3 id="policy-gradient">Policy Gradient</h3>
<ul>
<li>Search for a local maximum in $V(\theta)$</li>
<li>$\Delta \theta = \alpha \nabla _\theta V(\theta)$<ul>
<li>Policy Gradient = $\delta _\theta V(\theta)$</li>
<li>The gradient is with respect to the parameters that define our policy (policy-based) instead of Q function (value-based)</li>
</ul>
</li>
</ul>
<h3 id="compute-gradients-by-finite-differences">Compute Gradients By Finite Differences</h3>
<ul>
<li>Evaluate Gradient of $\pi_\theta(s,a)$</li>
<li>For each dimension k in [1, n]<ul>
<li>Estimate the kth partial derivative of the objective function with respect to $\theta$</li>
<li>Do this by perturbing $\theta$ a small amount in the kth dimension<ul><li>$\frac{\delta V(\theta)}{\delta \theta_k} \approx \frac{V(\theta + \epsilon u_k) - V(\theta)}{\epsilon}$<ul><li>Where $u_k$ is a unit vector in the kth dimension (0 in all other dimensions)</li></ul>
</li></ul>
</li>
<li>Use n evaluations to compute policy gradient in n dimensions<ul>
<li>Sample, noisy, inefficient but sometimes effective</li>
<li>Works for nondifferentiable policies</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="compute-the-gradient-analytically">Compute the Gradient Analytically</h3>
<ul><li>Assume the policy is differentiable and we can compute the gradient ($\delta _\theta V(\theta)$)</li></ul>
<h3 id="likelihood-ratio-policies">Likelihood Ratio Policies</h3>
<ul>
<li>Denote a state-action trajectory as ($s_0, a_0, r_0 \dots s _{T-1}, a _{T-1}, r _{T-1}, s_T$) (reaches a terminal state)</li>
<li>Let $R(\tau) = \sum_0^TR(s_t, a_t)$ be the sum of rewards for trajectory $\tau$</li>
<li>Policy Value: $V(\theta) = \mathbf{E} _{\pi\theta}[\sum_0^T R(s_t, a_t); \pi _{\theta}] = \sum _{\tau} P(\tau; \theta)R(\tau)$<ul>
<li>For each trajectory sum the Probability of a trajectory multiplied by the reward of that trajectory</li>
<li>Goal: $argmax_\theta V(\theta) = argmax_\theta \sum _{\tau} P(\tau; \theta)R(\tau)$<ul>
<li>Take the gradient of this with respect to $\theta$: $\nabla_\theta V(\theta) = \nabla_\theta \sum _{\tau} P(\tau; \theta)R(\tau)$</li>
<li>$\nabla _\theta \sum _{\tau} P(\tau; \theta)R(\tau) = \sum _{\tau} \nabla _\theta P(\tau; \theta)R(\tau)$</li>
<li>$= \sum _{\tau} \nabla _\theta \frac{P(\tau; \theta)}{P(\tau; \theta)} P(\tau; \theta)R(\tau)$</li>
<li>$= \sum _{\tau} \frac{\nabla _\theta*P(\tau; \theta)}{P(\tau; \theta)} P(\tau; \theta)R(\tau)$<ul><li>Log Likelihood Ratio: $\frac{\nabla _\theta*P(\tau; \theta)}{P(\tau; \theta)}$</li></ul>
</li>
<li>$= \sum _{\tau} \nabla _\theta \log{P(\tau; \theta)} P(\tau; \theta)R(\tau)$</li>
</ul>
</li>
<li>Approximate the estimate for m sample paths under policy $\pi _\theta$<ul><li>$\nabla _\theta (\theta) \approx \hat{g} = \frac{1}{m} \sum_1^m R(\tau^i)\nabla _\theta \log{P(\tau^i; \theta)}$<ul><li>Intuition: moving in the direction of gradient $\hat{g}_i$ pushes the log probability of the sample in proportion to how good it is based on the reward function</li></ul>
</li></ul>
</li>
</ul>
</li>
</ul></section></main></body>
</html>
