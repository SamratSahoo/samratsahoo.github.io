<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about hermes">
<meta property="og:description" content="A paper about hermes">
<link rel="canonical" href="https://samratsahoo.com/2025/10/17/hermes">
<meta property="og:url" content="https://samratsahoo.com/2025/10/17/hermes">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-10-17T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-10-17T00:00:00+00:00","datePublished":"2025-10-17T00:00:00+00:00","description":"A paper about hermes","headline":"HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/10/17/hermes"},"url":"https://samratsahoo.com/2025/10/17/hermes"}</script><title> HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/2508.20085">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Humans continuously create bimanual manipulation data</li>
<li>Data targets robots with grippers, failing to generalize to dexterous hands</li>
<li>Interaction between robotic hands + manipulation objects usually omitted</li>
<li>Recent approaches use RL to learn motion strategies under guidance of reference trajectories<ul>
<li>Usually draw on limited human motion data</li>
<li>Oftentimes has not been transferred to real world</li>
<li>Current sim2real require full knowledge over object and robot state<ul><li>Fails to achieve end-to-end visual learning</li></ul>
</li>
</ul>
</li>
<li>HERMES: Embodied learning framework for bimanual dexterous hand manipulation<ul>
<li>Diverse sources of human motion<ul><li>Teleop, Mocap, Video</li></ul>
</li>
<li>End2end vision-based sim2real transfer<ul>
<li>Uses DAgger distillation to convert state-based expert policies into vision-based student policies</li>
<li>Introduce generalized object-centric depth augmentation + hybrid control</li>
</ul>
</li>
<li>Mobile manipulation<ul>
<li>Gives robots mobile manipulation skills</li>
<li>Uses RGB-D for localization</li>
<li>Task modeled as a Perspective-n-Point (PnP) problem addressed through iterative process</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>System Design</strong><ul>
<li>
<strong>Hardware Design</strong><ul>
<li>X1 mobile base, two 6-DoF Galaxea A1 arms, and two OYMotion 6-DoF dexterous hands</li>
<li>RealSense L515 to capture RGBD observations</li>
<li>RERVISION Fisheye camera for navigation</li>
</ul>
</li>
<li>
<strong>Simulation Design</strong><ul>
<li>Use MuJoCo + MJX</li>
<li>Actuation range of joints matches real robot</li>
<li>Use Mujoco’s closed chain mechanisms to model DoFs without motors (i.e., fingers)</li>
<li>Use equality constraint feature in MuJoCo</li>
<li>Approximate geometry using primitive shapes for collisions</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Reinforcement Learning Method</strong><ul>
<li>
<strong>Task Formulation</strong><ul>
<li>Standard RL MDP Formulation</li>
<li>Use reference trajectory as the goal, $\mathcal{G}$</li>
<li>State includes proprioception info ($s^p$) and goal state ($s^g$)</li>
<li>Reward is a function of both states</li>
</ul>
</li>
<li>
<strong>Collect One-shot Human Motion</strong><ul>
<li>3 sources of human motion<ul>
<li>Teleop in sim</li>
<li>MoCap</li>
<li>Hand object poses from raw video</li>
</ul>
</li>
<li>Can use single human reference to get generalizable policy without need for extensive demonstrations</li>
<li>Teleop<ul><li>Use apple vision pro to get hand poses + arm movements</li></ul>
</li>
<li>MoCap<ul>
<li>Retargeting data to robot hands is hard; use RL to learn behavior</li>
<li>Use the OakInk2 dataset</li>
</ul>
</li>
<li>Arm + Hand poses from video<ul>
<li>Use WiLoR to detect hands in video + extract 2D keypoints + corresponding 3D counterpart</li>
<li>Select stable keypoints for estimation<ul><li>Wrist + metacarpophalangeal joints</li></ul>
</li>
<li>Spatial translation of wrist is estimated by solving perspective-n-point problem<ul><li>Palm orientation derived by fitting 3d plane to selected 3d points</li></ul>
</li>
<li>Use FoundationPose to estimate object pose from video</li>
</ul>
</li>
<li>Synthesize Multiple Trajectories<ul>
<li>Trajectory augmentation by randomizing positions + orientations in predefined range<ul>
<li>$\hat{A}^{pose}[\tau_k] = T^{trans} \cdot A^{pose}[\tau_k]$</li>
<li>Apply transformation matrix to alter pose</li>
</ul>
</li>
<li>Enables spatial generalization (avoids need for more demonstrations)</li>
</ul>
</li>
<li>Use Dexpilot for retargeting</li>
<li>Apply RL to refine + adapt robot behaviors</li>
</ul>
</li>
<li>
<strong>Generalizable Reward Design for Manipulation</strong><ul>
<li>Use one reward function that can be reused across tasks</li>
<li>Note: see paper for formulas for each reward</li>
<li>Object centric distance chain ($r _{chain}$): Use fingers and center of object’s collision mesh as keypoints to model spatial relationships between hand and object<ul><li>Compute number of contact points between mesh and fingers and if its above threshold, reward is activated</li></ul>
</li>
<li>Object trajectory tracking ($r _{obj}$): Align’s policy behavior with object’s trajectory</li>
<li>Power Penalty ($r_{penalty}$): Used to alleviate jittering actions</li>
</ul>
</li>
<li>
<strong>Residual Action Learning</strong><ul>
<li>Arm actions:<ul>
<li>Coarse action derived from human trajectory</li>
<li>Fine (residual) component, $\Delta _{a _{f}}$ from learned network</li>
</ul>
</li>
<li>Hand actions:<ul><li>Let network model entire action due to inaccuracies in retargeting</li></ul>
</li>
<li>Use early termination to avoid inefficient exploration</li>
<li>Disable object collisions in early stages of training</li>
</ul>
</li>
<li>
<strong>Reinforcement Learning Algorithm</strong><ul>
<li>Implement DrM algorithm<ul><li>Off-policy method that uses dormant ratio mechanism to enhance exploration</li></ul>
</li>
<li>Concurrently, also use PPO</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Sim-to-Real Transfer</strong><ul>
<li>Need to distill state policy into visual policy</li>
<li>
<strong>Leveraging Depth Image as Visual Input</strong><ul>
<li>Clip depth values beyond a threshold distance, $d$<ul><li>Missing depth values filled with max depth</li></ul>
</li>
<li>Emulate real-world edge noise + blur, add gaussian noise + blur to simulated depth images<ul><li>Mimic missing depth by setting 0.5% of pixel values to max depth</li></ul>
</li>
<li>Linearly blend simulation rendered depth with data depth map: $\hat{o} = \alpha o _{sim} + (1 - \alpha) o _{dataset}$<ul><li>Enriches diversity of depth noise distribution</li></ul>
</li>
</ul>
</li>
<li>
<strong>DAgger Distillation Training</strong><ul>
<li>SOTA expert policy acts as teacher for student visual policy</li>
<li>HERMES distills state into raw visual observations of entire visual scenario<ul><li>Avoids need for camera calibration</li></ul>
</li>
<li>Model Architecture<ul>
<li>Input = 140x140 pixels + stacked 3 frames</li>
<li>Passed into image encoder<ul><li>First 2 layers used to capture fine-grained visual details</li></ul>
</li>
<li>Use GroupNorm instead of BatchNorm for distributional consistency</li>
</ul>
</li>
<li>Trajectory Rollout Scheduler<ul>
<li>Expert policy rolls out trajectories at beginning of DAgger</li>
<li>These rollouts gradually decrease throughout training by annealing the probability<ul><li>Also increases student’s participation in rollouts</li></ul>
</li>
<li>Exponentially decay the probability</li>
<li>Optimize student policy using L1 and L2 action loss terms</li>
<li>Add uniform noise into proprioception states for regularization</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Hybrid Sim2real Control</strong><ul>
<li>Real world visuals used to infer action which is then applied to sim environment</li>
<li>Updated joints positions from sim are transferred to real robot</li>
<li>Camera then captures real world image + incorporates proprioception states for next cycle</li>
<li>Sim2real discrepancy is mitigated because of shared inverse kinematics + dynamic parameters</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Navigation Methodology</strong><ul>
<li>
<strong>ViNT Navigation Foundation Model</strong><ul>
<li>HERMES uses image goal navigation foundation model</li>
<li>ViNT searches for goal observations in a topological map<ul><li>Computes sequence of relative waypoints based on current and goal observations</li></ul>
</li>
</ul>
</li>
<li>
<strong>Closed-loop PnP Localization</strong><ul>
<li>Discrepancy between final and target pose can lead manipulation policy to fail</li>
<li>ViNT doesn’t guarantee termination within a tight enough bound</li>
<li>Local refinement step after ViNT<ul><li>PnP algorithm adjusts robot pose</li></ul>
</li>
<li>Use neural feature matching module (Efficient LoFTR) to detect correspondence between captured and goal image</li>
<li>Features lifted to 3D space using intrinsics + depth map</li>
<li>Apply RANSAC PnP to compute relative rotation + translation and minimize reprojection error</li>
<li>Using realtime feedback from PnP, incrementally converge toward target pose + refine pose estimation</li>
<li>Use PID controller to adjust pose of robot<ul>
<li>Outputs planar velocity commands</li>
<li>Includes a sequential adjustment strategy which prioritizes error correction because of reorientation displacement on omnidirectional chassis</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>Goals<ul>
<li>Verify efficacy of HERMES</li>
<li>Exhibit effectiveness for sim2real</li>
<li>Quantify accuracy + reliability in navigation localization</li>
<li>Demonstrate effectiveness of HERMES in mobile manipulation</li>
</ul>
</li>
<li>
<strong>Sample Efficiency of HERMES</strong><ul>
<li>Regardless of human motion data, HERMES successfully converts actions to robot executable behaviors</li>
<li>HERMES has superior performance relative to ObjDex across all tasks with ObjDex formulation in framework<ul><li>Also achieves higher sample efficiency</li></ul>
</li>
</ul>
</li>
<li>
<strong>Comparison with Non-learning Approach</strong><ul>
<li>Kinematic retargeting fails to map capture object interactions + contact information</li>
<li>RL shapes policies to be human like while establising context-appropriate object interactions</li>
<li>Learning residual actions adaptively adjusts the movements + enhances execution success rates</li>
</ul>
</li>
<li>
<strong>Training Wall Time</strong><ul>
<li>HERMES benefits from reduced wall clock training time</li>
<li>HERMES also has better sample efficiency + stronger asymptotic performance under PPO</li>
</ul>
</li>
<li>
<strong>Real-world Manipulation Evaluation</strong><ul>
<li>Substantial noise in trajectory or transparent objects leads to jittering</li>
<li>Fine tune the policy with real-world trajectories</li>
<li>HERMES achieves 0-shot transfer diverse long-horizon + contact-rich bimanual dexterous manipulation tasks</li>
</ul>
</li>
<li>
<strong>The Effectiveness of Closed-loop PnP</strong><ul>
<li>ViNT by itself suffers from instability in localization accuracy</li>
<li>Proposed approach helps mitigate this</li>
<li>HERMES aligns both RGB images + point clouds with target position</li>
</ul>
</li>
<li>
<strong>The Localization Ability of Closed-loop PnP in the Texture-less Scenario</strong><ul><li>HERMES PnP refinement is robust in texture-less scenarios too</li></ul>
</li>
<li>
<strong>Mobile Manipulation Evaluation</strong><ul>
<li>Without closed-loop PnP, policy cannot generalize or complete the tasks when there are positional/rotational shifts</li>
<li>HERMES achieves notable improvement in manipulation success rate</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Limitations and Future Work</strong><ul>
<li>For highly dynamic velocity-dependent tasks, system identification still required for sim2real</li>
<li>Physics collision paramters manually tuned</li>
<li>Objects approximated with primitive shapes</li>
<li>Assembly + calibration between sim and hardware persist</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-10-17T00:00:00+00:00">October 17, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
