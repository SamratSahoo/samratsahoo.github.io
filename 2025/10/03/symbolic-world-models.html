<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about using VLMs to create symbolic world models">
<meta property="og:description" content="A paper about using VLMs to create symbolic world models">
<link rel="canonical" href="https://samratsahoo.com/2025/10/03/symbolic-world-models">
<meta property="og:url" content="https://samratsahoo.com/2025/10/03/symbolic-world-models">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-10-03T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-10-03T00:00:00+00:00","datePublished":"2025-10-03T00:00:00+00:00","description":"A paper about using VLMs to create symbolic world models","headline":"From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/10/03/symbolic-world-models"},"url":"https://samratsahoo.com/2025/10/03/symbolic-world-models"}</script><title> From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/2501.00296">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Model free imitation learning = doesn’t generalize well</li>
<li>Instead, from demonstrations, learn symbolic world models which includes properties and object relations (predicates)<ul><li>Grounded directly in low-level inputs (i.e., pixels)</li></ul>
</li>
<li>Captures task-agnostic world dynamics<ul><li>Enables cross-embodiment learning and generalizes across tasks</li></ul>
</li>
<li>Models represented in Planning Domain Definition Language (PDDL), enabling using PDDL planners</li>
<li>Compositional behavior is difficult for model free imitation<ul><li>With predicates, model can compose them into unseen tasks</li></ul>
</li>
<li>For training, assume no prior knowledge of predicates; number, structure, and meaning must be discovered<ul>
<li>Use VLMs to propose ground candidate predicates</li>
<li>Use VLMs to then ground and evaluate predicate based on images associated with state</li>
</ul>
</li>
<li>Raw set of predicates doesn’t generalize due to evaluation noise + overfitting world model<ul>
<li>Instead generate large pool of candidates and subselect predicates with efficient + effective planning</li>
<li>Use a program synthesis method that learns action models as an operator on predicates to search over pixel based predicates and be robust to VLM output noise<ul><li>Selects from synonymous predicates – can be labeled accurately by VLM and useful for downstream decision making</li></ul>
</li>
<li>pix2pred: Creates compact but semantically meaningful predicate set + symbolic world model</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Background and Problem Setting</strong><ul>
<li>
<strong>States, Actions, and Objects</strong><ul>
<li>State consists of multiple images, $s_t^{img}$ (potentially from multiple cameras) + an object centric state, $s_t^{obj}$ which is a set of vectors for each unique object in the images</li>
<li>Each object in the state $o \in \mathcal{O}$ consists of a name, optional type, and descriptor<ul><li>Descriptors = user provided phrases for an object<ul><li>Enable disambiguation</li></ul>
</li></ul>
</li>
<li>Action defined by a set of skills, $\mathcal{C}$ with each skill being a sequence of low level commands<ul>
<li>Each skill ($C((\lambda_1 \dots \lambda_v), \theta) \in \mathcal{C}$)has a semantically meaningful name, policy function, and optional discrete parameters ($\lambda_1 \dots \lambda_v$)</li>
<li>Each action is a skill with discrete + continuous arguments: $a = C((o_1 \dots o_v), \theta)$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Predicates and Symbolic World Models</strong><ul>
<li>Predicate characterized by 1) name, 2) ordered list of $m$ arguments $(\lambda_1 \dots \lambda_m)$ and classifier function $c_\psi: S \times A \rightarrow True/False$<ul>
<li>Ground atom ($\underline{\psi}$): Consists of predicate ($\psi$) + objects ($o_1 \dots o_m$)<ul><li>Example: Holding(spot, apple) = true if apple being held by the spot</li></ul>
</li>
<li>Feature-based predicates: Operate exclusively over object-centric state</li>
<li>Visual predicates: Operate on image based state</li>
</ul>
</li>
<li>Set of predicates ($\psi$) induce abstract state space ($S^\psi$)</li>
<li>For planning, we need action model, $\Omega$ that specifies abstract transition model $S^\psi$<ul>
<li>Action model learned through PDDL styled operators</li>
<li>For skills with continuous paramters, symbolic operator uses a generative sampler for paramters</li>
<li>Predicates + operators = world model!</li>
</ul>
</li>
<li>High Level Algorithm<ul>
<li>Start at initial state $s_0$</li>
<li>Evaluate all provided predicates ($\psi$) to get abstract state $ Use PDDL planner to achieve goal from abstract state</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Learning and Deployment</strong><ul>
<li>Learning Phase<ul>
<li>Learn from $n$ demonstrations $\mathcal{D}$ starting with initial predicates $\psi^{init}$</li>
<li>Each demonstration consists of objects $\mathcal{O}^d$, a k-step trajectory, and a goal expression $g^d$<ul><li>$g^d$ = conjunction of ground atoms with predicates from $\psi^{init}$ and objects from $\mathcal{O}^d$</li></ul>
</li>
</ul>
</li>
<li>Deployment Phase<ul>
<li>Given: set of new objects $\mathcal{O}^{test}$, novel initial state $s_0$, and novel goal $g^{test}$</li>
<li>Output: Set of $m$ actions (the plan) that achieves a state $s_m$ where $g^{test}$ holds</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Learning Symbolic World Models from Pixels</strong><ul>
<li>Given demonstrations, $\mathcal{D}$, we learn symbolic world model $\mathcal{W}^D$</li>
<li>Need to learn new predicates $\psi^{\mathcal{D}}$, operators for these predicates and initial predicates, and a set of generative samplers</li>
<li>
<strong>Proposing an Initial Pool of Visual Predicates</strong><ul>
<li>Prompt VLM on each demonstration<ul>
<li>Get image at each timestep</li>
<li>Add text heading to image</li>
<li>Pass images + actions executed + descriptors directly to VLM</li>
<li>Prompt VLM for set of proposals for ground atoms</li>
</ul>
</li>
<li>Parse ground atoms that are syntactically incorrect<ul><li>I.e., remove object names not in demonstration</li></ul>
</li>
<li>Create typed variables for predicates</li>
<li>Remove duplicate atoms; add only unique predicates to pool for final list of predicates</li>
</ul>
</li>
<li>
<strong>Implementing Visual Predicates with a VLM</strong><ul>
<li>Pass the ground atom (predicates with arguments replaced) into a VLM prompt and ask it evaluate it</li>
<li>Provide previous state too f state is not initial state</li>
</ul>
</li>
<li>
<strong>Learning Symbolic World Models via Optimization</strong><ul>
<li>Hill climbing procedure to select predicates</li>
<li>Generate initial pool of predicates</li>
<li>Combine visual predicates with feature based predicates</li>
<li>Perform operator learning<ul><li>Introduce regularization with early stopping via hyperparameter ($J _{thresh}$) + by deleting operators from operator learning procedure that model small set of transitions<ul><li>Prevents overfitting on noisy outlier data</li></ul>
</li></ul>
</li>
<li>Outputs subset of predicates</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>2 Questions<ul>
<li>How does pix2pred generalize to novel + complex goals compared to imitation approach that doesn’t use planner</li>
<li>How necessary is score-based optimization needed for subselection of predicates</li>
</ul>
</li>
<li>Domains: Kitchen, Burger, Coffee, Cleanup, Juice (See paper for details on each)</li>
<li>Approaches:<ul>
<li>Pix2pred: This method</li>
<li>VLM Subselect: VLM selects compact set of visual predicates</li>
<li>No Subselect: No hill climbing to select predicates; uses all predicates</li>
<li>No invent: no new predicates beyond $\psi^{init}$</li>
<li>No visual only feature based predicates</li>
<li>No feature: only visual based predicates</li>
<li>VLM feat. pred: VLM invents feature based predicates</li>
<li>ViLa: VLM plans without abstractions</li>
</ul>
</li>
<li>Experimental Setup:<ul>
<li>5 Domains</li>
<li>All demonstrations from POV of a human<ul><li>No object centric state available</li></ul>
</li>
<li>GPT-4o as VLM for training, gemini flash for real world</li>
</ul>
</li>
<li>Results and Analysis:<ul>
<li>Pix2pred outperforms other methods on 4/5 domains by wide margins</li>
<li>Generalizes better to complex tasks than ViLa</li>
<li>ViLa tends to pattern match and struggles on tasks requiring true generalization</li>
<li>Pix2pred outperforms ViLa on real world domains</li>
<li>Hill climbing improves test performance significantly</li>
<li>No subselect baseline fails entirely</li>
<li>In many domains, VLM proposes over 100 predicates; learning operators causes overfitting</li>
<li>Pix2pred fails due to noise in VLM labeling</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Limitations</strong><ul>
<li>Objects need to be specified with unambiguous descriptors<ul><li>Currently done manually and can be time consuming</li></ul>
</li>
<li>Assumes full observability of objects in all states</li>
<li>Hill climbing algorithm is slow</li>
<li>Hill climbing sensitive to hyperparamters</li>
<li>Approach assumes demonstrations segmented in terms of paramterized skills with names corresponding to function</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-10-03T00:00:00+00:00">October 3, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
