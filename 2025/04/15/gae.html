<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="High-Dimensional Continuous Control Using Generalized Advantage Estimation">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about better advantage estimation">
<meta property="og:description" content="A paper about better advantage estimation">
<link rel="canonical" href="https://samratsahoo.com/2025/04/15/gae">
<meta property="og:url" content="https://samratsahoo.com/2025/04/15/gae">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-15T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="High-Dimensional Continuous Control Using Generalized Advantage Estimation">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-04-15T00:00:00+00:00","datePublished":"2025-04-15T00:00:00+00:00","description":"A paper about better advantage estimation","headline":"High-Dimensional Continuous Control Using Generalized Advantage Estimation","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/04/15/gae"},"url":"https://samratsahoo.com/2025/04/15/gae"}</script><title> High-Dimensional Continuous Control Using Generalized Advantage Estimation - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>High-Dimensional Continuous Control Using Generalized Advantage Estimation</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1502.05477">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Credit Assignment Problem / Distal Reward Problem: Delay between actions and their positive/negative impact on rewards<ul><li>Value functions allow us to estimate goodness of action</li></ul>
</li>
<li>Using stochastic policy → noisy gradient estimates of expected total returns<ul>
<li>Variance scales unfavorably with more time due to compounding</li>
<li>Actor-critic algorithms use value function instead of raw returns → lower variance but higher bias<ul><li>Bias can cause algorithm to fail to converge or converge suboptimally</li></ul>
</li>
</ul>
</li>
<li>Generalized Advantage Estimation (GAE): policy gradient estimator that reduces variance while maintaining tolerable bias. Parameterized by $\lambda \in [0,1], \gamma \in [0,1]$</li>
</ul>
</li>
<li>
<strong>Preliminaries</strong><ul>
<li>Use undiscounted formulation of policy optimization (discount factor absorbed into reward function)</li>
<li>Policy gradient: $g = \mathbb{E}[\sum_{t=0}^\infty\psi \nabla_\theta \log \pi_\theta (a_t\vert s_t)]$<ul><li>$\psi$ can be total reward of trajectory, reward following action, baselined reward, state-action value function, advantage function, or a TD residual<ul><li>Advantage usually results in lowest variance but usually needs to be estimated<ul><li>Intuitively makes sense because policy gradient increases probability of better than average actions decreases probability of worse than average actions → advantage inherently does this!</li></ul>
</li></ul>
</li></ul>
</li>
<li>$\gamma$ parameter for downweighting rewards corresponding to delayed effects (less variance but more bias)<ul><li>Equivalent to discount factor usually used</li></ul>
</li>
<li>Discounted approximation to policy gradient using advantages: $g^\gamma = \mathbb{E}[\sum_{t=0}^\infty A^{\pi, \gamma}(s_t, a_t) \nabla_\theta \log \pi_\theta (a_t\vert s_t)]$</li>
<li>$\gamma-just$ estimator is one where: $g^\gamma = \mathbb{E}[\hat{A}(s_{0:\infty}, a_{0:\infty}) \nabla_\theta \log \pi_\theta (a_t\vert s_t)] = \mathbb{E}[A^{\pi, \gamma}(s_{t}, a_{t}) \nabla_\theta \log \pi_\theta (a_t\vert s_t)]$<ul>
<li>If it is $\gamma -just$ for all $t$ then<ul><li>$\mathbf{E}[\sum_{t=0}^\infty \hat{A}(s_t, a_t) \nabla_\theta \log \pi_\theta (a_t\vert s_t)]= g^\gamma$</li></ul>
</li>
<li>Can be $\gamma-just$ if $\hat{A} = Q_t(s_{t:\infty}, a_{t:\infty}) - b_t(s_{0:t}, a_{0:t-1})$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Advantage Function Estimation</strong><ul>
<li>We want to create an accurate estimate of $\hat{A_t}$ which we can use for estimating policy gradients</li>
<li>We can have 1-step, 2-step,…, k-step estimates of advantages:<ul><li>$\hat{A}_t^{(k)} = \sum_{l=0}^{k-1}\gamma^l\delta_{t+1}^V = -V(s_t) + r_t + \gamma r_{t+1} + \dots + \gamma^{k-1}r_{t+k-1} + \gamma^kV(s_{t+k})$<ul><li>Bias becomes smaller as $k \rightarrow \infty$ (this is empirical returns minus value function baseline)</li></ul>
</li></ul>
</li>
<li>GAE: Exponentially weighted average of k-step estimators; uses two parameters, $\lambda, \gamma$<ul>
<li>$\hat{A}_t^{GAE(\gamma, \lambda)} = (1-\lambda)(\hat{A}^{(1)}_t + \lambda\hat{A}^{(2)}_t + \lambda^2 \hat{A}^{(3)}_t + \dots) = \sum_{l=0}^\infty (\gamma\lambda)^l\delta^V_{t+l}$</li>
<li>$\lambda = 1$: High variance term because it is sum of terms but is $\gamma-just$ regardless of $V$.</li>
<li>$\lambda = 0$: Lower variance and is $\gamma-just$ for $V = V^{\pi, \gamma}$; induces bias otherwise</li>
<li>$0 \lt \lambda \lt 1$ makes compromise between bias and variance</li>
<li>$\gamma$ determines scale of value function → if $\gamma \lt 1$ then policy gradient estimate is biased regardless of function accuracy</li>
<li>$\lambda \lt 1$ only introduces bias if value function is inaccurate</li>
<li>Generally best value for $\lambda$ is lower than best value for $\gamma$</li>
<li>Biased estimator of $g^\gamma$:<ul><li>$g^\gamma \approx \mathbb{E}[\sum_{t=0}^\infty \nabla_\theta \log \pi_\theta (a_t \vert s_t) \hat{A}_t^{GAE(\gamma, \lambda)}] = \mathbb{E}[\sum_{t=0}^\infty \nabla_\theta \log \pi_\theta (a_t \vert s_t) \sum_{l=0}^\infty (\gamma\lambda)^l \delta_{t+1}^V]$</li></ul>
</li>
</ul>
</li>
<li>
<strong>Interpretation as Reward Shaping</strong><ul>
<li>Reward shaping is transforming a reward function of an MDP<ul>
<li>$\tilde{r}(s,a,s’) = r(s,a,s) + \gamma\Phi(s’) -\Phi(s)$</li>
<li>Same as defining a transformed MDP with new rewards</li>
<li>Transformed value and advantage functions<ul>
<li>$\tilde{Q}^{\pi, \gamma}(s,a) = Q^{\pi, \gamma}(s,a) - \Phi(s)$</li>
<li>$\tilde{V}^{\pi, \gamma}(s,a) = V^{\pi, \gamma}(s,a) - \Phi(s)$</li>
<li>$\tilde{A}^{\pi, \gamma}(s,a) = (Q^{\pi, \gamma}(s,a) - \Phi(s)) - (V^{\pi, \gamma}(s,a) - \Phi(s)) = A^{\pi, \gamma}(s,a)$</li>
</ul>
</li>
</ul>
</li>
<li>Reward shaping leaves policy gradient + optimal policy unchanged when we want to maximize the sum of discounted rewards</li>
<li>When we set $\Phi = V$, we get:<ul>
<li>$\sum_{l=0}^\infty (\gamma\lambda)^l\tilde{r}(s_{t+l}, a_t, s_{t+l+1}) = \sum_{l = 0}^\infty (\gamma\lambda)^l \delta_{t+1}^V = \hat{A}_t^{GAE(\gamma, \lambda)}$</li>
<li>GAE is $\gamma\lambda$ discounted sum of shaped rewards</li>
</ul>
</li>
<li>Response Function<ul>
<li>$\chi(l;s_t, a_t) = \mathbb{E}[r_{t+l} \vert s_t, a_t] - \mathbb{E}[r_{t+l} \vert s_t]$</li>
<li>$A^{\pi,\gamma}(s,a) = \sum_{l=0}^\infty \gamma^l \chi(l;s,a)$; response function decomposes advantage across timesteps</li>
<li>Allows us to quantify temporal assignment (long range dependencies)</li>
<li>Discounted policy gradient estimator: $\nabla_{\theta}\log \pi_{\theta}(a_t \vert s_t)A^{\pi, \gamma}(s_t, a_t) = \nabla_{\theta}\log \pi_\theta(a_t \vert s_t) \sum_{l = 0}^\infty \gamma^l \chi(l;s_t, a_t)$<ul><li>$\gamma &lt; 1$ means you drop terms with $l » \frac{1}{1-\gamma}$</li></ul>
</li>
<li>If the reward function is obtained using $\phi = V^{\pi, \gamma}$, $\mathbb{E}[\tilde{r}_{t+l} \vert s_t, a_t] =\mathbb{E}[\tilde{r} _{t+l}\vert s_t]$: A temporally extended response turns into an immediate response because the value function reduces temporal spread (this is done through reward shaping).<ul><li>Helps gradient focus on near-term outcomes</li></ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Value Function Estimation</strong><ul>
<li>To estimate value functions, you can use monte-carlo returns and solve linear regression on this: $minimize_\phi \sum_{n=1}^N\vert\vert V_\phi (s_n) - \hat{V_n} \vert\vert^2$</li>
<li>Use trust region to avoid overfitting on batch of data (constraint $\frac{1}{N}\sum_{n=1}^N \frac{\vert\vert V_\phi (s_n) - V_{\phi_{old}} \vert\vert^2}{2 \sigma^2} \leq \epsilon$)<ul>
<li>Similar to TRPO KL Divergence Constraint</li>
<li>Use approximate solution with conjugate gradients</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Policy Optimization Algorithm</strong><ul>
<li>For experiements, they use GAE with TRPO</li>
<li>Vary the $\gamma, \lambda$ parameters to see effects</li>
<li>Use value function for advantage estimation</li>
</ul>
</li>
<li>
<strong>Experimental Setup</strong><ul>
<li>
<strong>Architecture</strong><ul>
<li>3 Hidden Layers with tanh activations</li>
<li>Same policy + value function architecture</li>
<li>Final layer = linear</li>
</ul>
</li>
<li>
<strong>Task Details</strong><ul><li>See cartpole, mujoco Bipedal locomotion, quadrapedal locomotion, and dynamically standing up bipedal</li></ul>
</li>
</ul>
</li>
<li>
<strong>Experimental Results</strong><ul>
<li>
<strong>Cart-Pole</strong><ul><li>Fixed $\gamma$ with $\lambda = [0, 1]$. Best $\lambda = [0.92, 0.98]$ for fastest policy improvement</li></ul>
</li>
<li>
<strong>3D Bipedal Locomotion</strong><ul><li>Best $\gamma = [0.99, 0.995]$ with $\lambda = [0.96, 0.99]$</li></ul>
</li>
<li>
<strong>3D Robot Tasks</strong><ul>
<li>Quadrapedal Locomotion: Fixed $\gamma = 0.995$ with best $\lambda = 0.96$</li>
<li>3D standing: Fixed $\gamma = 0.995$ with best $\lambda = [0.96, 1]$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Discussion</strong><ul>
<li>Control problems difficult to solve because of high sample complexity</li>
<li>Reduce sample complexity by good estimates of advantages</li>
<li>Future work should look to tune $\gamma, \lambda$ automatically</li>
<li>If we know the relationship between the policy gradient estimation error and value function estimation error, we could choose a error function for value function that is well-matched</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-04-15T00:00:00+00:00">April 15, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
