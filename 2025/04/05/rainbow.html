<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Rainbow - Combining Improvements in Deep Reinforcement Learning">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about combining deep RL advancements into one">
<meta property="og:description" content="A paper about combining deep RL advancements into one">
<link rel="canonical" href="https://samratsahoo.com/2025/04/05/rainbow">
<meta property="og:url" content="https://samratsahoo.com/2025/04/05/rainbow">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-05T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Rainbow - Combining Improvements in Deep Reinforcement Learning">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-04-05T00:00:00+00:00","datePublished":"2025-04-05T00:00:00+00:00","description":"A paper about combining deep RL advancements into one","headline":"Rainbow - Combining Improvements in Deep Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/04/05/rainbow"},"url":"https://samratsahoo.com/2025/04/05/rainbow"}</script><title> Rainbow - Combining Improvements in Deep Reinforcement Learning - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Rainbow - Combining Improvements in Deep Reinforcement Learning</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1710.02298">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>DQN: Combination of CNNs + Experience Replay</li>
<li>DDQN: Addresses overestimation bias</li>
<li>PER: Improves data efficiency</li>
<li>Dueling Architecture: Generalizes across actions by separating value and action advantages</li>
<li>A3C: Multi-step bootstrap targets to shift bias-variance trade off + propagate newly observed rewards</li>
<li>Distributional Q-Learning: Learns categorical distribution of discounted returns instead of estimating mean</li>
<li>Noisy DQN: Stochastic network layers for exploration</li>
<li>Rainbow combines all aforementioned improvements</li>
</ul>
</li>
<li>
<strong>Background</strong><ul>
<li>MDP: Formalizes the interaction between agent and environment</li>
<li>Agent tries to maximize expected discounted reward</li>
<li>Estimate value function and use epsilon greedy policy for exploration</li>
<li>Deep RL + DQN<ul>
<li>Intractable state / action spaces require us to use neural nets and use gradient descent to minimize some loss</li>
<li>Use squared TD error for loss + gradients propagated backwards through online network (selects actions)</li>
<li>Target network has parameters copied over every so often → used to estimate current q-value → stability</li>
<li>Experience Replay → stability</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Extensions to DQN</strong><ul>
<li>Double Q Learning<ul><li>Action selection and evaluation decoupled to avoid maximization bias</li></ul>
</li>
<li>Prioritized Replay<ul><li>Samples transitions with a probability that is based on the magnitude of TD error</li></ul>
</li>
<li>Dueling Network<ul><li>Two streams of computation - one for advantages and one for state-value functions</li></ul>
</li>
<li>Multistep Learning<ul><li>Instead of just using immediate reward + discounted bootstrap, you can use n steps of rewards for target:<ul><li>$R_t^{(n)} = \sum_{k=0}^{n-1} = \gamma_t^{(k)}R_{t+k+1}$</li></ul>
</li></ul>
</li>
<li>Distributional RL<ul>
<li>You try to learn the distribution of returns instead of expected return<ul><li>Distribution: $d_t = (R_{t+1} + \gamma_{t+1}z, p_\theta(S_t, A_t))$</li></ul>
</li>
<li>Approximate a distribution and update parameters to match actual distribution as closely as possible</li>
<li>Learn probability masses - return distribution satisfy variant of bellman equation<ul>
<li>Take a distribution of returns, contract it by discount factor, and shift it by new distribution of rewards</li>
<li>Minimize KL divergence between target and current distribution</li>
</ul>
</li>
</ul>
</li>
<li>Noisy Nets<ul>
<li>Epsilon greedy policies limited in sparse reward settings</li>
<li>Noisy nets incorporate deterministic and noisy stream in linear layer</li>
<li>$y = (b + Wx) + ({noisy} \odot \epsilon^b + W_{noisy} \odot \epsilon^w)x$<ul><li>Epsilons are random variables</li></ul>
</li>
<li>Over time, network learns to ignore noise</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>The Integrated Agent</strong><ul>
<li>Using distributional RL KL divergence loss → makes it multi-step instead of 1 step<ul><li>New distribution: $d_t = (R_{t}^{(n)} + \gamma_{t}^{(n)}z, p_\theta(S_{t+n}, a^*_{t+n}))$</li></ul>
</li>
<li>Combine with double Q learning<ul>
<li>Online network chooses action at $S_{t+n}$</li>
<li>Target network evaluates the action</li>
</ul>
</li>
<li>Uses prioritized proportional replay based on DL loss → more robust to noisy stochastic environments</li>
<li>Dueling network architecture<ul>
<li>Shared representation fed into value and advantage stream</li>
<li>Streams aggregated and fed into softmax for return distribution</li>
<li>Replace all layers with noisy layers with Gaussian noise</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experimental Methods</strong><ul>
<li>Evaluation Methodology<ul>
<li>Tested on 57 atari games</li>
<li>Scores normalized and compared to human expert baselines</li>
<li>Test with random starts (insert 30 no-op actions)</li>
<li>Test with human starts (sample points from human expert data)</li>
</ul>
</li>
<li>Hyperparameter Tuning<ul>
<li>Number of hyperparameters too large for search</li>
<li>Perform limited tuning</li>
<li>DQN uses 200k learning starts to ensure no temporal correlations → with prioritized replay, we can learn after 80k</li>
<li>DQN uses annealing to decrease exploration rate from 1 to 0.1<ul>
<li>With noisy nets, we act greedily ($\epsilon = 0$) with value of 0.5 for standard deviation</li>
<li>Without noisy nets, use epsilon greedy but decrease $\epsilon$ faster</li>
</ul>
</li>
<li>Adam Optimizer</li>
<li>For prioritized replay, use proportional variant with importance sampling exponent increased from 0.4 to 1 over training</li>
<li>Multi-step number of steps was set to 3 (both 3 and 5 performed better than single step)</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Analysis</strong><ul>
<li>Rainbow is better than any of the baselines ( A3C, DQN, DDQN, Prioritized DDQN, Dueling DDQN, Distributional DQN, and Noisy DQN)<ul>
<li>Both in data efficiency + final performance</li>
<li>Match DQN in 7M frames (vs 44M)</li>
<li>153% human performance with human starts and 223% with no-ops</li>
</ul>
</li>
<li>Learning speed varied by 20% across all variants</li>
<li>Ablation Studies<ul>
<li>PER + multi-step learning were the most important parts<ul>
<li>Removing both hurt early performance</li>
<li>Removing multi-step hurt final performance</li>
</ul>
</li>
<li>Distributional Q learning 3rd most important<ul><li>Not much performance difference early on but lags later on in training</li></ul>
</li>
<li>Noisy nets also helped in most games → drop in performance in some but increase in performance in others</li>
<li>No significant difference when removing dueling networks</li>
<li>Double Q learning caused significant difference in median performance</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Discussion</strong><ul>
<li>Rainbow is based on value-based methods<ul><li>Have not considered policy based methods like TRPO or actor-critic methods</li></ul>
</li>
<li>Value Alternatives<ul>
<li>Optimality tightening: constructs additional inequality bounds instead of replacing 1-step targets in Q learning</li>
<li>Eligibility traces combine n-step returns across many n</li>
<li>Single step methods = more computation per gradient than n-step + need to determine how to store in PER</li>
<li>Episodic control = better data efficiency + improves learning using episodic memory as complementary system (can re-enact successful actions sequences)</li>
</ul>
</li>
<li>Other Exploration Schemes<ul>
<li>Bootstrapped DQN</li>
<li>Intrinsic Motivation</li>
<li>Count based exploration</li>
</ul>
</li>
<li>Computational Architecture<ul>
<li>Asynchronous learning from parallel environments (A3C)</li>
<li>Gorila</li>
<li>Evolution Strategies</li>
<li>Hierarchal RL</li>
<li>State Representation via pixel + feature control</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-04-05T00:00:00+00:00">April 5, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
