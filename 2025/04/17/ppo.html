<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Proximal Policy Optimization Algorithms">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about proximal policy optimization">
<meta property="og:description" content="A paper about proximal policy optimization">
<link rel="canonical" href="https://samratsahoo.com/2025/04/17/ppo">
<meta property="og:url" content="https://samratsahoo.com/2025/04/17/ppo">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-17T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Proximal Policy Optimization Algorithms">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-04-17T00:00:00+00:00","datePublished":"2025-04-17T00:00:00+00:00","description":"A paper about proximal policy optimization","headline":"Proximal Policy Optimization Algorithms","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/04/17/ppo"},"url":"https://samratsahoo.com/2025/04/17/ppo"}</script><title> Proximal Policy Optimization Algorithms - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Proximal Policy Optimization Algorithms</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1502.05477">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>We want RL methods that are 1) robust 2) data efficient and 3) scalable<ul>
<li>Q-Learning fails on simple problems + poorly understood</li>
<li>TRPO is complicated + incompatible with architectures with noise or parameter sharing</li>
</ul>
</li>
<li>PPO aims to get data efficiency + reliabiliyt of TRPO while only using 1st order optimization</li>
<li>Uses novel objective with clipped probability ratios</li>
<li>Alternates between using replay buffer data and on-policy data</li>
</ul>
</li>
<li>
<strong>Background: Policy Optimization</strong><ul>
<li>
<strong>Policy Gradient Methods</strong><ul>
<li>Estimates policy gradient and applies stochastic gradient ascent</li>
<li>Estimator: $\hat{g} = \hat{\mathbb{E}}_t [\nabla _\theta \log \pi _\theta (a_t \vert s_t)\hat{A}_t]$<ul><li>Expectation refers to average over a minibatch</li></ul>
</li>
<li>Loss Function: $L^{PG} = \hat{\mathbb{E}}_t [\log \pi _\theta (a_t \vert s_t)\hat{A}_t]$</li>
<li>Usually has large policy updates (destructive)</li>
</ul>
</li>
<li>
<strong>Trust Region Methods</strong><ul>
<li>Maximize surrogate objective: $maximize _\theta \hat{\mathbb{E}}_t [\frac{\pi _\theta (a_t \vert s_t)}{\pi _{\theta _{old}}(a_t \vert s_t)} \hat{A}_t]$</li>
<li>Subject to a KL divergence constraint</li>
<li>You could use a penalty for the KL divergence constraint and solve the unconstrained optimization problem but choosing a weight for the penalty is difficult<ul>
<li>$maximize_\theta \hat{\mathbb{E}}_t [\frac{\pi _\theta (a_t \vert s_t)}{\pi _{\theta _{old}}(a_t \vert s_t)} \hat{A}_t - \beta KL(\pi _{\theta _{old}}(\cdot \vert s_t), \pi _\theta(\cdot \vert s_t))]$: Hard to choose $\beta$</li>
<li>TRPO just makes it a hard constraint instead</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Clipped Surrogate Objective</strong><ul>
<li>Let $r_t(\theta) = \frac{\pi _\theta (a_t \vert s_t)}{\pi _{\theta _{old}}(a_t \vert s_t)}$</li>
<li>TRPO maximizes surrogate: $L^{CPI} = \hat{\mathbb{E}}_t [r_t(\theta) \hat{A}_t]$<ul><li>Without constraint, this would lead to large updates</li></ul>
</li>
<li>To avoid large updates: $L^{CLIP}(\theta) = \hat{\mathbb{E}}_t[min(r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon)\hat{A}_t)]$<ul>
<li>Clipping removes incentive for moving $r_t$ outside interval of $[1 - \epsilon, 1 + \epsilon]$</li>
<li>Minimum ensures we take a lower (pessimistic) bound on unclipped objective</li>
<li>$L^{CLIP}$ is lower bound for $L^{CPI}$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Adaptive KL Penalty Coefficient</strong><ul>
<li>We can use a KL-penalized objective</li>
<li>Empirically performs worse</li>
<li>Algorithm:<ul>
<li>Optimize the following on several minibatches using SGD: $L^{KLPEN}(\theta) = \hat{\mathbb{E}}_t [\frac{\pi _\theta (a_t \vert s_t)}{\pi _{\theta _{old}}(a_t \vert s_t)} \hat{A}_t - \beta KL(\pi _{\theta _{old}}(\cdot \vert s_t), \pi _\theta(\cdot \vert s_t))]$</li>
<li>Compute $d = \mathbb{E}_t[KL(\pi _{\theta _{old}}(\cdot \vert s_t), \pi _{\theta}(\cdot \vert s_t))]$<ul>
<li>If $d \lt d _{targ} / 1.5, \beta = \beta / 2$</li>
<li>If $d \gt d _{targ} \cdot 1.5, \beta = \beta \cdot 2$</li>
<li>1.5 and 2 are chosen heuristically but can be changed</li>
<li>We will see updates that diverge away from $d _{targ}$ but they are rare</li>
<li>Initial $\beta$ not important</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Algorithm</strong><ul>
<li>When using automatic differentiation, we replace $L^{PG}$ in policy gradient with $L^{CLIP}$ or $L^{KLPEN}$ and apply stochastic gradient ascent</li>
<li>If estimating a value function, we can combine our loss using a value function error term, and can add an entropy bonus: $L^{CLIP + VF + S} _t(\theta) = \hat{\mathbb{E} _t}[L^{CLIP} _t(\theta) - c_1 L^{VF}_t + c_2 S [\pi _{\theta}(s _t)]]$<ul><li>$S$ is entropy bonus and $L^{VF}$ is an MSE error</li></ul>
</li>
<li>To estimate advantages, we can use GAE or n-step returns</li>
<li>PPO Actor-Critic Style<ul><li>for iteration $1, 2 \dots$<ul>
<li>for actor $1, 2, \dots N$<ul>
<li>Run policy $\pi _{\theta _{old}}$ on environment</li>
<li>Compute advantages</li>
</ul>
</li>
<li>Optimize surrogate with respect to parameters with minibatch with K epochs</li>
<li>$\theta _{old} = \theta$</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Comparison of Surrogate Objectives</strong><ul>
<li>Compare no clipping, clipping, and KL penalty</li>
<li>Tried clipping in log-space but does not perform better</li>
<li>Clipping (with $\epsilon = 0.2$) produced the best results</li>
<li>No clipping or penalty produced the worst results</li>
</ul>
</li>
<li>
<strong>Comparison to Other Algorithms in the Continuous Domain</strong><ul>
<li>Compared with cross-entropy method, vanilla policy gradient with adaptive step size, A2C, A2C + Trust Region</li>
<li>PPO comes out with as good or the best performance on all 7 mujoco tasks</li>
</ul>
</li>
<li>
<strong>Showcase in the Continuous Domain: Humanoid Running and Steering</strong><ul><li>PPO produces a good policy here too (paper doesn’t say much)</li></ul>
</li>
<li>
<strong>Comparison to Other Algorithms on the Atari Domain</strong><ul>
<li>Compared with A2C and ACER</li>
<li>PPO trained faster than ACER and A2C</li>
<li>PPO’s final performance was a bit worse than ACER</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-04-17T00:00:00+00:00">April 17, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
