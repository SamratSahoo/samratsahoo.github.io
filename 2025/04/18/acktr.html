<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about a trust region optimization using Kronecker-factored approximation">
<meta property="og:description" content="A paper about a trust region optimization using Kronecker-factored approximation">
<link rel="canonical" href="https://samratsahoo.com/2025/04/18/acktr">
<meta property="og:url" content="https://samratsahoo.com/2025/04/18/acktr">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-18T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-04-18T00:00:00+00:00","datePublished":"2025-04-18T00:00:00+00:00","description":"A paper about a trust region optimization using Kronecker-factored approximation","headline":"Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/04/18/acktr"},"url":"https://samratsahoo.com/2025/04/18/acktr"}</script><title> Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1708.05144">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>SGD and first-order methods explore the weight space inefficiently<ul><li>Takes deep RL days to learn continuous control tasks</li></ul>
</li>
<li>Training time can be reduced with parallel environments but diminishing returns as parallelism increases</li>
<li>To improve sample efficiency, techniques like natural policy gradient follow steepest descent direction using Fisher metric as the underlying metric (looks at manifold)<ul>
<li>Intractable due to fisher matrix inversion</li>
<li>TRPO avoids inversion via Fisher-vector products + using many conjugate gradient iterations for update<ul>
<li>Requires large number of samples per batch to estimate curvature</li>
<li>Impractical for large models + sample inefficient</li>
</ul>
</li>
</ul>
</li>
<li>Kronecker Factored Approximation (K-FAC)<ul>
<li>Each update is comparable to 1 SGD update</li>
<li>Keeps track of running average of curvature; allows small minibatches<ul><li>Implies improves sample efficiency</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Background</strong><ul>
<li>
<strong>Reinforcement learning and actor-critic methods</strong><ul><li>Paper follows policy gradient method with advantage function with k-step returns which is learned through function approximation<ul><li>Performs temporal difference updates with MSE</li></ul>
</li></ul>
</li>
<li>
<strong>Natural gradient using Kronecker-factored approximation</strong><ul>
<li>Using gradient descent, we find $\delta \theta$ that minimizes $J(\theta + \delta \theta)$ such that $\vert\vert \delta \theta \vert\vert _B \lt 1$<ul>
<li>$\vert\vert x \vert\vert_B = (x^TBx)^{\frac{1}{2}}$ and B is a positive semidefinite matrix</li>
<li>Solution: $\delta \theta \propto -B^{-1}\nabla _\theta J$<ul>
<li>When norm is euclidean, $B = I$ (used in gradient descent)</li>
<li>Euclidean norm depends on parameterization<ul>
<li>Not favorable because parameterization is arbitrary + shouldn’t affect optimization trajectory</li>
<li>Natural gradients constructs norm based on fisher information matrix (approximation of KL)<ul>
<li>Independent of model parameterization; more stable updates</li>
<li>Impractical for modern neural nets with millions of parameters; need approximations</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Kronecker-factored approximation to Fisher matrix allows approximate + efficient natural gradient updates<ul>
<li>Let $p(Y \vert x)$ be output distribution</li>
<li>Let $L = \log p(y \vert x)$ be the log-likelihood</li>
<li>Let $W$ be the weight matrix of a layer with $C _{in}$ and $C _{out}$ as the number of input and output neurons</li>
<li>Let $a$ be the input activation vector to the layer</li>
<li>Let $s = Wa$ be the pre-activation vector for the next layer</li>
<li>Let $\nabla _{W} L = (\nabla _{s}L)a^T$ be the weight gradient</li>
<li>K-FAC Fisher Information Approximation: $F _l = \mathbb{E}[vec (\nabla _W L) vec (\nabla _W L)^T] = \mathbb{E}[aa^T \otimes \nabla_s L (\nabla_s L)^T]$<ul>
<li>$\approx \mathbb{E}[aa^T] \otimes \mathbb{E}[ \nabla_s L (\nabla_s L)^T] = A \otimes S$</li>
<li>Where $A =\mathbb{E}[aa^T]$ and $S = \mathbb{E}[ \nabla_s L (\nabla_s L)^T]$</li>
<li>Assumes 2nd order statistics of activations + gradients uncorrelated</li>
</ul>
</li>
<li>K-FAC is approximate natural gradient update<ul>
<li>$vec(\Delta W) = \hat{\mathbb{F _l}}^{-1} vec(\nabla _W J) = vec(A^{-1} \nabla _W JS^{-1})$</li>
<li>Only requires computations of matrix sizes of W</li>
</ul>
</li>
<li>Distributed K-FAC reaches 2 - 3x speed up</li>
<li>K-FAC has been extended to handle CNNs</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Methods</strong><ul>
<li>
<strong>Natural gradient in actor-critic</strong><ul><li>ACKTR: actor-critic using Kronecker-factored trust region<ul>
<li>Use kronecker factorization to compute natural gradient update and apply it to actor and critic</li>
<li>Fisher information matrix for policy:<ul>
<li>$F = \mathbb{E} _{p(\tau)}[(\nabla _{\theta} \log \pi(a_t \vert s_t))(\nabla _{\theta} \log \pi(a_t \vert s_t))^T]$</li>
<li>Where $p(\tau)$ is the trajectory distribution</li>
</ul>
</li>
<li>Fisher information matrix for critic:<ul>
<li>Define it to be a gaussian distribution $p(v \vert s_t) \sim \mathbb{N}(v; V(s_t), \sigma^2)$</li>
<li>Setting $\sigma = 1$, equivalent to vanilla gauss-newton method (extension of newton’s method).</li>
</ul>
</li>
<li>Usually beneficial to have architecture where lower level representation is shared but has distinct output layers<ul>
<li>Can define a joint distribution: $p(a,v \vert s) = \pi (a \vert s)p(v \vert s)$ and construct fisher information matrix with respect to this</li>
<li>Apply K-FAC for approximation and apply updates</li>
</ul>
</li>
</ul>
</li></ul>
</li>
<li>
<strong>Step-size Selection and trust-region optimization</strong><ul>
<li>Usually natural gradient uses SGD like updates: $\theta = \theta - \eta F^{-1}\nabla _{\theta}L$<ul><li>Can result in large updates, causing premature convergence to deterministic policy</li></ul>
</li>
<li>Trust Region Approach<ul>
<li>Update scaled down to modify policy distribution by at most a specified amount</li>
<li>Step size: $\eta = min(\eta _{max}, \sqrt{\frac{2 \delta}{\Delta \theta^T F \Delta \theta}})$</li>
<li>learning rate $\eta _{max}, trust region radius \delta$ are hyperparameters</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Discrete Control</strong><ul>
<li>ACKTR signficantly outperforms A2C and TRPO in terms of sample efficiency</li>
<li>ACKTR got 12 times the performance of humans</li>
<li>On par with Q learning methods for sample efficiency with less computation time</li>
</ul>
</li>
<li>
<strong>Continuous Control</strong><ul>
<li>ACKTR outperforms or is on par with A2C on most mujoco tasks</li>
<li>ACKTR achieves a specified threshold faster on all but 1 mujoco task where TRPO outperforms it</li>
<li>Learning from raw pixel data, ACKTR outperforms A2C in episodic reward</li>
</ul>
</li>
<li>
<strong>A better norm for critic optimization?</strong><ul>
<li>Regardless of which norm is used for critic, applying ACKTR for actor brings improvements compared to baseline</li>
<li>Using Gauss-Newton norm for critic is more substantial for sample efficiency and rewards than euclidean<ul>
<li>Also helps stabilize training (larger variance with euclidean norm)</li>
<li>Gauss-Newton sets $\sigma = 1$. Adaptive Gauss-Newton Using variance of bellman error instead<ul><li>Results in no signficant improvement</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>How does ACKTR compare with A2C in wall-clock time?</strong><ul><li>Only increases computing time by at most 25% relative to A2C</li></ul>
</li>
<li>
<strong>How do ACKTR and A2C perform with different batch sizes</strong><ul>
<li>ACKTR performed equally well with batch sizes of 160 and 640</li>
<li>With larger batch size, A2C experienced worse sample efficiency</li>
<li>Larger batch sizes work better with ACKTR than A2C</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-04-18T00:00:00+00:00">April 18, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
