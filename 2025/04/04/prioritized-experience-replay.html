<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Prioritized Experience Replay">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about prioritizing experience in replay buffers">
<meta property="og:description" content="A paper about prioritizing experience in replay buffers">
<link rel="canonical" href="https://samratsahoo.com/2025/04/04/prioritized-experience-replay">
<meta property="og:url" content="https://samratsahoo.com/2025/04/04/prioritized-experience-replay">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-04T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Prioritized Experience Replay">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-04-04T00:00:00+00:00","datePublished":"2025-04-04T00:00:00+00:00","description":"A paper about prioritizing experience in replay buffers","headline":"Prioritized Experience Replay","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/04/04/prioritized-experience-replay"},"url":"https://samratsahoo.com/2025/04/04/prioritized-experience-replay"}</script><title> Prioritized Experience Replay - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Prioritized Experience Replay</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1511.05952">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Without experience replay, we get temporally correlated updates<ul>
<li>Loses IID assumption</li>
<li>Forgetting potentially rare experiences is more useful</li>
</ul>
</li>
<li>With experience replay, we break temporal correlations<ul>
<li>Stabalizes value function training</li>
<li>Reduces amount of experience needed to learn</li>
</ul>
</li>
<li>By prioritizing which experiences are replayed, we can make experience replay even more efficient<ul><li>RL agent can learn from some experiences effectively than others<ul>
<li>Some transitions more more/less surprising or redundant</li>
<li>Some transitions not immediately relevant but relevant later on</li>
</ul>
</li></ul>
</li>
<li>Prioritized Experience Replay replays transitions with high expected learning progress → measured by magnitude of TD error<ul>
<li>Prioritization can lead to loss of diversity</li>
<li>Use stochastic prioritization + bias and correct with importance sampling for diversity</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Background</strong><ul>
<li>In neuroscience, experience replay is found in hippocampus of rodents<ul><li>Sequences with high TD error or rewards replayed more</li></ul>
</li>
<li>In value iteration, prioritizing updates makes planning more efficient<ul>
<li>Prioritized sweeping prioritizes updating state based on change in value of state</li>
<li>TD error also can be used</li>
<li>PER uses similar prioritization (TD error) but for model-free RL instead of model-based</li>
<li>Stochastic prioritization more robust when using function approximators</li>
</ul>
</li>
<li>TD errors tell us where we should focus exploration</li>
<li>When dealing with imbalanced datasets, we can resample<ul>
<li>Separate experience into two buckets (positive and negative rewards)</li>
<li>Choose fixed fraction from each bucket to replay</li>
<li>Only applicable for domains with positive and negative experiences</li>
</ul>
</li>
<li>DQN + DDQN are SOTA on atari</li>
</ul>
</li>
<li>
<strong>Prioritized Replay</strong><ul>
<li>PER can be implemented in two ways<ul>
<li>Which experiences to store</li>
<li>Which experiences to replay (paper does this)</li>
</ul>
</li>
<li>
<strong>Motivating Example</strong><ul><li>If we could experience replay with an oracle that tells us what order to use, we could get an optimal policy in far less updates</li></ul>
</li>
<li>
<strong>Prioritizing with TD-Error</strong><ul>
<li>TD error is a reasonable proxy for the amount the RL agent can learn from a transition<ul><li>Tells us how surprising a transition is by measuring how far value is from next step bootstrap estimate</li></ul>
</li>
<li>TD error prioritization algorithm<ul>
<li>Store TD error along with transition in memory</li>
<li>Play transition with highest TD error</li>
<li>New transitions without a known TD error are given maximum priority</li>
</ul>
</li>
<li>Implemented using a binary heap</li>
</ul>
</li>
<li>
<strong>Stochastic Prioritization</strong><ul>
<li>Issues with greedy prioritization<ul>
<li>TD errors only updated for replayed transitions<ul><li>First visit low TD errors may never be replayed for long periods of time</li></ul>
</li>
<li>Sensitive to noise spikes which is exacerbated with bootstrapping</li>
<li>Focuses on small subset of experience<ul><li>Errors shrink over time → initially high error transitions get replayed more frequently → prone to overfitting</li></ul>
</li>
</ul>
</li>
<li>Stochastic Sampling: interpolates between greedy and uniform sampling<ul><li>Probability of sampling a transition: $P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$<ul><li>$\alpha$ is a parameter which determines degree of prioritization ($\alpha = 0$) means using uniform sampling</li></ul>
</li></ul>
</li>
<li>Proportional Prioritization: $p_i = \vert\delta_i\vert + \epsilon$ where $\epsilon$ is a small positive constant to prevent 0 probability transitions<ul><li>Implemented with a sum-tree data structure</li></ul>
</li>
<li>Rank based prioritization: $p_i = \frac{1}{rank(i)}$ where $rank(i)$ is rank of transition $i$ when sorted by $\delta_i$<ul>
<li>P becomes a power law distribution</li>
<li>Insensitive to outliers</li>
<li>Implemented with an approximate CDF with piecewise linear function with k segments of equal probability<ul>
<li>Sample a segment and then sample transitions along it</li>
<li>Choose k to be size of minibatch (one transition / segment)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Annealing the Bias</strong><ul>
<li>Estimation of expected value with stochastic updates requires update distribution = expectation distribution</li>
<li>PER introduces bias because it changes distribution in uncontrolled fashion<ul>
<li>Changes solution estimates converge to</li>
<li>Since we are sampling based on importance, our updates to Q networks are not following the transitions they generated and are instead from a different distribution<ul><li>Use importance sampling to adjust how much we change weights by</li></ul>
</li>
</ul>
</li>
<li>Use weighted importance sampling:<ul>
<li>$w_i = (\frac{1}{N} \cdot \frac{1}{P(i)})^\beta$</li>
<li>Compensates for non-uniform probabilities if $\beta=1$</li>
<li>Weights can be folded into Q-learning update</li>
<li>Normalize weights by $1/max_i w_i$ for stability</li>
</ul>
</li>
<li>In RL, unbiased updates important near convergence (process tends to be very non-stationary anyways)<ul>
<li>Bias can generally be ignored</li>
<li>Annealing level of importance sampling over time such that $\beta$ reaches 1 near end → i.e., linear annealing</li>
</ul>
</li>
<li>Large steps disruptive because first order approximation only accurate locally → prevent with small step size<ul><li>With PER, high error transitions seen many times → importance sampling reduces gradient magnitudes + taylor expansion constantly re-approximated</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Atari Experiments</strong><ul>
<li>PER leads to substantial score improvements on majority of games</li>
<li>Aggregate learning is about twice as fast</li>
<li>PER is complementary to DDQN → increases performance another notch</li>
</ul>
</li>
<li>
<strong>Discussion</strong><ul>
<li>Rank based is more robust because not impacted by outliers nor error magnitudes<ul>
<li>Heavy-tail = diverse samples</li>
<li>Stratified sampling = stable gradient magnitude</li>
</ul>
</li>
<li>Rank-based blind to error scales = performance drop when there is structure in the error distribution<ul><li>Clipped rewards + TD errors in DQN = similar empirical performance</li></ul>
</li>
<li>With regular experience replay some experience slides out of window before ever replayed<ul><li>With PER, the bonus given to unseen transitions ensures this doesn’t happen</li></ul>
</li>
<li>When using PER with representations of transitions, representation with good transitions replayed much less and learning focuses on where representation is poor → more resources into distinguishing aliased states</li>
</ul>
</li>
<li>
<strong>Extensions</strong><ul>
<li>Prioritized Supervised Learning: We can use PER to replay samples where we can still learn much</li>
<li>Off policy Replay: PER is analogous to using off policy RL with rejection sampling or importance sampling</li>
<li>Feedback for Exploration: The number of times a transition is replayed is feedback for how useful it was to an agent → signal can be fed back for exploration<ul>
<li>Sample exploration hyperparameters from parameterized distribution</li>
<li>Monitor usefulness of experience</li>
<li>Update distribution towards generating more useful experience for exploration</li>
</ul>
</li>
<li>Prioritized Memories<ul>
<li>Controlling which memories to store and erase reduce redundancy</li>
<li>When erasing, we need to have stronger considerations of diversity (i.e., age of memory) to preserve old memories and prevent cycles</li>
<li>Also flexible enough to incorporate experience from external sources</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-04-04T00:00:00+00:00">April 4, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
