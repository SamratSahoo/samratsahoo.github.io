<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Deterministic Policy Gradient Algorithms">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about the deterministic policy gradient algorithm">
<meta property="og:description" content="A paper about the deterministic policy gradient algorithm">
<link rel="canonical" href="https://samratsahoo.com/2025/04/24/dpg">
<meta property="og:url" content="https://samratsahoo.com/2025/04/24/dpg">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-24T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Deterministic Policy Gradient Algorithms">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-04-24T00:00:00+00:00","datePublished":"2025-04-24T00:00:00+00:00","description":"A paper about the deterministic policy gradient algorithm","headline":"Deterministic Policy Gradient Algorithms","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/04/24/dpg"},"url":"https://samratsahoo.com/2025/04/24/dpg"}</script><title> Deterministic Policy Gradient Algorithms - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Deterministic Policy Gradient Algorithms</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://proceedings.mlr.press/v32/silver14.pdf">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Policy gradient algorithms adjust parameters in direction of greater cumulative reward<ul><li>Stochastically sample from policy</li></ul>
</li>
<li>We want deterministic policies using the same approach as policy gradient<ul><li>This is just the stochastic case as policy variance tends to 0!</li></ul>
</li>
<li>Stochastic policy gradient = integrate over state and action space (requires more samples)</li>
<li>Deterministic policy gradient = integrate over state space</li>
<li>Stochasticity enables exploration in stochastic policy gradients<ul><li>For deterministic, use off-policy learning based on a stochastic behavior policy</li></ul>
</li>
<li>Introduce compatible function approximation: ensures approximation doesn’t bias policy gradient</li>
</ul>
</li>
<li>
<strong>Background</strong><ul>
<li>
<strong>Preliminaries</strong><ul><li>Standard MDP Setting<ul><li>Objective is to maximize expected rewards</li></ul>
</li></ul>
</li>
<li>
<strong>Stochastic Policy Gradient Theorem</strong><ul><li>Policy gradient theorem: $\begin{multline}\nabla _\theta J(\pi _\theta) = \int_S \rho^\pi (s) \int_A \nabla _\theta \pi _\theta(a \vert s) Q^\pi(s,a) da ds = \mathbb{E} _{s \sim \rho^\pi, a \sim \pi _{\theta}}[\nabla _\theta \log \pi(a \vert s) Q^\pi(s,a)] \end{multline}$<ul><li>Can use sample returns to estimate Q value function</li></ul>
</li></ul>
</li>
<li>
<strong>Stochastic Actor-Critic Algorithms</strong><ul><li>Two components:<ul>
<li>Actor adjusts parameter of a stochastic policy, $\pi _\theta(s)$</li>
<li>Critic estimates action value function, $Q^w(s,a)$ via temporal difference learning<ul>
<li>Introduces bias when critic is not empirical returns</li>
<li>Compatability Requirements; ensures no bias:<ul>
<li>$Q^w(s,a) = \nabla _\theta \log \pi _\theta(a\vert s)^Tw$: linear in features of stochastic policy</li>
<li>Parameters chosen to minimize MSE: linear regression estimates Q from these features</li>
</ul>
</li>
<li>If compatability is achieved, equivalent to using no critic</li>
</ul>
</li>
</ul>
</li></ul>
</li>
<li>
<strong>Off-Policy Actor-Critic</strong><ul><li>Objective modified to be value function of target policy averaged over state distribution of behavior policy<ul>
<li>$J _\beta (\pi _\theta) = \int_S \int_A \rho^\beta(s) \pi _\theta(a\vert s) Q^\pi(s,a)dads$</li>
<li>Off policy actor critic gradient: $\nabla _\theta J _\beta (\pi _\theta) = \int_S \int_A \rho^\beta(s) \nabla _\theta \pi _\theta(a\vert s) Q^\pi(s,a)dads$<ul><li>$= \mathbb{E} _{s \sim \rho^\beta, a \sim \beta}[\frac{\pi _\theta(a \vert s)}{\beta _\theta(a \vert s)} \nabla _\theta \log \pi _\theta (a \vert s)Q^\pi(s,a)]$</li></ul>
</li>
<li>Instead of Q value function, we can use temporal difference error for objective</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Gradients of Deterministic Policies</strong><ul>
<li>
<strong>Action-Value Gradients</strong><ul>
<li>Policy evaluation methods estimate action-value function via TD learning or monte carlo evaluation</li>
<li>Policy improvement methods update policy via greedy maximization wrt the action value function<ul><li>In continuous spaces, greedy maximization is problematic; instead move policy in direction of Q<ul>
<li>Each state suggests different direction; take the expectation over the state distribution: $\theta^{k+1} = \theta^{k} + \alpha \mathbb{E} _{s \sim \rho^{\mu^k}}[\nabla _\theta Q^{\mu^k}(s,\mu _\theta(s))]$</li>
<li>Can decompose policy gradients (via chain rule) into gradients of action values and gradients of policy:<ul><li>$\theta^{k+1} = \theta^{k} + \alpha \mathbb{E} _{s \sim \rho^{\mu^k}}[\nabla _\theta \mu _\theta(s) \nabla_a Q^{\mu^k}(s,a)]$</li></ul>
</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Deterministic Policy Gradient Theorem</strong><ul>
<li>Performance objective for a deterministic policy $\mu: S \rightarrow A$<ul><li>$J(\mu _\theta) = \int_S \rho^\mu(s) r(s, \mu _\theta (s))ds = \mathbb{E} _{s \sim \rho^{\mu}}[r(s, \mu _\theta (s))]$</li></ul>
</li>
<li>Deterministic policy gradient: $\nabla _\theta J(\mu _\theta) = \int_S \rho^\mu(s) \nabla _\theta \mu _\theta(s) \nabla_a Q(s,a) \vert _{a = \mu _\theta(s)}ds = \mathbb{E} _{s \sim \rho^{\mu}}[\nabla _\theta \mu _\theta(s) \nabla_a Q(s,a) \vert _{a = \mu _\theta(s)}]$<ul><li>Uses the action value gradient rule from the previous section!</li></ul>
</li>
</ul>
</li>
<li>
<strong>Limit of the Stochastic Policy Gradient</strong><ul><li>Given a stochastic policy, we can parameterize the variance to be 0 and get a deterministic policy<ul><li>This can be proven using the stochastic policy gradient and deterministic policy gradients when taking the limit of the variance to 0 of the stochastic gradient</li></ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Deterministic Actor-Critic Algorithms</strong><ul>
<li>
<strong>On-Policy Deterministic Actor-Critic</strong><ul>
<li>Mainly useful if environment’s stochasticity is sufficient for exploration, else use an off-policy actor-critic</li>
<li>Substitute a differentable critic in place of the true critic<ul>
<li>Use some form of TD error to train critic ($Q^w$)</li>
<li>New update rule: $\theta _{t+1} = \theta_t + \alpha(\nabla _\theta \mu _\theta(s_t) \nabla_a Q^w (s_t, a_t)) \vert _{a = \mu _\theta(s)}$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Off-Policy Deterministic Actor-Critic</strong><ul>
<li>Performance objective: Value function of <em>deterministic target policy</em>, averaged over state distribution of <em>stochastic behavior policy</em><ul>
<li>$J _\beta (\mu _\theta) = \int _S \rho ^\beta (s) Q^\mu(s, \mu _\theta(s))ds$</li>
<li>Gradient: $\nabla _\theta J _\beta (\mu _\theta) = \int _S \rho^\beta (s) \nabla _\theta \mu(s) \nabla _a Q^\mu(s, \mu _\theta(s)) \vert _{a = \mu(s)}ds = \mathbb{E} _{s \sim \rho^\beta} [\nabla _\theta \mu(s) \nabla _a Q^\mu(s, \mu _\theta(s)) \vert _{a = \mu(s)}]$</li>
</ul>
</li>
<li>Differentiable action value function used in place of true action-value<ul><li>Same update rule as on-policy actor critic</li></ul>
</li>
</ul>
</li>
<li>
<strong>Compatible Function Approximation</strong><ul>
<li>Requirements for compatability:<ul>
<li>$Q^w(s,a) = \nabla _\theta \log \pi _\theta(a\vert s)^Tw$: linear in features of stochastic policy</li>
<li>Parameters chosen to minimize MSE: linear regression estimates Q from these features</li>
</ul>
</li>
<li>Substituting a differentiable critic is not necessarily enough to follow the true gradient of the action-value</li>
<li>We want a compatible function approximator: gradient of $Q^\mu$ can be replaced with gradient of $Q^\mu$</li>
<li>For a deterministic policy, there always exists a compatible function approximator in the form<ul>
<li>$Q^w(s,a) = (a - \mu _\theta(s))^T \nabla _\theta \mu _\theta(s)^T w+ V^v(s)$</li>
<li>Where $V$ is any baseline function indepedent of the action</li>
<li>We can set the first term of this equation equal to $A(s,a)$, the advantage</li>
</ul>
</li>
<li>Linear function approximators are good local critics, not good global ones<ul>
<li>Represents the local advantage of deviating from current deterministic policy by small amount ($\delta$)</li>
<li>Local Advantage: $A^w(s, \mu _\theta(s) + \delta) = \delta^T \nabla _\theta \mu _\theta(s)^T w$</li>
</ul>
</li>
<li>Linear regression problem with MSE:<ul>
<li>Features: $\phi(s,a)$: state-action features</li>
<li>Target: $\nabla _a Q^\mu (s,a) \vert _{a = \mu _\theta(s)}$</li>
<li>Difficult to do</li>
<li>Learn Q value function by standard policy evaluation methods</li>
</ul>
</li>
<li>Compatible Off-Policy Deterministic Actor Critic<ul>
<li>Critic: Linear function approximator from state-action value features; $\phi(s,a) = a^T \nabla _\theta \mu _\theta(s)$<ul><li>Can be learned using samples from off-policy behavior policy</li></ul>
</li>
<li>Actor: updates parameters in direction of critic’s action-value gradient</li>
<li>Update Rules:<ul>
<li>Actor: $\theta _{t+1} = \theta _{t} + \alpha _\theta \nabla _\theta \mu _\theta(s_t)(\nabla _\theta \mu _\theta (s_t)^T w)$</li>
<li>Critic: $w _{t+1} = w_t + \alpha_w \delta_t \phi(s,a)$</li>
<li>Value Function: $v _{t+1} = v_t + \alpha_v \delta_t \phi(s)$</li>
</ul>
</li>
</ul>
</li>
<li>Compatible Off-Policy Deterministic Actor Critic with Gradient Q Learning (COPDAC-GQ)<ul>
<li>Newer methods based on true gradient descent + gradient TD learning<ul><li>Minimize the mean squared project bellman error (MSPBE)</li></ul>
</li>
<li>Uses step sizes to ensure critic updated on faster time scale than actor (so that critic converges to minimizing MSPBE)</li>
</ul>
</li>
<li>Natural policy gradients can be extended into deterministic policies<ul>
<li>Fisher information matrix metric for deterministic policies: $M _\mu (\theta) = \mathbb{E} _{s \sim \rho^\mu}[\nabla _\theta \mu _\theta(s)\nabla _\theta \mu _\theta(s)^T]$</li>
<li>Policy gradient with compatible function approximation: $\nabla _\theta J(\mu _\theta) = \mathbb{E} _{s \sim \rho^\mu}[\nabla _\theta \mu _\theta(s)\nabla _\theta \mu _\theta(s)^T w]$<ul><li>Steepest ascent direction: $w = M _\mu(\theta)^{-1}\nabla _\theta J _\beta(\mu _\theta)$</li></ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Continuous Bandit</strong><ul>
<li>Continuous bandit problem with high dimensional quadratic cost function</li>
<li>Compare SAC to COPDAC<ul>
<li>SAC: Uses isotropic gaussian</li>
<li>COPDAC: Fixed width gaussian behavior policy</li>
<li>Critic estimated by mapping features to costs</li>
<li>Critic recomputed each successive batch of 2 million steps</li>
<li>Actor updated once per batch</li>
</ul>
</li>
<li>Evaluated via average cost per step incurred by the mean</li>
<li>COPDAC outperforms SAC by wide margin and this margin increases as the dimensionality increases</li>
</ul>
</li>
<li>
<strong>Continuous Reinforcement Learning</strong><ul>
<li>Mountain car, pendulum, and 2d puddle world tasks</li>
<li>COPDAC slightly outperforms SAC and OffPAC</li>
</ul>
</li>
<li>
<strong>Octopus Arm</strong><ul><li>COPDAC achieves good results on this environment</li></ul>
</li>
</ul>
</li>
<li>
<strong>Discussion</strong><ul>
<li>In stochastic policy gradient, policy becomes more deterministic as it finds a good strategy<ul><li>Harder to estimate gradient because it changes rapidy near the mean</li></ul>
</li>
<li>Deterministic actor-critic similar to Q learning which learns deterministic greedy policy, off policy while executing a noisy version of that policy<ul><li>COPDAC does the same thing</li></ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-04-24T00:00:00+00:00">April 24, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
