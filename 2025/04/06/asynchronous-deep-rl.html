<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Asynchronous Methods for Deep Reinforcement Learning">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about asynchronous parallel RL environments and modified algorithms">
<meta property="og:description" content="A paper about asynchronous parallel RL environments and modified algorithms">
<link rel="canonical" href="https://samratsahoo.com/2025/04/06/asynchronous-deep-rl">
<meta property="og:url" content="https://samratsahoo.com/2025/04/06/asynchronous-deep-rl">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-06T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Asynchronous Methods for Deep Reinforcement Learning">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-04-06T00:00:00+00:00","datePublished":"2025-04-06T00:00:00+00:00","description":"A paper about asynchronous parallel RL environments and modified algorithms","headline":"Asynchronous Methods for Deep Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/04/06/asynchronous-deep-rl"},"url":"https://samratsahoo.com/2025/04/06/asynchronous-deep-rl"}</script><title> Asynchronous Methods for Deep Reinforcement Learning - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Asynchronous Methods for Deep Reinforcement Learning</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1602.01783">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Online RL algorithms thought to be unstable because of non-stationary data → updates highly correlated → use experience replay</li>
<li>Experience replay uses more memory and compute + requires off-policy learning</li>
<li>Instead of experience replay, use parallel agents in parallel environments<ul>
<li>Decorrelates agent data into stationary process</li>
<li>Agents experience will be very different at any arbitrary timestep</li>
<li>Enables on-policy SARSA, n-step methods, or actor-critic AND off-policy Q learning</li>
<li>Can also be used on multi-core CPU instead of GPU → takes less time than GPU methods</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Related Work</strong><ul>
<li>GORILA: Asynchronous training of RL agents in distributed setting<ul>
<li>Each process has agent that acts on own environment with separate replay memory</li>
<li>Gradients asynchronously sent to central server which updates central model</li>
</ul>
</li>
<li>Map Reduce for RL: Used parallelism to speed up matrix operations</li>
<li>Parallel SARSA: Seperate actors learn using SARSA and use p2p communication to share experience with other actors</li>
<li>Q Learning Convergence: Q learning is guaranteed to converge even with outdated information as long as it eventually gets discarded</li>
<li>Evolutionary Methods: These can be parallelized</li>
</ul>
</li>
<li>
<strong>Reinforcement Learning Background</strong><ul>
<li>Value based Methods: Minimize MSE in estimated Q value and actual Q value (from env) → make policy greedy or epsilon greedy with respect to Q value</li>
<li>Policy based Methods: Directly parameterize the policy<ul>
<li>REINFORCE: Update policy parameters in direction of $\nabla_\theta \log \pi(a_t \vert s_t; \theta)R_t$</li>
<li>Reduce variance by subtracting baselines from the return; converts gradient into $\nabla_\theta \log \pi(a_t \vert s_t; \theta)(R_t - b_t(s_t))$<ul>
<li>Common baseline is value function $V^\pi(s_t)$</li>
<li>Advantage of action: $A(s_t, a_t) = Q(a_t, s_t) - V(s_t)$</li>
<li>Similar to actor critic where policy is actor and baseline is critic</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Asynchronous RL Framework</strong><ul>
<li>Use multiple async actors on one machine’s CPU threads → removes communication costs</li>
<li>Each actor gets a different exploration policy → makes online updates less correlated<ul><li>No replay memory</li></ul>
</li>
<li>Reduction in training time, roughly linear with number of actor-learners</li>
<li>On-policy training is now stable</li>
<li>
<strong>Async one-step Q Learning</strong><ul>
<li>Each thread interacts with own copy of environment and computes Q learning loss</li>
<li>Accumulates multiple steps of gradients before applying (similar to using minibatches)<ul>
<li>Reduces chance of actors overwriting other actor updates</li>
<li>Trades off computational efficiency for data efficiency</li>
</ul>
</li>
<li>Use epsilon greedy with a sampled epsilon value for each environment</li>
</ul>
</li>
<li>
<strong>Async one-step SARSA</strong><ul><li>Same algorithm as Q learning except instead of taking max Q, we use SARSA pairs for target</li></ul>
</li>
<li>
<strong>Async n-step Q Learning</strong><ul>
<li>Computes n-step returns in forward view (instead of backward view like its usually done)</li>
<li>Computes gradients for n-step Q learning updates for each state-action pair encountered since last update<ul>
<li>Uses longest possible n-step return<ul><li>One-step update for last state, two step for second to last, etc.</li></ul>
</li>
<li>Accumulated updates applied in single gradient step</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Advantageous Actor Critic (A3C)</strong><ul>
<li>Maintains a policy and value function</li>
<li>Operates in forward view eligibility trace and uses n-step returns to update policy + value function</li>
<li>Updated every $t_{max}$ steps or when a terminal state is reached<ul>
<li>Updates based on REINFORCE update step</li>
<li>$\nabla_\theta^{‘}\log \pi(a_t \vert s_t; \theta^{‘})A(s_t, a_t; \theta, \theta_v)$</li>
</ul>
</li>
<li>Parallel actors improve policy and value<ul><li>Policy and value likely share some parameters</li></ul>
</li>
<li>Added entropy to policy to improve exploration to discourage suboptimal convergence<ul><li>Gradient with entropy regularization: $\nabla_\theta^{‘}\log \pi(a_t \vert s_t; \theta^{‘})(R_t - V(s_t; \theta_v)) + \beta\nabla_\theta^{‘} H(\pi(s_t;\theta))$</li></ul>
</li>
</ul>
</li>
<li>
<strong>Optimization</strong><ul><li>Used non-centered RMSProp</li></ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Atari 2600</strong><ul>
<li>Asynchronous methods faster than synchronous ones</li>
<li>N-step methods faster than one-step ones</li>
<li>Tuned hyperparameters using a search</li>
<li>A3C significantly improves on SOTA average score in half the training time and no GPU for 57 games</li>
<li>Matches human median score of dueling double DQN Almost matches median human score of Gorila</li>
</ul>
</li>
<li>
<strong>TORCS Car Racing Simulator</strong><ul><li>A3C best performing agent -- received 75% to 90% of the score obtained by human tester</li></ul>
</li>
<li>
<strong>Continuous Action Control Using the MuJoCo Physics Simulator</strong><ul><li>Found good policies in under 24 hours</li></ul>
</li>
<li>
<strong>Labyrinth</strong><ul><li>Each episode is a new maze → much more challenging<ul><li>Finds a reasonable strategy for exploring mazes</li></ul>
</li></ul>
</li>
<li>
<strong>Scalability and Data Efficiency</strong><ul>
<li>Parallel workers lead to substantial speed ups<ul><li>Order of magnitude faster with 16 threads</li></ul>
</li>
<li>Async Q Learning + SARSA = superlinear speedups that cannot be explained by computational gains</li>
<li>One-step methods require less data<ul><li>Reduced bias with multiple threads</li></ul>
</li>
</ul>
</li>
<li>
<strong>Robustness and Stability</strong><ul><li>Large choice of learning rates leads to good scores → async methods are robust<ul>
<li>Almost no points with scores of 0</li>
<li>Methods are stable and do not collapse or diverge</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Conclusions and Discussions</strong><ul>
<li>Parallel actors has stabalizing effect on learning process</li>
<li>Online q learning possible without experience replay<ul><li>Although experience replay with async environments could substantially improve data efficiency</li></ul>
</li>
<li>Combining other RL methods with async framework could show even more improvements</li>
<li>Can improve A3C using generalized advantage estimation</li>
<li>Can try to use non-linear function approximation with temporal methods</li>
<li>Can use dueling architecture or spatial softmax for more improvements</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-04-06T00:00:00+00:00">April 6, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
