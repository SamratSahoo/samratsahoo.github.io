<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Combining Policy Gradient and Q-learning">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about the policy gradient and Q-learning algorithm">
<meta property="og:description" content="A paper about the policy gradient and Q-learning algorithm">
<link rel="canonical" href="https://samratsahoo.com/2025/05/18/pgql">
<meta property="og:url" content="https://samratsahoo.com/2025/05/18/pgql">
<meta property="og:site_name" content="samratâ€™s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-05-18T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Combining Policy Gradient and Q-learning">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-05-18T00:00:00+00:00","datePublished":"2025-05-18T00:00:00+00:00","description":"A paper about the policy gradient and Q-learning algorithm","headline":"Combining Policy Gradient and Q-learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/05/18/pgql"},"url":"https://samratsahoo.com/2025/05/18/pgql"}</script><title> Combining Policy Gradient and Q-learning - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Combining Policy Gradient and Q-learning</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1611.01626">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>With action value techniques, we fit an action value function<ul>
<li>SARSA is on-policy and fits Q function current policy</li>
<li>Q learning is off-policy and directly finds optimal Q values</li>
</ul>
</li>
<li>Policy gradients improve policy by updating parameters in direction of performance<ul><li>Actor-Critic methods are online and use an estimation of the action-value function (can be off-policy too)</li></ul>
</li>
<li>This paper derives link between Q values induced by policy and the policy itself (when policy is a fixed point of regularized policy gradient)<ul><li>Allows us to estimate Q values which can be refined with off-policy data</li></ul>
</li>
</ul>
</li>
<li>
<strong>Reinforcement Learning</strong><ul>
<li>Assume standard reinforcement learning setting</li>
<li>For PGQL, policy maps state-action pair to probability of taking that action in that state</li>
<li>
<strong>Action-Value Learning</strong><ul>
<li>Approximate Q values using function approximators</li>
<li>Update parameters so that Q values are as close as possible to fixed point of bellman equation<ul>
<li>SARSA uses normal bellman operator</li>
<li>Q learning uses optimal bellman operator (maximum over actions)</li>
</ul>
</li>
<li>Bellman operator approximated via sampling + boostrapping in online setting</li>
<li>Exploration happens via epsilon-greedy learning<ul><li>Alternatively, we can use boltzmann exploration which computes the softmax over Q values with a temperature: $\pi(s,a) = \frac{exp(Q(s,a)/T)}{\sum_b exp(Q(s,a)/T)}$</li></ul>
</li>
</ul>
</li>
<li>
<strong>Policy Gradient</strong><ul>
<li>Parameterizes policy directly and improves it via gradient ascent</li>
<li>In online case of policy gradient, we usually add an entropy regularizer for exploration (else policy becomes deterministic)</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Regularized Policy Gradient Theorem</strong><ul>
<li>
<strong>Tabular Case</strong><ul>
<li>Let:<ul>
<li>$f(\theta) = \mathbb{E} _{s,a}[Q^\pi(s,a) \nabla _{\theta} \log \pi(s,a)] + \alpha \mathbb{E} _{s}[\nabla _{\theta}H(\pi_s)]$</li>
<li>$g_s(\pi) \sum_a \pi(s,a)$</li>
</ul>
</li>
<li>Fixed Point: When we can no longer update $\theta$ without violating a constraint<ul>
<li>Constraints: $g_s(\pi) = 1$, $f(\theta)$ in span of vectors $\nabla _{\theta} g_s(\pi)$</li>
<li>Fixed point must satisfy $f(\theta) = \sum_s \lambda_s \nabla _{\theta}g_s(\pi)$ s.t. $g_s(\pi) = 1$ where $\lambda_s$ is a lagrange multiplier for a state, $s$<ul>
<li>Equal to: $\mathbb{E} _{s,a}[Q^\pi(s,a) - \alpha \log \pi(s,a) - c_s \nabla _{\theta} \pi(s,a)] = 0$<ul><li>$c_s$ absorbs all constants</li></ul>
</li>
<li>In tabular case, $\nabla _{\theta} \pi(s,a) = 1$ (single number for each state-action pair)</li>
<li>We want to find: $Q^\pi(s,a) - \alpha \log \pi(s,a) - c_s = 0$</li>
<li>Multiply by $\pi(a,s)$ and sum over $a \in \mathcal{A}$, $c_s = \alpha H^\pi(s) + V^\pi(s)$</li>
<li>Final policy formulation: $\pi(s,a) = exp(A^\pi(s,a) / \alpha - H^\pi(s))$<ul>
<li>Softmax over the advantage function</li>
<li>$\alpha$ can be used as a temperature</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Estimate of Q values using policy formulation: $\tilde{Q}^\pi(s,a) = \tilde{A}(s,a) + V^\pi(s) = \alpha(\log \pi(s,a) + H^\pi(s)) + V^\pi(s)$<ul><li>Gradient: $\theta \propto \mathbb{E} _{s,a}[(Q^\pi(s,a) - \tilde{Q}^\pi(s,a)) \nabla _\theta \log \pi(s,a)]$</li></ul>
</li>
<li>When policy parameterized by softmax, action preferences are Q values scaled by $1/\alpha$</li>
</ul>
</li>
<li>
<strong>General Case</strong><ul>
<li>Constrained optimization problem:<ul>
<li>Minimize: $\mathbb{E} _{s,a}[(q(s,a) - \alpha \log \pi(s,a))^2]$</li>
<li>Subject to: $\sum_a \pi(s,a) = 1, s \in \mathcal{S}$</li>
</ul>
</li>
<li>Find $\theta$ which parameterize $\pi$</li>
<li>Optimality Condition: $\mathbb{E} _{s,a}[(q(s,a) - \alpha \log \pi(s,a) + c_s)\nabla _\theta \log \pi(s,a)] = 0$<ul>
<li>$c_s$ is the lagrange multiplier for a state</li>
<li>If $q = Q^\pi$, we get same set of fixed points</li>
<li>Fixed points of regularized policy gradient = regression of log policy onto Q values</li>
</ul>
</li>
<li>Softmax formulation of policy only approximately holds<ul><li>We go ahead and estimate Q values using this formulation</li></ul>
</li>
</ul>
</li>
<li>
<strong>Connection to Action-Value Methods</strong><ul>
<li>Actor critic methods arejust action-value fitting methods</li>
<li>In Actor Critic<ul><li>Policy: $\pi^k(s,a) = exp(W^k(s,a) / \alpha) / \sum_b exp(W^k(s,b) / \alpha)$<ul>
<li>$W^k$: action preferences at iteration k, parameterized by $\theta$</li>
<li>Gradient: $\nabla _\theta \log \pi^k (s,a) = (1 / \alpha)(\nabla _{\theta}W^k(s,a) - \sum_b \pi^k(s,b) \nabla _\theta W^k(s,b))$</li>
<li>Updates:<ul>
<li>$\Delta \theta \propto \mathbb{E} _{s,a}[\delta _{ac}(\nabla _{\theta} W^k(s,a) - \sum_b \pi^k(s,b) \nabla _{\theta} W^k(s,b))]$</li>
<li>$\Delta w \propto \mathbb{E} _{s,a}\delta _{ac} \nabla_w V^k(s)$<ul>
<li>$w$: the parameters of $V$</li>
<li>$\delta _{ac}$: critic minus baseline</li>
</ul>
</li>
</ul>
</li>
</ul>
</li></ul>
</li>
<li>In action-value methods<ul>
<li>Q-Value Function: $Q(s,a) = Y^k(s,a) - \sum_b \mu(s,b) Y^k(s,b) + V^k(s)$<ul>
<li>$\mu$: Probability distribution</li>
<li>$Y^k$: parameterized function (by $\theta^k$)</li>
</ul>
</li>
<li>Exploration Policy: Boltzmann distribution<ul><li>$pi^k(s,a) = exp(Y^k(s,a) / \alpha) / \sum_b exp(Y^k(s,b) / \alpha)$</li></ul>
</li>
<li>Updates:<ul>
<li>$\Delta \theta \propto \mathbb{E} _{s,a}[\delta _{av}(\nabla _{\theta} Y^k(s,a) - \sum_b \mu(s,b) \nabla _{\theta} Y^k(s,b))]$</li>
<li>$\Delta w \propto \mathbb{E} _{s,a}\delta _{av} \nabla_w V^k(s)$<ul><li>$\delta _{av}$: action-value error term</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Equivalence between methods<ul>
<li>Policies identical if $W^k = Y^k$</li>
<li>Updates identical if $\mu = \pi^k$</li>
<li>Slightly modified action-value method is equivalent to actor-critic policy gradient</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Bellman Residual</strong><ul><li>We can show that $\vert \vert \mathcal{T}^\ast Q^{\pi _\alpha} -Q^{\pi _\alpha} \vert \vert \rightarrow 0$<ul>
<li>$\mathcal{T}^\ast Q^{\pi _\alpha} \geq \mathcal{T}^{\pi _\alpha} Q^{\pi _\alpha} = Q^{\pi _\alpha}$</li>
<li>$\Rightarrow \mathcal{T}^\ast Q^{\pi _\alpha}(s,a) - Q^{\pi _\alpha}(s,a) \geq 0$</li>
<li>$= \mathcal{T}^\ast Q^{\pi _\alpha}(s,a) - \mathcal{T}^{\pi _{\alpha}} Q^{\pi _\alpha}(s,a) \geq 0$</li>
<li>$= \mathbb{E} _{sâ€™}[max_c Q^{\pi _\alpha}(sâ€™, c) - \sum_b \pi _{\alpha}(sâ€™, b)Q^{\pi _{\alpha}}(sâ€™,b)]$</li>
<li>$= \mathbb{E} _{sâ€™}[\sum_b \pi _{\alpha}(sâ€™, b) (max_c Q^{\pi _\alpha}(sâ€™, c) - Q^{\pi _{\alpha}}(sâ€™,b))]$</li>
<li>$\leq \mathbb{E} _{sâ€™}[\sum_b exp((Q^{\pi _{\alpha}}(sâ€™, b) - Q^{\pi _\alpha}(sâ€™, b^\ast)) / \alpha) (max_c Q^{\pi _\alpha}(sâ€™, c) - Q^{\pi _{\alpha}}(sâ€™,b))]$</li>
<li>$= \mathbb{E} _{sâ€™}[\sum_b f _{\alpha} (max_c Q^{\pi _\alpha}(sâ€™, c) - Q^{\pi _{\alpha}}(sâ€™,b))]$<ul>
<li>$f _{\alpha}(x) = x exp(-x / \alpha)$</li>
<li>Since $f _\alpha (x) \leq sup_x f _{\alpha}(x) = f _{\alpha}(\alpha) = ae^{-1}$, we get:<ul>
<li>$0 \leq \mathcal{T}^\ast Q^{\pi _\alpha}(s,a) - Q^{\pi _\alpha}(s, a) \leq \vert \mathcal{A}\vert \alpha e^{-1}$</li>
<li>This tells us the bellman residual converges to 0 with decreasing $\alpha$</li>
<li>Also implies $\lim _{\alpha \rightarrow 0} Q^{\pi _\alpha} = Q^\ast$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>PGQL</strong><ul>
<li>
<strong>PGQL Update</strong><ul>
<li>Estimate of Q using policy: $\tilde{Q}^\pi(s,a) = \alpha(\log \pi(s,a) + H^{\pi}(s) + V(s))$<ul><li>Update parameters to reduce bellman residual similar to Q learning<ul>
<li>$\Delta \theta \propto \mathbb{E} _{s,a}[(\mathcal{T}^\ast \tilde{Q}^\pi(s,a) - \tilde{Q}^\pi(s,a))\nabla _{\theta} \log \pi(s,a)]$</li>
<li>$\Delta w \propto \mathbb{E} _{s,a}[(\mathcal{T}^\ast \tilde{Q}^\pi(s,a) - \tilde{Q}^\pi(s,a))\nabla_w V(s)]$</li>
</ul>
</li></ul>
</li>
<li>Full PGQL update combines regularized policy gradient with Q learning update<ul>
<li>$\Delta \theta \propto (1 - \eta) \mathbb{E} _{s,a}[(Q^\pi-\tilde{Q}^\pi)\nabla _{\theta}\log \pi ] +\eta \mathbb{E} _{s,a}[(\mathcal{T}^\ast \tilde{Q}^\pi(s,a) - \tilde{Q}^\pi(s,a))\nabla _{\theta} \log \pi]$</li>
<li>$\Delta w \propto (1-\eta) \mathbb{E} _{s,a}[(Q^\pi-\tilde{Q}^\pi)\nabla_w V] + \eta\mathbb{E} _{s,a}[(\mathcal{T}^\ast \tilde{Q}^\pi(s,a) - \tilde{Q}^\pi(s,a))\nabla_w V]$<ul>
<li>$\eta$: Weighting parameter to control how much of each update to apply<ul>
<li>$\eta = 0$: Entropy regularized policy gradient</li>
<li>$\eta = 1$: Q learning variant</li>
</ul>
</li>
<li>First term in updates encourage consistency with critic</li>
<li>Second term in updates encourage optimality over time</li>
</ul>
</li>
</ul>
</li>
<li>Interpretations:<ul>
<li>Actor critic update where critic is weighted between standard and optimizing critic</li>
<li>Update is a combination of expected SARSA and Q-learning<ul><li>Q values paramterized as sum of advantage and value functions</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Practical Implementation</strong><ul>
<li>We donâ€™t have access to an exact critic, $Q^\pi$</li>
<li>Instead agents interact with environment and update shared paramters of actor-critic algorithm<ul>
<li>Policy + critic update online</li>
<li>Maintains a replay buffer that is occasionally sampled and performs a step of Q learning on policy<ul>
<li>Allows critic to accumulate MC return over many time periods</li>
<li>Can prioritize important samples</li>
<li>Reduces temporal correlation</li>
<li>Acts as a regularizer to prevent policy from moving too far from bellman equations</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Modified Fixed Point</strong><ul>
<li>PGQL updates modified the fixed point of the algorithm</li>
<li>New Q value estimate: $\tilde{Q}^\pi = (1 - \eta)Q^\pi + \eta \mathcal{T}^\ast \tilde{Q}^\pi$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Numerical Experiments</strong><ul>
<li>
<strong>Grid World</strong><ul>
<li>Tested against actor-critic where estimate of value function is used for critic and Q learning where an update is performed from replay buffer data</li>
<li>Deterministic environment</li>
<li>PGQL outperformed Actor-Critic and Q learning</li>
</ul>
</li>
<li>
<strong>Atari</strong><ul>
<li>Compared against A3C + asynchronous version of deep Q learning</li>
<li>PGQL performed the best 34 games, A3C in 7, and Q learning in 10<ul><li>6 games had two methods tie</li></ul>
</li>
<li>PGQL has highest mean + median in human starts</li>
<li>PGQL only the worst in 1 game</li>
<li>In every game that PGQL did not perform well, it had better data efficiency early on</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-05-18T00:00:00+00:00">May 18, 2025</time> Â· <a href="/tags/research">research</a></span></section></main></body>
</html>
