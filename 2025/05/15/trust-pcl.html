<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Trust-PCL: An Off-Policy Trust Region Method for Continuous Control">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about the trust-pcl algorithm">
<meta property="og:description" content="A paper about the trust-pcl algorithm">
<link rel="canonical" href="https://samratsahoo.com/2025/05/15/trust-pcl">
<meta property="og:url" content="https://samratsahoo.com/2025/05/15/trust-pcl">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-05-15T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Trust-PCL: An Off-Policy Trust Region Method for Continuous Control">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-05-15T00:00:00+00:00","datePublished":"2025-05-15T00:00:00+00:00","description":"A paper about the trust-pcl algorithm","headline":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/05/15/trust-pcl"},"url":"https://samratsahoo.com/2025/05/15/trust-pcl"}</script><title> Trust-PCL: An Off-Policy Trust Region Method for Continuous Control - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1707.01891">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>For continuous control, we cannot use classical Q learning so we use DDPG instead<ul><li>Unfortunately this is hyperparameter sensitive</li></ul>
</li>
<li>For better stability, we got TRPO but TRPO cannot exploit off-policy data</li>
<li>When using entropy regularization with and optimal policy + value function, we satisfy a set of pathwise consistency properties under any path<ul><li>Allows on + off-policy data to be used for training (i.e., PCL algorithm)</li></ul>
</li>
<li>This paper expands on PCL for more challenging continuous control tasks<ul>
<li>Augmenting maximum reward objective with relative entropy regularizers (KL Divergence), we still satisfy consistency properties</li>
<li>Resulting objective = penalty based divergence constraint from previous policy</li>
<li>Entropy coefficient agnostic to reward scale</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Notation and Background</strong><ul>
<li>Assume standard reinforcement learning setting<ul><li>Maximize expected reward</li></ul>
</li>
<li>Path Consistency Learning (see <a href="https://samratsahoo.com/2025/05/11/pcl">PCL notes</a>)<ul>
<li>Augment objective with entropy regularizer</li>
<li>Minimize squared error between LHS and RHS of $V^\ast(s_0) = \mathbb{E} _{r_i, s_i}[\gamma^d V^\ast(s_d) + \sum^{d-1} _{i=0}\gamma^i(r_i - \tau \log \pi^\ast (a_i \vert s_i))]$<ul>
<li>Can simultaneously optimize parameterized $\pi _\phi$ and $V _\phi$</li>
<li>Can use on and off-policy data</li>
</ul>
</li>
<li>TRPO (see <a href="https://samratsahoo.com/2025/04/07/trpo">TRPO notes</a>)<ul><li>Creates a KL divergence constraint that ensures policy updates are within a certain boundary</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Method</strong><ul>
<li>Augment entropy regularized expected reward with discount relative trust region around prior policy<ul>
<li>$maximize _{\pi} \mathbb{E}_s [O _{ENT}(\pi)] s.t. {E}_s [O _{ENT}(\mathbb{G}(s, \pi, \tilde{\pi}))] \leq \epsilon$</li>
<li>Discounted relative entropy: $\mathbb{G}(s, \pi, \tilde{\pi}) = \mathbb{E} _{a, s’}[\log (a \vert s) - \log \tilde{\pi}(a \vert s) + \gamma \mathbb{G}(s’, \pi, \tilde{\pi})]$</li>
<li>Objective tries to maximize entropy regularized expected reward + maintain proximity to previous policy<ul>
<li>Entropy regularization: Improves exploration</li>
<li>Relative entropy: improves stability + faster learning</li>
</ul>
</li>
</ul>
</li>
<li>Cast constrained optimization into maximization problem: $O _{RELENT}(s, \pi) = O _{ENT}(s, \pi) - \gamma \mathbb{G}(s, \pi, \tilde{\pi})$<ul><li>Usually computed as expectation over states: $O _{RELENT}(\pi) = \mathbb{E}_s [O _{RELENT}(s, \pi)]$</li></ul>
</li>
<li>
<strong>Path Consistency with Relative Entropy</strong><ul>
<li>$O _{RELENT}$ can be decomposed into entropy regularized expected reward with transformed rewards<ul><li>$O _{RELENT} = \tilde{O} _{ER}(s, \pi) + (\tau + \lambda) \mathbb{H}(s, \pi)$<ul><li>$\tilde{O} _{ER}(s, \pi)$ has transformed reward distribution: $\tilde{r}(s,a) = r(s,a) + \lambda \log \tilde{\pi}(a \vert s)$</li></ul>
</li></ul>
</li>
<li>Optimal policy: $\pi^\ast(a_t \vert s_t) = exp(\frac{\mathbb{E} _{\tilde{r}_t \sim \tilde{r}(s_t, a_t), s _{t+1}}[\tilde{r}_t + \gamma V^\ast(s _{t+1})] - V^\ast(s_t)}{\tau + \lambda})$<ul>
<li>Softmax state values: $V^\ast(s_t) = (\tau + \lambda)\log \int_A exp(\frac{\mathbb{E} _{\tilde{r}_t \sim \tilde{r}(s_t, a_t), s _{t+1}}[\tilde{r}_t + \gamma V^\ast(s _{t+1})]}{\tau + \lambda})da$<ul><li>Simplification for single-step consistency: $V^\ast(s_t) = \mathbb{E} _{\tilde{r}_t, s _{t+1}}[\tilde{r}_t - (\tau + \lambda) \log \pi^\ast (a_t \vert s_t) + \lambda \log \tilde{\pi}(a _{t+i} \vert s _{t+i}) + \gamma V^\ast(s _{t+1})]$</li></ul>
</li>
<li>Expand single-step consistency to multi-step consistency: $V^\ast(s_t) = \mathbb{E} _{\tilde{r} _{t+i}, s _{t+i}}[\gamma^d V^\ast (s _{t+d}) + \sum _{i=0}^{d-1} \gamma^i(r _{t+i} - (\tau + \lambda) \log \pi^\ast (a _{t+i} \vert s _{t+i}) + \lambda \log \tilde{\pi}(a _{t+i} \vert s _{t+i}))]$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Trust-PCL</strong><ul><li>Multi-step consistency error: $C(s _{t:t+d}, \theta, \phi) = - V _{\phi}(s_t) + \gamma^d V _\phi(s _{t+d}) + \sum _{i=0}^{d-1} \gamma^i (r _{t+i} - (\tau + \lambda) \log \pi _\theta (a _{t+i} \vert s _{t+i}) + \lambda \log \pi _{\tilde{\theta}}(a _{t+i} \vert s _{t + i}))$<ul>
<li>Minimize the squared consistency error over a batch of episodes</li>
<li>Batch can be from on or off-policy data</li>
</ul>
</li></ul>
</li>
<li>
<strong>Automatic Tuning of the Lagrange Multiplied $\lambda$</strong><ul>
<li>$\lambda$ needs to adapt to distribution of rewards</li>
<li>Instead make $\lambda$ a function of $\epsilon$ where $\epsilon$ is a hard constraint on relative entropy</li>
<li>In trust PCL, you can perform a line search to find a $\lambda (\epsilon)$ which finds a $KL(\pi^\ast \vert \vert \pi _{\tilde{\theta}})$ as close as possible to $\epsilon$<ul>
<li>See paper for analysis for KL maxium divergence</li>
<li>$\epsilon$ can change during training; as episode length increases, KL generally increases too</li>
<li>For a set of episodes, approximate $\lambda$ that yields maximum divergence of $\frac{\epsilon}{N}\sum _{k=1}^N T_k$<ul><li>$\epsilon$ becomes constraint on length averaged KL</li></ul>
</li>
</ul>
</li>
<li>To avoid many interactions with environment, use last 100 episodes in practice<ul>
<li>Not exactly the same as sampling from old policy</li>
<li>But close enough since old policy is lagged version of online policy</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Setup</strong><ul>
<li>Tested on discrete + continuous control tasks</li>
<li>Compared with TRPO</li>
</ul>
</li>
<li>
<strong>Results</strong><ul>
<li>Trust PCL is able to match or exceed TRPO in reward and sample efficiency</li>
<li>
<strong>Hyperparmeter Analysis</strong><ul>
<li>As $\epsilon$ increases, instability also increases</li>
<li>Standard PCL would fail in many of these scenarios because standard PCL is when $\epsilon \rightarrow \infty$</li>
<li>Trust PCL is better than TRPO because of its ability to learn in an off-policy manner (better sample efficiency)</li>
<li>$\tau$ not too important for tasks evaluated<ul>
<li>Best results with $\tau = 0$</li>
<li>$\tau &gt; 0$ had marginal effect</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-05-15T00:00:00+00:00">May 15, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
