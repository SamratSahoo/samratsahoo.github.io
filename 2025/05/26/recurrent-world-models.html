<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Recurrent World Models Facilitate Policy Evolution">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about world models">
<meta property="og:description" content="A paper about world models">
<link rel="canonical" href="https://samratsahoo.com/2025/05/26/recurrent-world-models">
<meta property="og:url" content="https://samratsahoo.com/2025/05/26/recurrent-world-models">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-05-26T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Recurrent World Models Facilitate Policy Evolution">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-05-26T00:00:00+00:00","datePublished":"2025-05-26T00:00:00+00:00","description":"A paper about world models","headline":"Recurrent World Models Facilitate Policy Evolution","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/05/26/recurrent-world-models"},"url":"https://samratsahoo.com/2025/05/26/recurrent-world-models"}</script><title> Recurrent World Models Facilitate Policy Evolution - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Recurrent World Models Facilitate Policy Evolution</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1809.01999">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Humans develop mental model based on perception<ul><li>Predict future sensory data based on actions</li></ul>
</li>
<li>Use recurrent networks in partially observable scenarios to draw on memories</li>
<li>Predictive model (M) exploited by controller (C) to which learns through RL to perform a task</li>
<li>Model based RL approaches learn an environment but also train on one<ul><li>We can replace RL environments with generated ones with the M model</li></ul>
</li>
<li>Use a temperature parameter to control uncertainty<ul><li>Controller trained in more uncertain version of generated environment; prevents controller from taking advantage of imperfections of M</li></ul>
</li>
</ul>
</li>
<li>
<strong>Agent Model</strong><ul>
<li>Visual sensory component compressed into representation</li>
<li>Memory component that makes predictions about future representations based on past observations</li>
<li>Decision making component to take actions based on vision and memory</li>
<li>Vision Component:<ul><li>VAE converts 2d image to representation</li></ul>
</li>
<li>Memory Component<ul>
<li>Compresses what happens over time into representation</li>
<li>Output a probabibility density function for stochastic representation</li>
<li>Approximate representation through mixture of gaussians</li>
<li>Computes: $P(z _{t+1} \vert a_t, z_t, h_t)$</li>
<li>Can use temperature ($\tau$) to control uncertainty</li>
<li>Called a mixture density network (MDN-RNN)</li>
</ul>
</li>
<li>Decision Making Component:<ul>
<li>Single linear layer model that maps vision representation and memory output to an action output</li>
<li>$a_t = W_c [z_t, h_t] + b_c$</li>
<li>Can be trained via backprop or evolutionary strategies</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Car Racing Experiment: World Model for Feature Extraction</strong><ul>
<li>Training procedure<ul>
<li>Collect rollouts</li>
<li>Train VAE to encode frames</li>
<li>Train MDN-RNN to model $P(z _{t+1} \vert a_t, z_t, h_t)$</li>
<li>Evolve controller to maximize episodic reward (via CMA-ES)</li>
</ul>
</li>
<li>
<strong>Experiment Results</strong><ul>
<li>Vision without Memory:<ul>
<li>Controller network becomes $a_t = W_c z_t + b_c$</li>
<li>Agent can still navigate the track but misses on sharp corners and wobbles around</li>
<li>Adding another layer to controller helps but isn’t sufficient to solve the task</li>
</ul>
</li>
<li>Vision and Memory:<ul>
<li>Combining vision and memory, agent has good representation of the current state and future expectations</li>
<li>Can attack sharp corners well + is more stable</li>
<li>Doesn’t need to plan ahead because RNN representation contains distribution of future which it can use to guide decisions</li>
<li>Does not need any preprocessing on frames $\rightarrow$ can take raw RGB pixels</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>VizDoom Experiment: Learning Inside of a Generated Environment</strong><ul>
<li>We want to know if an RL agent can learn inside of its generated environment</li>
<li>
<strong>Experiment Setup</strong><ul>
<li>We make the model predict next state and whether the agent terminates</li>
<li>Generated environment is more challenging because of our ability to add uncertainty<ul>
<li>Agent ends up scoring higher on generated environment as a result</li>
<li>Higher temperature = prevent agent from taking advantage of imperfections</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Cheating the World Model</strong><ul>
<li>Agents sometimes discover adversarial policies (i.e., moving in a speciifc way to extinguish fireballs)</li>
<li>Since M is probabilistic, it may generate trajectories that don’t follow laws of actual environment</li>
<li>Controller also has access to all hidden states of M<ul>
<li>Agent can determine how to manipulate hidden states $\rightarrow$ easier to find an adversarial policy to fool dynamics model</li>
<li>Makes it so that previous model based RL methods can’t use learned dynamics models to fully replace environments</li>
<li>Uncertainty estimates can help mitigate this</li>
<li>Recent works combine model-based with model-free to fine tune policies on real environment after the policy has been learned</li>
</ul>
</li>
<li>Use MDN-RNN as distribution of possible outcomes rather than predicting a deterministic future<ul>
<li>Builds in stochasticity and allows us to use temperature to control randomness</li>
<li>Using a mixture of gaussians is useful to model discrete events</li>
<li>Too low of a temperature = mode collapse (doesn’t generalize well to real environment)</li>
<li>Too high of a temperature = too hard for agent to learn</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Discussion</strong><ul>
<li>Agents trained to simulate reality could be useful for sim2real</li>
<li>VAE may encode irrelevant parts of the task<ul>
<li>Training with the MDN-RNN lets it learn task relevant features</li>
<li>Would need to retrain VAE to reuse for new tasks</li>
</ul>
</li>
<li>Need an iterative training process so that the world model can be improved over time</li>
<li>LSTMs have limited memory inside of weights<ul>
<li>Need to replace the model with higher capacity models</li>
<li>Use external memory modules for more complicated worlds</li>
</ul>
</li>
<li>Classical Controller-Model systems ignore spatio-temporal details (don’t profit from hierarchal planning)<ul>
<li>Alternative approaches allow controllers to learn to addres subroutines of the world model</li>
<li>We can compress the controller and world model into 1 and use behavioral replay to avoid forgetting old prediction + control skills when learning new ones</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-05-26T00:00:00+00:00">May 26, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
