<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Action-dependent Control Variates for Policy Optimization via Stein’s Identity">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about stein control variates">
<meta property="og:description" content="A paper about stein control variates">
<link rel="canonical" href="https://samratsahoo.com/2025/05/10/stein-control-variates">
<meta property="og:url" content="https://samratsahoo.com/2025/05/10/stein-control-variates">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-05-10T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Action-dependent Control Variates for Policy Optimization via Stein’s Identity">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-05-10T00:00:00+00:00","datePublished":"2025-05-10T00:00:00+00:00","description":"A paper about stein control variates","headline":"Action-dependent Control Variates for Policy Optimization via Stein’s Identity","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/05/10/stein-control-variates"},"url":"https://samratsahoo.com/2025/05/10/stein-control-variates"}</script><title> Action-dependent Control Variates for Policy Optimization via Stein’s Identity - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Action-dependent Control Variates for Policy Optimization via Stein’s Identity</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1710.11198">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Policy gradient = high variance</li>
<li>Control Variate: subtract monte carlo gradient estimator by baseline which has 0 expectation<ul><li>Doesn’t introduce bias but in theory reduces variance</li></ul>
</li>
<li>Baseline choices<ul>
<li>Constant (REINFORCE)</li>
<li>State-dependent baseline like V(s) (A2C)</li>
<li>Action dependent baseline (Q-Prop)</li>
</ul>
</li>
<li>State-action dependent baselines could be more powerful but hard to design when expectation needs to be 0</li>
<li>Use stein control variates (depends on stein’s identity) to create arbitrary baseline functions that depend on state and action</li>
</ul>
</li>
<li>
<strong>Background</strong><ul>
<li>
<strong>Reinforcement Learning and Policy Gradient</strong><ul>
<li>Assume classical reinforcement learning and policy gradient settings</li>
<li>Obtain trajectories, empirically find $\hat{Q}^\pi (s_t, a_t)$, and estimate $\nabla _\theta J(\theta)$<ul><li>$\hat{\nabla} _{\theta} J(\theta) = \frac{1}{n} \sum _{t = 1}^n \gamma ^{t-1} \nabla _\theta \log \pi(a_t \vert s_t) \hat{Q}^\pi (s_t, a_t)$</li></ul>
</li>
</ul>
</li>
<li>
<strong>Control Variate</strong><ul>
<li>Control Variate: a function, $f(s,a)$ with a known expectation under $\tau$ which can be assumed to be 0</li>
<li>Alternative unbiased estimator: $\hat{\mu} = \frac{1}{n} \sum _{t=1}^n (g(s_t, a_t) - f(s_t, a_t))$<ul>
<li>Variance: $var _{\tau}(g - f) / n$</li>
<li>When $f$ similar to $g$, (ideally: $f = g - \mu$), variance is reduced causing a better estimator</li>
</ul>
</li>
<li>For A2C and REINFORCE, the baseline cannot be equal to $Q^\pi (s,a)$ because $\phi$ (the baseline) only depends on state and not action</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Policy Gradient with Stein Control Variate</strong><ul>
<li>
<strong>Stein’s Identity</strong><ul><li>Stein’s identity: $\mathbb{E} _{\pi(a \vert s)}[\nabla _a \log \pi(a \vert s) \phi(s,a) + \nabla_a \phi(s,a)] = 0$<ul>
<li>$\phi(s,a)$ can be any function that hold for some conditions</li>
<li>We can achieve zero-variance estimators for general monte carlo estimations</li>
</ul>
</li></ul>
</li>
<li>
<strong>Stein Control Variate for Policy Gradient</strong><ul>
<li>Cannot apply stein’s identity directly to policy gradient<ul>
<li>Policy gradient takes gradients with respect to parameters, $\theta$</li>
<li>Stein’s identity takes gradient with respect to actions</li>
</ul>
</li>
<li>Reparameterization:<ul>
<li>Let $a = f _\theta(s, \xi)$ where $\xi$ is random noise independent of $\theta$</li>
<li>$\pi(a, \xi \vert s)$: joint distribution of $(a, \xi)$ conditioned on $s \Rightarrow \pi(a \vert s) = \int \pi(a \vert s, \xi) \pi(\xi) d\xi$<ul>
<li>$\pi(\xi)$ is the noise generating distributions</li>
<li>$\pi(a \vert s, \xi) = \delta(a - f(s, \xi))$</li>
</ul>
</li>
<li>Reparameterized expectation: $\mathbb{E} _{\pi(a \vert s)}[\nabla _a \log \pi(a \vert s) \phi(s,a)] = \mathbb{E} _{\pi(a, \xi \vert s)}[\nabla _\theta f _{\theta}(s, \xi) \nabla_a \phi(s,a)]$</li>
</ul>
</li>
<li>Stein Control Variate Policy Gradient Equation<ul>
<li>$\nabla _{\theta} J(\theta) = \mathbb{E} _\pi [\nabla _{\theta} \log \pi (a \vert s) (\hat{Q}^\pi(s,a) - \phi(s,a)) + \nabla _{\theta} f _{\theta}(s, \xi) \nabla_a \phi(s,a)]$</li>
<li>Estimator: $\hat{\nabla} _{\theta} J(\theta) = \frac{1}{n}\sum _{t=1}^n [\nabla _{\theta} \log \pi (a \vert s) (\hat{Q}^\pi(s,a) - \phi(s,a)) + \nabla _{\theta} f _{\theta}(s, \xi) \nabla_a \phi(s,a)]$</li>
</ul>
</li>
<li>Relation to Q-Prop<ul>
<li>Q-Prop creates a control variate using a taylor expansion; this is just a special $\phi$ with an action-independent gradient<ul><li>$\nabla_a \phi(s,a)$ in stein control variate becomes $\varphi(s)$</li></ul>
</li>
<li>We know that $\mathbb{E} _{\pi(\xi)}[\nabla _{\theta} f(s, \xi)] = \nabla _{\theta} \mathbb{E} _{\pi(\xi)}[f(s, \xi)] = \nabla _{\theta} \mu _{\pi}(s)$ which is th expectation of the action conditioned on $s$<ul><li>$\nabla _{\theta} f _{\theta}(s, \xi)$ becomes $\mu _{\pi}(s)$</li></ul>
</li>
<li>Baseline function: $\phi(s,a) = \hat{V}^\pi(s) + \langle \nabla_a \hat{Q}^\pi(s, \mu _{\pi}(s)), a - \mu _\pi(s)\rangle$</li>
</ul>
</li>
<li>Relation to Reparameterization<ul>
<li>Auxillary objective Function: $L_s(\theta) = \mathbb{E} _{\pi(a \vert s)}[\phi(s,a)] = \int \pi(a \vert s) \phi(s,a)da$</li>
<li>Apply log derivative trick: $\nabla _\theta L_s(\theta) = \int \nabla _\theta \pi(a \vert s)\phi(s,a)da = \mathbb{E} _\pi(a \vert s)[\nabla _\theta \log \pi(a\vert s) \phi(s,a)]$</li>
<li>Reparameterized gradient when $a = f _\theta(s, \xi)$ and $L_s = \mathbb{E} _{\pi(\xi)}[\phi(s, f _\theta(s,\xi))]$: $\mathbb{E} _{\pi(a, \xi \vert s)}[\nabla _\theta f _{\theta}(s, \xi) \nabla_a \phi(s,a)]$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Constructing Baseline Functions for Stein Control Variate</strong><ul>
<li>Assume a parameteric form for baseline: $\phi_w(s,a)$</li>
<li>If $\phi$ constructed on trajectory data = additional dependency + no longer unbiased (albeit negligible bias in practice)</li>
<li>Estimating $\phi$ by fitting Q function<ul>
<li>If $\phi(s,a) = Q^\pi(s,a)$, then the stein control variate policy gradient becomes an interpolation between log-likelihood and reparamterized policy gradients</li>
<li>Leads to much smaller variances with an extreme case being a deterministic policy where log-likelihood policy gradient variance is infinite and reparametermized is close to 0<ul><li>In this scenario, we favor the reparameterized policy gradient</li></ul>
</li>
<li>We should set $\phi(s,a) = \hat{Q}^\pi(s,a)$ so that log-likelihood ratio term is small<ul><li>$min_w \sum _{t=1}^n(\phi_w(s_t, a_t) - R_t)^2$</li></ul>
</li>
</ul>
</li>
<li>Estimating $\phi$ by minimizing the variance<ul>
<li>We know $var(\hat{\nabla} _{\theta} J(\theta)) = \mathbb{E}[(\hat{\nabla} _{\theta} J(\theta))^2] - \mathbb{E}[\hat{\nabla} _{\theta} J(\theta)]^2$<ul><li>$\mathbb{E}[\hat{\nabla} _{\theta} J(\theta)] = \hat{\nabla} _{\theta} J(\theta)$: doesn’t rely on $\phi$ therefore we can omit minimizing this</li></ul>
</li>
<li>$min_w \sum _{t=1}^n \vert\vert \nabla _\theta \log \pi(a_t\vert s_t)(\hat{Q}^\pi(s_t, a_t) - \phi_w (s_t, a_t)) + \nabla _\theta f(s_t, \xi_t) \nabla_a \phi_w(s_t, a_t)\vert\vert_2^2$<ul><li>Hard to implement efficiently due to gradients with $a$ and $\theta$; use an approximation instead</li></ul>
</li>
</ul>
</li>
<li>Architectures of $\phi$<ul>
<li>Since $\phi$ and $Q$ are similar, decompose $\phi$: $\phi_2(s,a) = \hat{V}^\pi(s) + \psi_w (s,a)$</li>
<li>New gradient: $\hat{\nabla} _{\theta}J(\theta) = \frac{1}{n} \sum _{t=1}^n [\nabla _{\theta} \log \pi(a_t \vert s_t)(\hat{A}^\pi(s_t, a_t) - \psi_w(s_t, a_t)) + \nabla _{\theta} f _{\theta}(s_t, \xi_t) \nabla_a \psi_w(s_t, a_t)]$<ul><li>Seperating $\hat{V}^\pi(s)$ from $\psi_w(s,a)$ works well because it provides an estimation of $\phi$ and allows us to improve on top of baseline</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Stein Control Variate for Gaussian Policies</strong><ul>
<li>Gaussian policy: $\pi(a \vert s) = \mathcal{N}(a; \mu _{\theta_1}(s), \Sigma _{\theta_2}(s))$<ul><li>Mean and covariance are parameteric functions with parameters $\theta_1, \theta_2$ repsectively</li></ul>
</li>
<li>Policy gradient with respect to $\theta_1$: $\nabla _{\theta_1} J(\theta) = \mathbb{E} _\pi [\nabla _{\theta_1} \log \pi (a \vert s) (\hat{Q}^\pi(s,a) - \phi(s,a)) + \nabla _{\theta_1} \mu (s) \nabla_a \phi(s,a)]$</li>
<li>Gradient with respect to variance parameter (for each coordinate $\theta_l$): $\nabla _{\theta_l} J(\theta) = \mathbb{E} _{\pi}[\nabla _{\theta_l} \log \pi (a \vert s) (Q^\pi(s,a) - \phi(s,a)) - \frac{1}{2} \langle \nabla_a \log \pi(a \vert s) \nabla_a \phi(s,a)^T, \nabla _{\theta_l} \Sigma \rangle]$<ul>
<li>Angle brackets denote trace operator</li>
<li>Can simplify further with stein’s identity: $\nabla _{\theta_l} J(\theta) = \mathbb{E} _{\pi}[\nabla _{\theta_l} \log \pi (a \vert s) (Q^\pi(s,a) - \phi(s,a)) - \frac{1}{2} \langle \nabla _{a,a} \phi(s,a), \nabla _{\theta_l} \Sigma \rangle]$<ul><li>Requires 2nd derivative but may have lower variance</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>PPO with Stein Control Variate</strong><ul>
<li>Assumes a PPO-Penalty (KL Divergence penalty) loss function</li>
<li>Rewrite gradient of PPO: $\nabla _{\theta} J _{ppo}(\theta) = \mathbb{E} _{\pi _{old}}[w _\pi(s,a) \nabla _\theta \log (a \vert s) Q^\pi _\lambda(s,a)]$<ul>
<li>$w _\pi(s,a) = \frac{\pi _\theta(a \vert s)}{\pi _{old}(a \vert s)}$</li>
<li>$Q^\pi _\lambda(s,a) = Q^\pi(s,a) + \lambda w _\pi(s,a)^{-1}$: second term from KL penalty</li>
</ul>
</li>
<li>PPO Gradient with Stein Control Variates + Reparameterization: $\nabla _{\theta} J _{ppo}(\theta) = \mathbb{E} _{\pi _{old}}[w _{\pi}(s,a) (\nabla _{\theta} \log \pi (a \vert s)(Q^\pi _{\lambda}(s,a) - \phi(s,a)) + \nabla _{\theta} f _{\theta}(s,a)\nabla_a \phi(s,a))]$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>Tested on MuJoCo continuous control environments</li>
<li>Use gaussian policies<ul>
<li>Estimate $\hat{V}^\pi$ seperately</li>
<li>$\psi$ either minimizes MSE with Q value or minimizes variance</li>
<li>3 Architecutures for $\psi$<ul>
<li>Linear: $\psi_w (s,a) = \langle \nabla_a q_w(a, \mu _\pi(s)), (a - \mu _\pi(s)) \rangle$</li>
<li>Quadratic: $\psi_w (s,a) = -(a - \mu_w(s))^T \Sigma^{-1}_2 (a - \mu_w(s))$<ul>
<li>$\mu_w(s)$ is a neural network</li>
<li>$\Sigma_w$: positive diagonal matrix independent of $s$</li>
</ul>
</li>
<li>MLP: Neural network that concatenates state and action before passing into hidden layer and then an output layer</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Comparing the Variance of Different Gradient Estimators</strong><ul><li>Using MLP or quadratic result in significantly lower variance than regular valuye function baselines</li></ul>
</li>
<li>
<strong>Comparison with Q-Prop Using TRPO for Policy Optimization</strong><ul>
<li>Stein control variates outperform Q-Prop on all tasks<ul><li>Suprising because Q-Prop uses on-policy and off-policy data whereas Stein’s only uses on-policy</li></ul>
</li>
<li>Quadratic requires more iterations than MLP to converge in practice</li>
</ul>
</li>
<li>
<strong>PPO with Different Control Variates</strong><ul>
<li>All three stein control variates outperform value function baselines</li>
<li>Quadratic and MLP outperform linear in general</li>
<li>Variance minimization usually works better with MLPs and MSE works better with Quadratic</li>
<li>Variance minimization + MLP usually the best in most cases</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-05-10T00:00:00+00:00">May 10, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
