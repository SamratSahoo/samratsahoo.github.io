<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about hash based counts">
<meta property="og:description" content="A paper about hash based counts">
<link rel="canonical" href="https://samratsahoo.com/2025/05/29/hash-based-counts">
<meta property="og:url" content="https://samratsahoo.com/2025/05/29/hash-based-counts">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-05-29T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-05-29T00:00:00+00:00","datePublished":"2025-05-29T00:00:00+00:00","description":"A paper about hash based counts","headline":"#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/05/29/hash-based-counts"},"url":"https://samratsahoo.com/2025/05/29/hash-based-counts"}</script><title> #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1611.04717">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Current RL algorithms use simple exploration strategies<ul>
<li>Uniform sampling</li>
<li>IID / Correlated Gaussian noise</li>
</ul>
</li>
<li>More advanced exploration strategies<ul>
<li>Using an ensemble of Q networks</li>
<li>Intrinsic motivation methods with pseudo-counts</li>
<li>Variational Maximation Exploration to maximize information gain</li>
</ul>
</li>
<li>Classical + theoretically justified strategies<ul>
<li>Counting state-action visitations</li>
<li>Upper confidence bounds to choose action that maximizes $\hat{r}(a_t) + \sqrt{\frac{2 \log t}{n(a_t)}}$ where $n(a_t)$ is the number of times $a_t$ was chosen and $\hat{r(a_t)}$ is the estimated reward</li>
<li>Model based interval estimation exploration bonus (MBIE-EB): Counts state-action pairs with table $n(s,a)$ and applies bonus $\frac{\beta}{\sqrt{n(s,a)}}$</li>
<li>Only practical for small state spaces</li>
</ul>
</li>
<li>Hash based counts<ul>
<li>Discretize state space using hash function</li>
<li>Apply bonus based on state-visitation count</li>
<li>Hash function needs to balance generalization across states and distinguish between states</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Methodology</strong><ul>
<li>
<strong>Notation</strong><ul><li>Assume standard RL MDP setting</li></ul>
</li>
<li>
<strong>Count-Based Exploration via Static Hashing</strong><ul>
<li>Discretize state space with hash function: $\phi : \mathcal{S} \rightarrow \mathbb{Z}$</li>
<li>Reward bonus: $r^+(s) = \frac{\beta}{\sqrt{n(\phi(s))}}$<ul>
<li>$\beta$: Bonus coefficient</li>
<li>$n(\phi(s))$ is increased at every step by 1</li>
<li>Agent trained with reward $r+ r^+$</li>
</ul>
</li>
<li>Performance of the method depends on hash function<ul><li>Similar states should be merged; distant states counted seperately</li></ul>
</li>
<li>Locally sensitive hashing converts high dimensional data to hash codes<ul><li>SimHash measures simiarlity by angular distance; retrives binary code<ul><li>$\phi(s) = sgn(Ag(s)) \in set(-1, 1)^k$<ul><li>$g: \mathcal{S} \rightarrow \mathbb{R}^D$: optional preprocessing function<ul><li>$A: k x D$ matrix with IID entries from gaussian<ul><li>k: controls granularity (larger values = fewer collisions)</li></ul>
</li></ul>
</li></ul>
</li></ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Count-Based Exploration via Learned Hashing</strong><ul>
<li>Hard to cluster with SimHash from raw pixels</li>
<li>Instead use an autoencoder to learn hashcodes<ul>
<li>Input state</li>
<li>Hidden layer contains $D$ sigmoid units which round activations to closest binary number<ul>
<li>Issue is same hashcodes for two different units can be reconstructed perfectly</li>
<li>Can’t backprop through rounding function either</li>
<li>Instead inject uniform noise into sigmoid: $\mathcal{U}(-a, a)$<ul>
<li>When $a &gt; \frac{1}{4}$, autoencoder reconstructs distinct state inputs</li>
<li>Learns to spread sigmoid outputs such that $\vert b(s_i) - b(s_j) \vert &gt; \epsilon$ to counteract injected noise</li>
</ul>
</li>
<li>Loss function overcollected states: $L((s_n) _{n=1}^N) = -\frac{1}{N}\sum _{n=1}^N[\log p(s_n) - \frac{\lambda}{K} \sum _{i=1}^D min((1 - b_i(s_n))^2, b_i(s_n)^2)]$<ul>
<li>$p(s_n)$: Autoencoder output</li>
<li>NLL term + term that presures binary code layer to take on binary values<ul><li>Without this, there might be a state where sigmoid not used, causing value to fluctuate around 0.5</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Binary code is large to reconstruct the input; downsample the binary code to a lower dimensional space via SimHash</li>
<li>Need mapping from state to code to be consistent<ul><li>Difficult due to non-stationary training data<ul>
<li>Either downsample binary code</li>
<li>Or slow the training process down</li>
</ul>
</li></ul>
</li>
<li>Also need code to be sufficiently unique<ul>
<li>Done by the 2nd term of the loss function + by saturating sigmoid units</li>
<li>Saturating sigmoids causes loss gradients to be close to 0 $\rightarrow$ minimal change in code</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Continuous Control</strong><ul>
<li>SimHash is on par with VIME on MountainCar, worse than VIME on HalfCheetah, and better than VIME on SwimmerGather</li>
<li>Capable of reaching the goal in all environments</li>
</ul>
</li>
<li>
<strong>Arcade Learning Environment</strong><ul>
<li>Compare autoencoder-based learned hash code with Basic Abstraction of ScreenShots (BASS)<ul>
<li>BASS is a static preprocessing with hand designed feature transformations for images</li>
<li>They modify BASS to divide RGB screen into square cells, compute average intensity of each channel, and assign resulting values to bin that partition the range<ul>
<li>$feature(i,j,z) = \lfloor \frac{B}{255C^2} \sum _{(x,y) \in cell(i,j)} I(x,y,z) \rfloor$<ul>
<li>$C$: cell size</li>
<li>$B$: number of bins</li>
<li>$(i,j)$: cell location</li>
<li>$(x,y)$: pixel location</li>
<li>$z$: channel</li>
</ul>
</li>
<li>Resulting feature converted to hashcode with SimHash</li>
</ul>
</li>
</ul>
</li>
<li>BASS performs better than baseline and autoencoder performs better than BASS; autoencoder is SOTA</li>
<li>Static/adaptive preprocessing with BASS could be good for a good hash function</li>
<li>Does not achieve SOTA on all games because TRPO doesn’t reuse off-policy experience<ul><li>Less efficient in harnessing sparse rewards</li></ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-05-29T00:00:00+00:00">May 29, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
