<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Count-Based Exploration with Neural Density Models">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about pixel cnn pseudocounts">
<meta property="og:description" content="A paper about pixel cnn pseudocounts">
<link rel="canonical" href="https://samratsahoo.com/2025/05/28/pixelcnn-pseudocounts">
<meta property="og:url" content="https://samratsahoo.com/2025/05/28/pixelcnn-pseudocounts">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-05-28T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Count-Based Exploration with Neural Density Models">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-05-28T00:00:00+00:00","datePublished":"2025-05-28T00:00:00+00:00","description":"A paper about pixel cnn pseudocounts","headline":"Count-Based Exploration with Neural Density Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/05/28/pixelcnn-pseudocounts"},"url":"https://samratsahoo.com/2025/05/28/pixelcnn-pseudocounts"}</script><title> Count-Based Exploration with Neural Density Models - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Count-Based Exploration with Neural Density Models</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1703.01310">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Exploration = reducing uncertainty about the environment</li>
<li>Bayesian methods work but are intractable in large state spaces</li>
<li>Count based approaches aren’t applicable for function approximation<ul><li>Pseudo-counts are a generalization using a density model<ul>
<li>$\hat{N}(x) = \rho(x)\hat{n}(x)$<ul>
<li>$\rho(x)$: density model</li>
<li>$\hat{n}(x)$: total pseudo-count computed on model’s recoding probability (probabibility of seeing $x$ after being trained on an $x$)</li>
</ul>
</li>
<li>Assumptions on density model<ul>
<li>Learning positive (probability of states seen should increase with training)</li>
<li>Should be trained online</li>
<li>Model step size should decay at rate of $n^{-1}$</li>
</ul>
</li>
<li>Mixing MC and Q learning update rule allowed fast propagation of exploration bonuses</li>
</ul>
</li></ul>
</li>
<li>PixelCNN<ul>
<li>With pseudo-counts, we compute $\rho(x), \rho’(x)$ which is expensive<ul><li>PixelCNN uses a expressive but simplified architecture</li></ul>
</li>
<li>Need to be careful with online training because it can cause overfitting + catastrophic forgetting</li>
<li>Pseudo counts requires decaying learning rate<ul><li>Optimization of neural models are constrained step size - violating it causes worse effectiveness + stability of training</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Background</strong><ul>
<li>
<strong>Pseudo-Count and Prediction Gain</strong><ul>
<li>See <a href="https://samratsahoo.com/2025/05/23/cts-based-pseudocounts">notes</a> on pseudo-count for notation</li>
<li>Prediction Gain: $PG_n(x) = \log \rho_n’(x) - \log \rho_n(x)$<ul><li>Learning positive means $PG \geq 0$</li></ul>
</li>
<li>Pseudo-count: $\hat{N_n}(x) = \frac{\rho_n(x) (1 - \rho_n’(x))}{\rho_n’(x)- \rho_n(x)} = \hat{n}\rho_n(x)$<ul><li>Approximated via prediction gain of density model: $\hat{N}_n(x) \approx (e^{PG_n(x)} -1)^{-1}$<ul><li>Used as an exploration bonus: $r^+ (x) = \hat{N}_n(x)^{-1/2}$</li></ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Density Models for Images</strong><ul>
<li>Original pseudo-count paper model based on context tree switches (CTS)<ul><li>Takes in an image and assigns a probability based on product of filters where filters trained on past images</li></ul>
</li>
<li>PixelCNN<ul><li>Generative model that models pixel probabilities conditioned on previous pixels</li></ul>
</li>
</ul>
</li>
<li>
<strong>Multi-Step RL Methods</strong><ul>
<li>Multistep methods interpolate between SARSA and MC methods</li>
<li>Can also use a mixed monte carlo update: $Q(x,a) = Q(x,a) + \alpha[(1 - \beta)\delta(x,a) + \beta\delta _{MC}(x,a)]$</li>
<li>Retrace($\lambda$): Uses a product of truncated importance sampling ratios to replace $\deta$ with error term<ul>
<li>$\delta _{RETRACE} = \sum _{t=0}^\infty \gamma^t (\prod _{s=1}^t c_s)\delta (x_t, a_t)$</li>
<li>Mixes TD errors from all future timesteps</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Using PixelCNN for Exploration</strong><ul>
<li>Assumptions of density model<ul>
<li>Trained online</li>
<li>Decay at rate of $1/n$</li>
<li>Learning-positive</li>
</ul>
</li>
<li>Neural density model constraints<ul>
<li>Drawn randomly from dataset</li>
<li>Fixed learning rate schedule</li>
<li>Computationally lightweight to compute prediction gain (2 evaluations + 1 update)</li>
</ul>
</li>
<li>
<strong>Designing a Suitable Density Model</strong><ul>
<li>16 feature maps + 2 residual blocks</li>
<li>Images downsized to 42x42 + quantized to 3 bit grayscale</li>
</ul>
</li>
<li>
<strong>Training the Density Model</strong><ul>
<li>Trained completely online on sequence of experiences</li>
<li>Can train on temporally correlated states (just as good as random sequence)<ul>
<li>Also allows $\rho_n’ = \rho _{n+1}$ so the model update doesn’t need to be reverted</li>
<li>Optimizers like RMSProp track mean / variance<ul><li>Training from minibatches on top of temporal correlation may show different statistical characteristics, leading to unstable training</li></ul>
</li>
</ul>
</li>
<li>Used a constant learning rate - was more robust</li>
</ul>
</li>
<li>
<strong>Computing the Pseudo-Count</strong><ul>
<li>Learning rate schedule cannot be modified without deteriorating model performance</li>
<li>Replace $PG_n$ with $c_n \cdot PG_n$ with a decay sequence $c_n$<ul><li>Best decay sequence: $c_n = c \cdot n^{\frac{1}{2}}$</li></ul>
</li>
<li>Difficult to ensure learning positiveness for neural nets<ul><li>Threshold the $PG$ value to 0</li></ul>
</li>
<li>Compute pseudo-count: $\hat{N}<em>n(x) = (exp(c \cdot n^{-1/2} \cdot (PG_n(x))</em>+)-1)^{-1}$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Exploration in Atari 2600 Games</strong><ul>
<li>
<strong>DQN with PixelCNN Exploration Bonus</strong><ul>
<li>PixelCNN provides an exploration bonus to a DQN agent</li>
<li>Used a mixed monte carlo update</li>
<li>Compared DQN-PixelCNN to DQN and DQN-CTS<ul>
<li>CTS and PixelCNN both outperform the baseline agent on Montezuma</li>
<li>PixelCNN is SOTA on other hard exploration games</li>
<li>PixelCNN outperforms CTS on 52/57 games</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>A Multi-Step RL Agent with PixelCNN</strong><ul>
<li>Combined PixelCNN with Reactor<ul>
<li>Only perform updates on 25% of steps to reduce computational burden</li>
<li>Prediction gain decay is $0.1n^{1/2}$</li>
</ul>
</li>
<li>PixelCNN improves baseline reactor which is an improvement on baseline DQN</li>
<li>On hard exploration games, Reactor can’t take advantage of the full exploration bonus<ul>
<li>Across long horizons in sparse reward settings, propagation of reward signal is crucial</li>
<li>Reactor relies on $\lambda$ and the trucated importance sampling ratio which discards off-policy trajectories $\rightarrow$ cautious learning<ul><li>Cautious learning causes it to not take advantage of the bonus</li></ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Quality of the Density Model</strong><ul>
<li>PixelCNN has lower and smoother prediction gain (lower variance)<ul><li>Shows pronounced peaks at infrequent states</li></ul>
</li>
<li>Per step prediction gain never vanishes because step size isn’t decaying<ul><li>Model reamins mildly suprised by significant state changes</li></ul>
</li>
</ul>
</li>
<li>
<strong>Importance of the Monte Carlo Return</strong><ul>
<li>We need the learning algorithm to understand the transient nature of exploration bonuses<ul><li>Mixed monte carlo updates help do this</li></ul>
</li>
<li>MMC also helps in long horizon sparse settings where rewards are far apart</li>
<li>Monte carlo return on-policy increases variance in learning algorithm $\rightarrow$ prevents convergence when training off-policy</li>
<li>MMC speeds up training + improves final performance when used in PixelCNN over base DQN on some games</li>
<li>MMC can also hurt performance in some games when using PixelCNN over base DQN</li>
<li>MMC + PixelCNN bonuses have a compounding effect</li>
<li>On hard exploration games, DQN fails completely but PixelCNN + DQN does well<ul><li>Reward bonus creates denser rewards<ul><li>Because bonuses are temporary, agent needs to learn the policy faster than 1-step methods $\rightarrow$ MMC is the solution!</li></ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Pushing the Limits of Intrinsic Motivation</strong><ul>
<li>In experiments, prediction gain was clamped to avoid adverse effects on easy exploration games</li>
<li>Increasing this scale leads to stronger exploration on hard exploration games<ul>
<li>Reaches peak performance rapidly</li>
<li>Deteriorates stability of training + long term performance too</li>
<li>With reward clipping, exploration bonus becomes essentially constant (no prediction gain decay) $\rightarrow$ no longer useful signal for intrinsic motivation</li>
</ul>
</li>
<li>Training on exploration bonus only is another way to get a high performing agent!</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-05-28T00:00:00+00:00">May 28, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
