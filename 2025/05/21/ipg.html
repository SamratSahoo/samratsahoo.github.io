<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about interpolated policy gradient">
<meta property="og:description" content="A paper about interpolated policy gradient">
<link rel="canonical" href="https://samratsahoo.com/2025/05/21/ipg">
<meta property="og:url" content="https://samratsahoo.com/2025/05/21/ipg">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-05-21T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-05-21T00:00:00+00:00","datePublished":"2025-05-21T00:00:00+00:00","description":"A paper about interpolated policy gradient","headline":"Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/05/21/ipg"},"url":"https://samratsahoo.com/2025/05/21/ipg"}</script><title> Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1706.00387">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>On-Policy: Stable but data inefficient</li>
<li>Off-Policy: Data efficient but unstable / hard to use</li>
<li>Combining on-policy and off-policy<ul>
<li>Mix a ratio of on and off-policy gradients / update steps (ACER, PGQL)<ul><li>No theoretical bounds on error of off-policy updates</li></ul>
</li>
<li>Off-policy Q critic trained and used as control variate to reduce on-policy gradient variance (Q-Prop)<ul><li>Policy updates don’t use off-policy data</li></ul>
</li>
</ul>
</li>
<li>Interpolated Policy Gradient: interpolates between on-policy and off-policy learning<ul><li>Biased method but bias is bounded</li></ul>
</li>
</ul>
</li>
<li>
<strong>Preliminaries</strong><ul>
<li>
<strong>On-Policy Likelihood Ratio Policy Gradient</strong><ul><li>Likelihood Policy Gradient: $\nabla _{\theta} J(\theta) = \mathbb{E} _{\rho^\pi, \pi}[\nabla _\theta \log \pi _{\theta}(a_t \vert s_t) \hat{A}(s_t, a_t)]$<ul>
<li>Unbiased</li>
<li>High variance</li>
<li>Sample insensitive</li>
</ul>
</li></ul>
</li>
<li>
<strong>Off-Policy Deterministic Policy Gradient</strong><ul>
<li>Value and Policy Weights:<ul>
<li>Value: $w \leftarrow argmin \mathbb{E} _{\beta}[(Q_w(s_t, a_t) - y_t)^2]$</li>
<li>Policy: $\theta \leftarrow argmin \mathbb{E} _{\beta}[(Q_w(s_t, \mu _\theta(s_t)))]$<ul><li>$\beta$: Behavior policy (i.e., exploration buffer)</li></ul>
</li>
</ul>
</li>
<li>Deterministic Policy Gradient: $\nabla _{\theta} J(\theta) \approx \mathbb{E} _{\rho^\beta}[\nabla _{\theta}Q_w(s_t, \mu _{\theta}(s_t))]$<ul><li>Biased due to imperfect estimator in $Q_w$ + off-policy sampling from $\beta$ - unbounded bias<ul><li>Causes off-policy to be less stable</li></ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Off-Policy Control Variate Fitting</strong><ul><li>We can use baselines to reduce the variance of a monte carlo estimator<ul>
<li>Q-Prop used a first order taylor expansion of $Q_w$</li>
<li>Gradient: $\nabla _{\theta} J(\theta) = \mathbb{E} _{\rho^\pi, \pi}[\nabla _{\theta} \log \pi _{\theta}(a_t \vert s_t)(\hat{Q}(s_t, a_t) - \tilde{Q}_w (s_t, a_t))] + \mathbb{E} _{\rho^\pi}[\nabla _{\theta}Q_w(s_t, \pi _{\theta}(s_t))]$<ul>
<li>Combines likelihood ratio + deterministic policy gradient</li>
<li>Lower variance + is stable</li>
<li>Uses only on-policy samples for estimating policy gradient</li>
</ul>
</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Interpolated Policy Gradient</strong><ul>
<li>Mixes likelihood gradient with deterministic gradient<ul><li>$\nabla _\theta J(\theta) \approx (1-\nu) \mathbb{E} _{\rho^\pi, \pi}[\nabla _\theta \log \pi _{\theta}(a_t \vert s_t) \hat{A}(s_t, a_t)] + \nu \mathbb{E} _{\rho^\beta}[\nabla _{\theta}\bar{Q}_w^\pi(s_t)]$<ul>
<li>Biased from off-policy sampling + inaccuracies in $Q_w$</li>
<li>$\bar{Q}_w^\pi$: state-value function</li>
</ul>
</li></ul>
</li>
<li>
<strong>Control Variates for Interpolated Policy Gradient</strong><ul>
<li>$\nabla _{\theta}J(\theta) \approx (1-\nu) \mathbb{E} _{\rho^\pi, \pi}[\nabla _\theta \log \pi _{\theta}(a_t \vert s_t) \hat{A}(s_t, a_t)] + \nu \mathbb{E} _{\rho^\beta}[\nabla _{\theta}\bar{Q}_w^\pi(s_t)]$<ul><li>Biased approximation from IPG</li></ul>
</li>
<li>$= (1-\nu) \mathbb{E} _{\rho^\pi, \pi}[\nabla _\theta \log \pi _{\theta}(a_t \vert s_t) \hat{A}(s_t, a_t) - A_w^\pi(s_t, a_t)] + (1 - \nu)\mathbb{E} _{\rho^\pi}[\nabla _{\theta}\bar{Q}_w^\pi(s_t)] + \nu \mathbb{E} _{\rho^\beta}[\nabla _{\theta}\bar{Q}_w^\pi(s_t)]$</li>
<li>$\approx (1-\nu) \mathbb{E} _{\rho^\pi, \pi}[\nabla _\theta \log \pi _{\theta}(a_t \vert s_t) \hat{A}(s_t, a_t) - A_w^\pi(s_t, a_t)] + \mathbb{E} _{\rho^\beta}[\nabla _{\theta}\bar{Q}_w^\pi(s_t)]$<ul>
<li>Replaces $\rho^\pi$ in control variate correction term with $\rho^\beta$</li>
<li>Adds bias if $\beta \neq \pi$</li>
<li>Gradients proportional to Residual likelihood ratio gradient: $\hat{A}(s_t, a_t) - A_w^\pi(s_t, a_t)$<ul><li>Difference between advantages of on-policy and off policy</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Relationship to Prior Policy Gradient and Actor-Critic Methods</strong><ul>
<li>REINFORCE: $\nu = 0$, no control variate</li>
<li>Q-Prop: $\beta = \pi, \nu = 0$, no control variate</li>
<li>DDPG: $\nu = 1$</li>
<li>PGQL: $\beta \neq \pi$, no control variate</li>
<li>Algorithm:<ul>
<li>Initialize $Q_w$, stochastic policy $\pi _{\theta}$, replay buffer</li>
<li>Repeat until convergence:<ul>
<li>Rollout $\pi _{\theta}$ to collect batch of episodes</li>
<li>Fit $Q_w$ using replay buffer and $\pi$</li>
<li>Fit state-value function using batch of epsiodes (E) with timesteps (T)</li>
<li>Compute advantage estimate using batch data + state-value function</li>
<li>If using control variate<ul>
<li>Compute critic advantage estimate ($\bar{A} _{t,e}$) using batch data, $Q_w$, and $\pi _{\theta}$</li>
<li>Compute + center learning signals: $l _{t,e} = \hat{A} _{t,e} - \bar{A} _{t,e}$ + set $b=1$ else:</li>
<li>Center learning signals: $l _{t,e} = \hat{A} _{t,e}$ + set $b=\nu$</li>
</ul>
</li>
<li>Multiply $l _{t,e}$ by $\nu$</li>
<li>Sample data (M samples) from replay buffer or batch data (depending on behavior policy)</li>
<li>Compute gradient $\nabla _{\theta} J(\theta) \approx \frac{1}{ET} \sum_e \sum_t \nabla _\theta \log \pi _{\theta}(a _{t,e} \vert s _{t,e})l _{t,e} + \frac{b}{M} \sum_m \nabla _{\theta}\bar{Q}_w^\pi(s_m)$</li>
<li>Update policy using gradient</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>$\nu = 1$: Actor-Critic methods</strong><ul>
<li>Policy can be deterministic</li>
<li>Learning can be done completely off-policy</li>
<li>Bias from off-policy sampling increases as total variation or KL divergence between $\beta$ and $\pi$ increases</li>
<li>Actor-Critic with on-policy exploration could be more reliable</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Theoretical Analysis</strong><ul>
<li>
<strong>$\beta \neq \pi, \nu = 0$: Policy Gradient with Control Variate and Off-Policy Sampling</strong><ul>
<li>When plugging in $\beta \neq \pi, \nu = 0$ to gradient approximation, we get equation similar to Q-Prop<ul><li>$\approx \mathbb{E} _{\rho^\pi, \pi}[\nabla _\theta \log \pi _{\theta}(a_t \vert s_t) \hat{A}(s_t, a_t) - A_w^\pi(s_t, a_t)] + \mathbb{E} _{\rho^\beta}[\nabla _{\theta}\bar{Q}_w^\pi(s_t)]$<ul><li>Main difference: allows utilizing off-policy data for updating policy too</li></ul>
</li></ul>
</li>
<li>Let $\tilde{J}(\pi, \tilde{\pi})$ be a local approximation to $J(\pi)$<ul>
<li>$J(\pi) = J(\tilde{\pi}) + \mathbb{E} _{\rho^\pi, \pi}[A^{\tilde{\pi}}(s_t, a_t)] \approx J(\tilde{\pi}) + \mathbb{E} _{\rho^{\tilde{\pi}}, \pi}[A^{\tilde{\pi}}(s_t, a_t)] = \tilde{J}(\pi, \tilde{\pi})$<ul>
<li>$\tilde{\pi}$: Usually policy at iteration k</li>
<li>$\pi$: Usually policy at iteration k+1</li>
</ul>
</li>
<li>Approximate objective: $\tilde{J}^{\beta, \nu = 0, CV} (\pi, \tilde{\pi}) = J(\tilde{\pi}) + \mathbb{E} _{\rho^\tilde{\pi}, \pi}[A^{\tilde{\pi}}(s_t, a_t) - A_w^{\tilde{\pi}}(s_t, a_t)] + \mathbb{E} _{\rho^\beta}[\bar{A}_w^{\pi, \tilde{\pi}}(s_t)] \approx \tilde{J}(\pi, \tilde{\pi})$<ul><li>$\bar{A}_w^{\pi, \tilde{\pi}}(s_t) = \mathbb{E} _{\pi}[Q_w(s_t, \cdot)] - \mathbb{E} _{\tilde{\pi}}[Q_w(s_t, \cdot)]$</li></ul>
</li>
</ul>
</li>
<li>KL divergence between policies can be bounded: $\vert \vert J(\pi) - \tilde{J}^{\beta, \nu = 0, CV} \vert \vert_1 \leq 2 \frac{\gamma}{(1 - \gamma)^2}(\epsilon \sqrt{D _{KL}^{max}(\tilde{\pi}, beta)} + \zeta \sqrt{D _{KL}^{max}(\tilde{\pi}, \tilde{\pi})})$ $\epsilon = max_s \vert \bar{A}_w^{\pi, \tilde{\pi}}(s) \vert$ $\zeta = max_s \vert \bar{A}^{\pi, \tilde{\pi}}(s) \vert$<ul>
<li>First term bounds bias from off-policy sampling using KL between $\tilde{\pi}$ and $\beta$</li>
<li>Second term confirms $\tilde{J}^{\beta, \nu = 0, CV}$ is a local approximation around $\pi$</li>
<li>Works well with policy gradient methods that constrain KL divergence (TRPO, NPG, etc.)</li>
</ul>
</li>
<li>
<strong>Monotonic Policy Improvement Guarantee</strong><ul><li>IPG does in fact have theoretical guarantees on policy improvement<ul>
<li>Impractical to implement</li>
<li>IPG with trust region updates approximates this monotonicity</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>General Bounds on the Interpolated Policy Gradient</strong><ul><li>Let $\delta = max _{s,a} \vert A^{\tilde{\pi}(s,a)} - A_w^{\tilde{\pi}(s,a)}\vert$, $\epsilon = max_s \vert \bar{A}_w^{\pi, \tilde{\pi}}(s) \vert$, $\zeta = max_s \vert \bar{A}^{\pi, \tilde{\pi}}(s) \vert$<ul>
<li>Without Control Variate:<ul>
<li>Local Approximation: $\tilde{J}^{\beta, \nu} (\pi, \tilde{\pi}) = J(\tilde{\pi}) + (1 - \nu)\mathbb{E} _{\rho^{\tilde{\pi}, \pi}}[\hat{A}^{\tilde{\pi}}] + \nu \mathbb{E} _{\rho^{\beta}}[\bar{A}_w^{\pi, \tilde{\pi}}]$</li>
<li>Bias Bound: $\vert \vert J(\pi) - \tilde{J}^{\beta, \nu}(\pi, \tilde{\pi}) \vert \vert_1 \leq \frac{\nu \delta}{1-\gamma} + 2 \frac{\gamma}{(1 - \gamma)^2}(\nu\epsilon \sqrt{D _{KL}^{max}(\tilde{\pi}, beta)} + \zeta \sqrt{D _{KL}^{max}(\tilde{\pi}, \tilde{\pi})})$</li>
</ul>
</li>
<li>With Control Variate:<ul>
<li>Local Approximation: $\tilde{J}^{\beta, \nu, CV} (\pi, \tilde{\pi}) = J(\tilde{\pi}) + (1 - \nu)\mathbb{E} _{\rho^{\tilde{\pi}, \pi}}[\hat{A}^{\tilde{\pi}} - A_w^{\tilde{\pi}}] + \nu \mathbb{E} _{\rho^{\beta}}[\bar{A}_w^{\pi, \tilde{\pi}}]$</li>
<li>Bias Bound: $\vert \vert J(\pi) - \tilde{J}^{\beta, \nu, CV}(\pi, \tilde{\pi}) \vert \vert_1 \leq \frac{\nu \delta}{1-\gamma} + 2 \frac{\gamma}{(1 - \gamma)^2}(\epsilon \sqrt{D _{KL}^{max}(\tilde{\pi}, beta)} + \zeta \sqrt{D _{KL}^{max}(\tilde{\pi}, \tilde{\pi})})$</li>
</ul>
</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>$\beta \neq \pi, \nu = 0$, with the control variate</strong><ul>
<li>A variant of this method gives us monotonic convergence guarantees under certain conditions</li>
<li>When using off-policy (random) sampling from replay buffer, we get faster convergence than solely on-policy<ul><li>Decorrelates the samples, allowing for more stable gradients</li></ul>
</li>
<li>Replay buffer random sampling provides improvement on top of Q-Prop<ul>
<li>These samples are not the same as DDPG / DQN samples</li>
<li>They are samples from within a trust region update, allowing greater regularity but less exploration<ul><li>Key to good performance</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>$\beta = \pi, \nu = 1$</strong><ul>
<li>On-policy sampling + on-policy version of deterministic actor-critic<ul><li>More similar to TRPO or Q-Prop than DDPG</li></ul>
</li>
<li>DDPG gets stuck while IPG monotonically improves</li>
<li>Running IPG with Ornstein–Uhlenbeck exploration (what DDPG does) degrades performance<ul><li>Bias upper bounds become larger<ul>
<li>In the off-policy case, difference between $\pi, \beta$ was bounded by trust region which bounded bias</li>
<li>in this case, the off-policy samples from exploration result in excessive bias<ul><li>More effective to use on-policy exploration with bounded policy updates than design heurstic exploration rules</li></ul>
</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>General Cases of Interpolated Policy Gradient</strong><ul>
<li>In general, $\nu = 0.2$ performed better than Q-Prop, TRPO, or other actor-critic methods consistently</li>
<li>$\nu = 0$ is Q-Prop and TRPO (depending if control variate is used)</li>
<li>$\nu = 1$ is a variant of actor-critic</li>
<li>Best performing cases are ones that interpolate between actor-critic and policy-gradient</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-05-21T00:00:00+00:00">May 21, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
