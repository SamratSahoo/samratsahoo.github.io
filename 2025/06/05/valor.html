<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Variational Option Discovery Algorithms">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about valor">
<meta property="og:description" content="A paper about valor">
<link rel="canonical" href="https://samratsahoo.com/2025/06/05/valor">
<meta property="og:url" content="https://samratsahoo.com/2025/06/05/valor">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-06-05T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Variational Option Discovery Algorithms">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-06-05T00:00:00+00:00","datePublished":"2025-06-05T00:00:00+00:00","description":"A paper about valor","headline":"Variational Option Discovery Algorithms","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/06/05/valor"},"url":"https://samratsahoo.com/2025/06/05/valor"}</script><title> Variational Option Discovery Algorithms - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Variational Option Discovery Algorithms</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1807.10299">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Reward-free Option Discovery: Discover skills without rewards</li>
<li>Variational option discovery: Option discovery based on variational inference<ul>
<li>Policy: Encoder that translates context from noise distribution into trajectories</li>
<li>Decoder: Recovers context from trajectories<ul><li>Context: Random vectors that have no association with trajectories but become associated with trajectories through training</li></ul>
</li>
<li>VIC and DIAYN are variations of this algorithm but use individual states instead of trajectories</li>
</ul>
</li>
<li>Variational Autoencoding Learning of Options by Reinforcement (VALOR): Encourages learning dynamics modes instead of goal-attaining ones (i.e., move in circle vs go to specific state)<ul><li>Uses curriculum learning (contexts increase )</li></ul>
</li>
</ul>
</li>
<li>
<strong>Variational Option Discovery Algorithms</strong><ul>
<li>Policy conditioned on state ($s_t$) and context ($c$)<ul>
<li>Context specifies a specific skill</li>
<li>Context arbitrarily assigned/discovered during training</li>
</ul>
</li>
<li>Context sampled from noise distribution ($G$)<ul><li>Encoded into a trajectory ($\tau$) by a policy</li></ul>
</li>
<li>Context then decoded from trajectory using a decoder ($D$)<ul>
<li>If $\tau$ unique to $c$, decoder gives high probability to $c$ + policy should be reinforced</li>
<li>Can apply supervised learning to $c$</li>
</ul>
</li>
<li>Encourage exploration via entropy regularization</li>
<li>Training objective: $max _{\pi, D}\mathbb{E} _{c \sim G}[\mathbb{E} _{\tau \sim \pi, c}[\log P_D(c \vert \tau)] + \beta \mathcal{H}(\pi \vert c)]$<ul>
<li>$P_D$: Distribution of contexts from decoder</li>
<li>Corresponds to $\beta$-VAE objective<ul>
<li>$c$ is the data</li>
<li>$\tau$ is the latent representation</li>
<li>$\pi$ and the MDP represent the encoder</li>
<li>$D$ represents the decoder</li>
<li>Entropy regularization represents KL divergence term</li>
</ul>
</li>
</ul>
</li>
<li>Algorithm<ul>
<li>Generate initial policy $\pi _{\theta_0}$, decoder $D _{\phi_0}$</li>
<li>For $k = 1, 2 \dots$<ul>
<li>Sample $c \sim G$ and roll out a trajectory</li>
<li>Update policy using any RL algorithm to maximize training objective above</li>
<li>Update decoder to maximize $\mathbb{E} [\log P_D(c \vert \tau)]$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Connections to Prior Work</strong><ul>
<li>Variational Intrinsic Control<ul>
<li>VIC optimizes variational lower bound of mutual information between context and final state conditioned on initial state</li>
<li>Differs from VALOR because<ul>
<li>$G$ can be optimized</li>
<li>$G$ depends on initial state</li>
<li>$G$ is entropy regularized</li>
<li>$\pi$ is not entropy regularized</li>
<li>Decoder only looks at first and last state of the trajectory</li>
</ul>
</li>
<li>VIC is a form of VALOR<ul><li>Keep $G$ fixed + let $\log P_D(c \vert \tau) = \log P_D(c \vert S_T)$ with no entropy regularization on policy</li></ul>
</li>
</ul>
</li>
<li>Diversity is all you need<ul>
<li>Optimizes variational lower bound of mutual information between context and every state in a trajectory</li>
<li>Minimizes mutual information between actions and contexts conditioned on states</li>
<li>Maximizes entropy of mixture policy over contexts</li>
<li>DIAYN is a form of VALOR with $\log P_D(c \vert \tau) = \sum _{t = 0}^T \log P_D(c \vert s_t)$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>VALOR</strong><ul>
<li>Optimizes Equation 2 with following caveats<ul>
<li>Decoder never sees actions; if decoder could see actions, it could communicate signal contex through actions, ignoring environment<ul><li>Forces agent to manipulate environment to communicate with decoder</li></ul>
</li>
<li>Decoder does not decompose as sum of per-timestep computations (unlike DIAYN)<ul><li>Inhibits decoder’s ability to distinguish between behaviors which share states</li></ul>
</li>
</ul>
</li>
<li>Implemented with a recurrent architecture (bidirectional LSTM)<ul>
<li>Recurrent layer is of length 11 (11 evenly-spaced points in trajectory)<ul><li>Efficient and encoders only low-frequency behaviors instead of high frequency ones (i.e., jitters)</li></ul>
</li>
<li>Compute differences between every k states<ul><li>Encodes the prior that agent should be moving (staying in the same state will be indistinguishable to decoder)</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Curriculum Approach</strong><ul>
<li>VIC and DIAYN use discrete contexts $c \sim Uniform(K)$<ul><li>Worked poorly for large $K$</li></ul>
</li>
<li>Instead, start with a small $K$ where learning was easy and gradually increase it over time<ul>
<li>$\mathbb{E}[\log P_D(c \vert \tau)]$ should pass a threshold</li>
<li>Increase K: $K \leftarrow min(int(1.5 \cdot K + 1), K _{max})$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experimental Setup</strong><ul>
<li>Test Environment: 2D Point Agent, HalfCheetah, Swimmer, a modified Ant environment, Dexterous Hand, and humanoid toddler</li>
<li>Implementation: implement VALOR, VIC, and DIAYN with vanilla policy gradient</li>
<li>Training Techniques: Curriculum generation (above equation) and context embeddings<ul><li>Context Embeddings: Let agent learn own embedding vector for each context</li></ul>
</li>
</ul>
</li>
<li>
<strong>Results</strong><ul>
<li>Using embeddings improves speed + stability of training</li>
<li>Training with a uniform distribution becomes more challenging as $K$ increases<ul><li>Curriculum learning helps alleviate this difficulty</li></ul>
</li>
<li>Variational option discovery methods find locomotion gaits in variety of speeds + directions in mujoco environments</li>
<li>DIAYN has tendency to learn behaviors to atain a target state</li>
<li>Curriculum learning doesn’t increase diversity of behaviors found<ul><li>Does make distribution of scores more consistent across seeds</li></ul>
</li>
<li>Hand + Toddler Environment: Optimizing hand was easy, toddler was not<ul><li>Learned very few behaviors + were unnatural beahviors<ul><li>Due to limitations of information theoretic RL - does not have strong priors on natural behavior</li></ul>
</li></ul>
</li>
<li>Was able to learn hundreds of behaviors on the point environment<ul><li>Correlated with decoder capacity (higher capacity = more easily overfit to small differences that would otherwise be undetectable)</li></ul>
</li>
<li>Mode Interpolation: Interpolate between context embeddings<ul><li>Smooth interpolated behaviors achieved $\rightarrow$ suggests training learns universal policies</li></ul>
</li>
<li>Downstream Tasks: Used a VALOR policy for Ant and used it as a lower level policy for Ant maze<ul>
<li>Only upper level policy trained</li>
<li>Worked as well as a hierarchial policy (with random network for lower level policy) trained from scratch and non-hierarchial policy trained from scratch</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-06-05T00:00:00+00:00">June 5, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
