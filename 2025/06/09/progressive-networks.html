<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Progressive Neural Networks">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about progressive networks">
<meta property="og:description" content="A paper about progressive networks">
<link rel="canonical" href="https://samratsahoo.com/2025/06/09/progressive-networks">
<meta property="og:url" content="https://samratsahoo.com/2025/06/09/progressive-networks">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-06-09T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Progressive Neural Networks">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-06-09T00:00:00+00:00","datePublished":"2025-06-09T00:00:00+00:00","description":"A paper about progressive networks","headline":"Progressive Neural Networks","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/06/09/progressive-networks"},"url":"https://samratsahoo.com/2025/06/09/progressive-networks"}</script><title> Progressive Neural Networks - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Progressive Neural Networks</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1606.04671">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Fine tuning: model is trained on source domain and output layers trained on target layers<ul><li>Doesn’t work well when there are multiple tasks<ul>
<li>Which model should initialize subsequent models</li>
<li>Need to support transfer learning without catastrophic forgetting</li>
</ul>
</li></ul>
</li>
<li>Distillation is a solution but requires persistent data for all tasks</li>
<li>Progressive Networks: Architecture with support for transfer across tasks<ul>
<li>Retain pool of pretrained models throughout training</li>
<li>Learn lateral connections to extract features for new tasks</li>
<li>Accumulate experience + immune to catastrophic forgetting</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Progressive Networks</strong><ul>
<li>Instantiates new neural network for each task being solved<ul><li>Transfer enabled via lateral connections to prior layers</li></ul>
</li>
<li>Architecture:<ul>
<li>$L$ layers<ul><li>Hidden activations in layer $i$: $h^{(1)}_i \in \mathbb{R}^{n_i}$<ul>
<li>$n_i$: number of units in layer $i$</li>
<li>Converged parameters: $\Theta^{(1)}$</li>
</ul>
</li></ul>
</li>
<li>Training a second task means $\Theta^{(1)}$ is frozen and new parameters $\Theta^{(2)}$ instantiated<ul>
<li>$h^{(2)}_i$ gets input from both $h^{(2)} _{i-1}$ and $h^{(1)} _{i-1}$</li>
<li>Generalizing to $K$ tasks: $h_i^{(k)} = f(W_i^{(k)} h _{i-1}^{(k)} + \sum _{j &lt; k} U_i^{(k:j)}h _{i-1}^{(j)})$</li>
</ul>
</li>
</ul>
</li>
<li>Classical pre-train + fine-tune paradigm<ul>
<li>Parameters only need to adjusted slightly to target domain</li>
<li>Assumes overlap between tasks</li>
</ul>
</li>
<li>Progressive Networks make no assumptions about relationship between tasks<ul>
<li>Allows them to reuse, modify, or ignore previously learned features via lateral connections</li>
<li>Previous connections not impacted by newly learned features in forward pass (due to lateral connection + frozen parameters)<ul><li>Ensures no catastrophic forgetting</li></ul>
</li>
</ul>
</li>
<li>Application to RL<ul>
<li>Each column of progressive network trains a seperate MDP</li>
<li>$\pi^{(k)}(a \vert s)$: Policy for kth column</li>
</ul>
</li>
<li>Adapters: Augment progressive network with non-linear lateral connections<ul>
<li>Improve initial conditioning + perform dimensionality reduction</li>
<li>Replace lateral connection with single layer MLP + multiply it by a learned scalar<ul>
<li>Adjust scales for different inputs</li>
<li>Its a projection onto a subspace</li>
<li>As $k$ grows, this ensures number of parameters growing from lateral connections is same order as $\vert \Theta^{(1)} \vert$</li>
</ul>
</li>
<li>$h_i^{(k)} = \sigma(W_i^{(k)} h _{i-1}^{(k)} + U_i^{(k:j)} \sigma(V_i^{(k:j)}\alpha _{i-1}^{(&lt; k)} h _{i-1}^{(k)}))$<ul>
<li>$V_i$: The projection matrix</li>
<li>$\alpha$: the learned scalar</li>
</ul>
</li>
</ul>
</li>
<li>Limitations of Progressive Networks<ul>
<li>Number of parameters increases with number of tasks but only fraction of capacity used<ul><li>Growth can be addressed with fewer layers or online compression during learning</li></ul>
</li>
<li>Choosing which column to use for inference requires knowledge of task label</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Transfer Analysis</strong><ul>
<li>Average Perturbation Sensitivity: Inject gaussian noise at points in architecture and see impact of perturbation on performance<ul><li>Drop in performance indicates reliance on feature map for prediction</li></ul>
</li>
<li>Average Fisher Sensistivity: Compute modified diagonal fisher ($\hat{F}$) of policy ($\pi$) network with respect to normalized activations ($\hat{h}_i^{(k)}$) at each layer<ul>
<li>Represents sensistivity of policy to small perturbations in representation</li>
<li>$\hat{F}_i^{(k)} = \mathbb{E} _{\rho(s,a)} [\frac{\partial \log \pi}{\partial \hat{h}_i^{(k)}} \frac{\partial \log \pi^T}{\partial \hat{h}_i^{(k)}}]$</li>
<li>$AFS(i,k,m) = \frac{\hat{F}_i^{(k)}(m,m)}{\sum \hat{F}_i^{(k)}(m,m)}$<ul>
<li>m: feature</li>
<li>i: layer</li>
<li>k: column</li>
<li>Often useful to consider AFS by layer by summing over features: $AFS(i,k) = \sum_m AFS(i,k,m)$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Setup</strong><ul>
<li>Use A3C</li>
<li>Use average score per epsiode as performance metric</li>
<li>Transfer Score: Relative performance of architecture compared with single column baseline</li>
</ul>
</li>
<li>
<strong>Pong Soup</strong><ul>
<li>Variants<ul>
<li>Noisy: Gaussian noise added to inputs</li>
<li>White: White background</li>
<li>Zoom: Input scaled by 75% and translated</li>
<li>V-Flip/H-Flip/VH-Flip: Input flipped vertically, horizontally, or both</li>
</ul>
</li>
<li>Fine-tuning only the output layer (other layers frozen) fails to learn task in most scenarios (negative transfer)</li>
<li>Fine-tuning all layers results in much better transfer</li>
<li>Progressive networks outperform fine-tuning methods<ul>
<li>Mean and median scores improve</li>
<li>Mean score much higher, indicating that progressive networks can exploit transfer when transfer is possible</li>
</ul>
</li>
<li>Pong to H-flip: low and mid level vision layers largely reused by fully connected layers need to be relearned</li>
<li>Pong to Zoom: low level vision reused, mid-level vision relearned</li>
<li>Pong to Noisy: Low level vision relearned (filter not sufficiently tolerant to added noise)<ul><li>Noisy to Pong: does not require low level vision to be relearned</li></ul>
</li>
</ul>
</li>
<li>
<strong>Atari Games</strong><ul>
<li>Train on 3 source games (pong, river raid, and seaquest)</li>
<li>Assess if transfer occurs to target games (Alien, Asterix, etc.)</li>
<li>Positive transfer occurs in about 8/12 tasks</li>
<li>Negative transfer in 2/12</li>
<li>With full fine-tuning, transfer only occurs in 5/12 tasks</li>
<li>For dissimilar games, there is negative transfer with fine-tuning but not with progressive nets</li>
<li>Analysis:<ul>
<li>Negative Transfer occurs when there is a dependence on convolutional layers of previous columns<ul>
<li>May be due to fast covergence to a local minima<ul><li>Inductive bias from learned tasks can both help or hinder in target task</li></ul>
</li>
<li>Exploration problem: Representation is good enough for a functional but suboptimal policy</li>
</ul>
</li>
<li>Positive Transfer occurs when earlier features augmented by new features</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Labyrinth</strong><ul>
<li>Labyrinth: 3D maze with partial obervability</li>
<li>Progressive nets offer more positive transfer than other approaches</li>
<li>Less transfer on dense reward sceanrios (easily learned)</li>
<li>On easy levels, all transfer learning approaches do well and are stable</li>
<li>On hard levels, fine tuning approach struggles but progressive nets do well</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-06-09T00:00:00+00:00">June 9, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
