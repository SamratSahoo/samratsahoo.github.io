<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Universal Value Function Approximators">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about universal value function approximators">
<meta property="og:description" content="A paper about universal value function approximators">
<link rel="canonical" href="https://samratsahoo.com/2025/06/12/uvfa">
<meta property="og:url" content="https://samratsahoo.com/2025/06/12/uvfa">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-06-12T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Universal Value Function Approximators">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-06-12T00:00:00+00:00","datePublished":"2025-06-12T00:00:00+00:00","description":"A paper about universal value function approximators","headline":"Universal Value Function Approximators","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/06/12/uvfa"},"url":"https://samratsahoo.com/2025/06/12/uvfa"}</script><title> Universal Value Function Approximators - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Universal Value Function Approximators</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://proceedings.mlr.press/v37/schaul15.pdf">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>General value function ($V_g(s)$): represent utility of state $s$ in achieving goal $g$<ul>
<li>Collection of these can learn from single stream of experience</li>
<li>Each one can generate a policy (i.e., greedy policy)</li>
<li>Can be used as a predictive representation of state</li>
</ul>
</li>
<li>Usually represented as a neural net or linear combination<ul>
<li>Usually exploits state space structure for generalization</li>
<li>Goal space usually also similar amount of structure</li>
</ul>
</li>
<li>Universal Value Function Approximator: $V(s, g, \theta)$<ul>
<li>Extends value function approximation to states and goals</li>
<li>Exploits structure across states and goals</li>
<li>Genearlizes to the set of all goals (even infinite sets!)</li>
<li>Exploits two kinds of structures between goals<ul>
<li>Structure of induced value function</li>
<li>Similarity encoded priors in goal representations</li>
</ul>
</li>
</ul>
</li>
<li>Learning UVFA is hard because we see small subset of $(s,g)$<ul>
<li>Challenging regression problem in supervised setting</li>
<li>Decompose regression<ul>
<li>View data as sparse table of values - one row + one col for each state-goal pair $\rightarrow$ Find low rank factorization into goal and state embeddings $\phi(s), \psi(g)$</li>
<li>Learn non-linear mappings between states and state embeddings + goals and goal embeddings</li>
</ul>
</li>
<li>2 Approaches to learn UVFA:<ul>
<li>Maintain finite horde of value functions and seed a table to learn $V(s, g, \theta)$</li>
<li>Bootstrap from value of UVFA at successor states</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Background</strong><ul>
<li>Assume standard MDP RL setting</li>
<li>$\gamma_g$: Pseudo-discount function<ul>
<li>State-dependent discounting</li>
<li>Soft termination (equal to 0 if state is terminal based on goal)</li>
</ul>
</li>
<li>Pseudo-discounted expected pseudo-return: $V _{g, \pi}(s) = \mathbb{E}[\sum _{t=0}^\infty R_g(s _{t+1}, a_t, s_t) \prod _{k=0}^t \gamma_g(s_k) \vert s_0 = s]$<ul><li>Action value function: $Q _{g, \pi}(s,a) = \mathbb{E} _{s’}[R_g(s, a, s’) + \gamma_g(s’) \cdot V _{g, \pi}(s’)]$</li></ul>
</li>
</ul>
</li>
<li>
<strong>Universal Value Function Approximators</strong><ul>
<li>3 possible value function approximators<ul>
<li>$\mathcal{F}: \mathcal{S} x \mathcal{G} \rightarrow \mathbb{R}$: Concatenate goal and state</li>
<li>$\phi: \mathcal{S} \rightarrow \mathbb{R}^n, \psi: \mathcal{G} \rightarrow \mathbb{R}^n, h: \mathbb{R}^n x \mathbb{R}^n \rightarrow \mathbb{R}$: Two stream architecture<ul>
<li>$\phi, \psi$ are general function approximators</li>
<li>Exploits common structures between states and goals</li>
<li>If $\mathcal{G} \subseteq \mathcal{S}$, can use shared representation for $\phi, \psi$</li>
<li>UVFA can be symmetric: $V^\ast _s(g) = V^\ast _g(s)$<ul>
<li>Partially Symmetric: Share some of the same parameters between goal and state but not identical</li>
<li>Symmetric: $\phi = \psi$</li>
<li>Small distances between representations = indicate similar states</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Supervised Learning of UVFAs</strong><ul>
<li>Approach 1: End to End Training<ul><li>Backprop on MSE: $\mathbb{E}[(V^\ast_g(s) - V(s, g; \theta))^2]$ and apply SGD</li></ul>
</li>
<li>Approach 2: Two stage training procedure based on matrix factorization<ul>
<li>Layout all values of $V^\ast_g(s)$ in table, one row for each state, one column for each goal</li>
<li>Factorize the matrix and find low rank approximation<ul>
<li>$\hat{\phi}_s$: Target embedding vector for row of $s$</li>
<li>$\hat{\psi}_g$: Target embedding vector for column of $g$</li>
</ul>
</li>
<li>Learn parameters for $\phi,\psi$ via regression toward target embeddings</li>
<li>(Optional) fine-tune with end-to-end training</li>
</ul>
</li>
<li>Factorization = Finds idealized embeddings</li>
<li>Learning = achieve idealized embeddings from states and goals</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Supervised Learning Experiments</strong><ul>
<li>Train UVFA on ground truth data</li>
<li>Evaluate using MSE on unseen state-goal pairs</li>
<li>Measure policy quality of a value function approximator as true expected discounted reward average over all start states<ul>
<li>Follow softmax policy of values (with a temperature) and compare it to optimal value function</li>
<li>Normalize policy quality such that optimal policy = 1, and uniform random policy = 0</li>
</ul>
</li>
<li>Test on LavaWorld<ul>
<li>4 rooms for states + 4 directions for actions</li>
<li>Contains deadly lava blocks when touched</li>
</ul>
</li>
<li>
<strong>Tabular Completion</strong><ul>
<li>States + goals represented as 1 hot vectors</li>
<li>$\phi, \psi$ are identity functions</li>
<li>We see how unseen state-goal pairs can be reconstructed with low rank approximation<ul>
<li>Policy quality saturates optimally even if value error continues to improve</li>
<li>Low rank embeddings can recover topological structures in LavaWorld</li>
</ul>
</li>
<li>Test reliability with respect to missing/unreliable data<ul>
<li>Reconstruct $V(s, g; \theta) = \hat{\phi}_s \cdot \hat{\psi}_g$</li>
<li>Policy quality degrades gracefully as less and less value info is provided</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Interpolation</strong><ul>
<li>We want to know if training set goals gives reasonable estimates to never seen goals</li>
<li>Interpolation does in fact occur and we get good estimates</li>
</ul>
</li>
<li>
<strong>Extrapolation</strong><ul>
<li>We can interpolate between similar goals but can we extrapolate between dissimilar goals</li>
<li>Partially symmetry allows us to transfer knowledge between $\phi$ to $\psi$</li>
<li>Doing this enables extrapolation</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Reinforcement Learning Experiments</strong><ul>
<li>In RL we don’t have no ground truth values<ul>
<li>Test via horde of value functions for targets</li>
<li>Test via Bootstrapping for targets</li>
</ul>
</li>
<li>
<strong>Generalizing from Horde</strong><ul>
<li>Seed the data matrix from the horde</li>
<li>Use two stream factorization to build a UVFA</li>
<li>Each demon learns a $Q_g(s,a)$ for its goal off-policy<ul>
<li>Build the data matrix from estimates</li>
<li>Column: Corresponds to goal</li>
<li>Row: Corresponds to time index of one transition</li>
</ul>
</li>
<li>Produce target embeddings and learn the UVFA</li>
<li>Performance determined by amount of experience and amount of computation to build UVFA</li>
<li>Challenge: Data depends on how behavior policy explores environment<ul><li>I.e., might not see much data relevant to goals of interest</li></ul>
</li>
<li>After a certain amount of data, there is a tipping point where UVFA gives reasonable estimates even for goals it wasn’t trained on</li>
</ul>
</li>
<li>
<strong>Ms Pacman</strong><ul>
<li>Trained 150 demons</li>
<li>Used 29 demons to seed data matrix</li>
<li>Tested on 5 goal locations from the remanining 121 demons</li>
<li>Showed that small horde of demons can approximate larger horde of demons</li>
</ul>
</li>
<li>
<strong>Direct Bootstrapping</strong><ul>
<li>Bootstrapping update: $Q(s_t, a_t, g) = \alpha(r_g + \gamma_g max _{a’}Q(s _{t+1}, a’, g)) + (1 - \alpha)(Q(s_t, a_t, g))$</li>
<li>Learning process can be unstable<ul>
<li>Use smaller learning rates</li>
<li>Use a better behaved $h$</li>
</ul>
</li>
<li>Use a distance based $h(a,b) = \gamma^{\vert \vert a - b\vert \vert_2}$<ul><li>Does not recover 100% policy quality but UVFA still generalizes well when trained on 25% of possible state-goal pairs</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Discussion</strong><ul>
<li>UVFAs can be used for transfer learning to new tasks with same dynamics but different goals</li>
<li>Generalized value functions can be used as features to represent state</li>
<li>UVFA can be used to generate options<ul><li>Option can act greedily with respect to $V(s, g, \theta)$</li></ul>
</li>
<li>UVFA can act as a universal option model: $V(s, g, \theta)$ can approximate discounted probability of reaching $g$ under $s$ under a policy</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-06-12T00:00:00+00:00">June 12, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
