<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Diversity is All You Need: Learning Skills without a Reward Function">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about diayn">
<meta property="og:description" content="A paper about diayn">
<link rel="canonical" href="https://samratsahoo.com/2025/06/04/diayn">
<meta property="og:url" content="https://samratsahoo.com/2025/06/04/diayn">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-06-04T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Diversity is All You Need: Learning Skills without a Reward Function">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-06-04T00:00:00+00:00","datePublished":"2025-06-04T00:00:00+00:00","description":"A paper about diayn","headline":"Diversity is All You Need: Learning Skills without a Reward Function","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/06/04/diayn"},"url":"https://samratsahoo.com/2025/06/04/diayn"}</script><title> Diversity is All You Need: Learning Skills without a Reward Function - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Diversity is All You Need: Learning Skills without a Reward Function</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1802.06070">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Agents can learn skills without supervision and use these skills to satisfy goals later<ul>
<li>Good for sparse reward environments</li>
<li>Helps with exploration</li>
<li>Primitives for hierarchial RL</li>
<li>Reduces amount of supervision necessary for a task</li>
<li>Challenging to determine what tasks an agent should learn</li>
</ul>
</li>
<li>Skill: Latent conditioned policy that alters environment in consistent way</li>
<li>Setting: reward function unknown; maximize utility of skills<ul><li>Learning objective: Each skill is distinct and skills explore large parts of state space<ul>
<li>Use discriminability between skills as objective</li>
<li>Skills should be as diverse as possible<ul><li>“Pushes” skils away from each other</li></ul>
</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Diversity is All You Need</strong><ul>
<li>Setting: Unsupervised RL paradigm where agent has unsupervised exploration stage followed by supervised stage</li>
<li>
<strong>How it Works</strong><ul>
<li>3 Ideas<ul>
<li>Skills distinguishable</li>
<li>Use states to distinguish skills, not actions</li>
<li>Learn skills that act as randomly as possible</li>
</ul>
</li>
<li>Objective:<ul>
<li>$S$: Random variable for state space</li>
<li>$A$: Random variable for action space</li>
<li>$Z \sim p(z)$: latent variable on which policy is conditioned on<ul><li>The policy is the skill</li></ul>
</li>
<li>$I(\cdot ; \cdot)$: Mutual information</li>
<li>$\mathcal{H}(\cdot)$: Shannon entropy</li>
<li>$I(S; Z)$: Maximize the mutual information between skills and states<ul>
<li>Encodes a dependency between skills and states</li>
<li>To ensure states are used to distinguish skills, condition mutual information between actions and skills on state: $I(A; Z \vert S)$</li>
</ul>
</li>
<li>Maximize: $\mathcal{F}(\theta) \triangleq I(S; Z) + \mathcal{H}(A \vert S) - I(A; Z \vert S)$<ul>
<li>$= (\mathcal{H}(Z) - \mathcal{H}(Z \vert S)) + \mathcal{H}(A \vert S) - (\mathcal{H}(A \vert S) - \mathcal{H}(A \vert S, Z))$</li>
<li>$= \mathcal{H}(Z) - \mathcal{H}(Z \vert S) + \mathcal{H}(A \vert S, Z)$<ul>
<li>First term encourages prior, $p(z)$, to have high entropy<ul><li>Fix $p(z)$ to be uniform to maximize entropy</li></ul>
</li>
<li>Second term: Easy to infer skill Z from state</li>
<li>Third term: Each skill should act as randomly as possible</li>
</ul>
</li>
<li>We can’t compute $p(z \vert s)$ directly so we approximate the posterior using a learned discriminator ($q _\phi (z \vert s)$) that gives us a lower bound for $\mathcal{F}(\theta)$<ul>
<li>$\mathcal{F}(\theta) = \mathcal{H}(A \vert S, Z) - \mathcal{H}(Z \vert S) + \mathcal{H}(Z)$</li>
<li>$= \mathcal{H}(A \vert S, Z) + \mathbb{E} _{z \sim p(z), s \sim \pi(z)}[\log p(z \vert s)] - \mathbb{E} _{z \sim p(z)}[\log p(z)]$</li>
<li>By Jensen’s inequality: $\geq \mathcal{H}(A \vert S, Z) + \mathbb{E} _{z \sim p(z), s \sim \pi(z)}[\log q _{\phi}(z \vert s) - \log p(z)]$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Implementation</strong><ul>
<li>Implemented DIAYN with SAC; learned a policy $\pi _\theta(a \vert s, z)$<ul><li>Scale entropy regularizer to balance exploration and discriminability</li></ul>
</li>
<li>Use pseduo-reward to maximize variational lower bound: $r_z(s,a) \triangleq \log q _{\phi}(z \vert s) - \log p(z)$</li>
<li>Use categorical distribution for $p(z)$</li>
<li>Agent rewarded for visiting states easy to discriinate and discriminator updated to infer skill from states visited</li>
</ul>
</li>
<li>
<strong>Stability</strong><ul>
<li>DIAYN is cooperative (unlike prior adversarial RL methods)</li>
<li>On grid worlds, optimum covergence is to partition states between skills evenly</li>
<li>In continuous domains, DIAYN is robust to random seeds</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>
<strong>Analysis of Learned Skills</strong><ul>
<li>Skills learned<ul>
<li>2D Navigation Env<ul><li>Skills learned move away from each other to remain distinguishable</li></ul>
</li>
<li>Classical Control Tasks<ul><li>Learns multiple skills for solving the task</li></ul>
</li>
<li>Continuous Control Tasks<ul><li>Learns primitive behaviors for all tasks<ul>
<li>Running forwards and backwards</li>
<li>Doing flips, falling over</li>
<li>Jumping, walking, diving</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>Skill distribution becomes increasingly diverse during training</li>
<li>DIAYN favors skills that don’t overlap but is not limited to learning skills of disjoint sets of states<ul><li>I.e., visiting same initial states and then visit different later states</li></ul>
</li>
<li>DIAYN vs VIC<ul><li>In DIAYN, we do not learn a prior $p(z)$ whereas in VIC we do<ul>
<li>Causes the more diverse skills to be sampled more frequently</li>
<li>The fixed distribution in DIAYN causes it to discover more diverse skills</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Harnessing Learned Skills</strong><ul>
<li>
<strong>Accelerating Learning with Policy Initialization</strong><ul>
<li>We can adapt skills for a desired task</li>
<li>DIAYN can be unsupervised pre-training for more sample-efficient fine tuning for a specific task</li>
<li>By taking skill with highest reward for a benchmark task and finetuning it, we speed up learning</li>
</ul>
</li>
<li>
<strong>Using Skills for Hierarchal RL</strong><ul>
<li>Introduce a meta-controller whose actions are to choose which skill to execute for $k$ steps</li>
<li>VIME significantly underperforms DIAYN<ul>
<li>DIAYN skills partition the state space</li>
<li>VIME tries to learn a policy that vists many state</li>
</ul>
</li>
<li>DIAYN outperforms TRPO, SAC, and VIME on challenging robotics environments<ul><li>Skill learning helps in exploration and sparse reward scenarios</li></ul>
</li>
<li>We can bias DIAYN to discover particular types of skills<ul><li>Condition discriminator on subset of observation space<ul><li>Maximize: $\mathbb{E}[\log q _{\phi}(z \vert f(s))]$<ul><li>$f(s)$: Could compute center of mass $\rightarrow$ Skills learn to change center of mass</li></ul>
</li></ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Imitating an Expert</strong><ul>
<li>Replaying human actions fails in stochastic environments (closed-loop control necessary)</li>
<li>Imitation learning replaces this with a differentiable policy that we can adjust</li>
<li>Given an expert trajectory, we can use discriminator to determine what skills created it<ul><li>$\hat{z} = argmax_z \prod _{s \in \tau^\ast}q _{\phi}(z \vert s_t)$</li></ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-06-04T00:00:00+00:00">June 4, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
