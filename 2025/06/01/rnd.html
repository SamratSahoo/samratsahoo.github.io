<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="Exploration by Random Network Distillation">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about random network distillation">
<meta property="og:description" content="A paper about random network distillation">
<link rel="canonical" href="https://samratsahoo.com/2025/06/01/rnd">
<meta property="og:url" content="https://samratsahoo.com/2025/06/01/rnd">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-06-01T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Exploration by Random Network Distillation">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-06-01T00:00:00+00:00","datePublished":"2025-06-01T00:00:00+00:00","description":"A paper about random network distillation","headline":"Exploration by Random Network Distillation","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/06/01/rnd"},"url":"https://samratsahoo.com/2025/06/01/rnd"}</script><title> Exploration by Random Network Distillation - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>Exploration by Random Network Distillation</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/1810.12894">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Counts, pseudo-counts, infomration gain, and prediction gain hard to scale up large numbers of parallel environments</li>
<li>Key insight: neural nets have lower prediction errors on examples similar to those that have already been trained on<ul>
<li>Use prediction error on past experience to determine novelty of new experience</li>
<li>Issue: Using prediction errors can cause agents to be attracted to transitions where the output is stochastic (i.e., noisy TV problem)<ul><li>Solution: Exploration bonus uses prediction problem where answer is deterministic</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Method</strong><ul>
<li>
<strong>Exploration Bonuses</strong><ul><li>Replace extrinsic reward, $e_t$, with extrinsic and intrinsic rewards, $r_t = e_t + i_t$<ul>
<li>$i_t$ is higher if state is novel<ul>
<li>Usually a decreasing function of visitation count</li>
<li>Uses density estimates in non-tabular settings (pseudo-counts)</li>
</ul>
</li>
<li>Alternatively $i_t$ can be prediction error related to agent’s transitions<ul>
<li>I..e, error of predicting forward dynamics, inverse dynamics, physical properties of objects agent interacts with, etc.</li>
<li>Errors decrease as agent collects experience</li>
</ul>
</li>
</ul>
</li></ul>
</li>
<li>
<strong>Random Network Distillation</strong><ul>
<li>Prediction problem is randomly generated</li>
<li>2 neural networks<ul>
<li>Fixed + randomly initialized target network which sets prediction problem<ul><li>Takes an observation to an embedding $f: \mathcal{O} \rightarrow \mathbb{R}^k$</li></ul>
</li>
<li>Predictor network trained on agent data: $\hat{f}: \mathcal{O} \rightarrow \mathbb{R}^k$<ul><li>Minimizes expected MSE with respect to parameters using gradient descent<ul><li>$\vert \vert \hat{f}(x; \theta) - f(x) \vert \vert^2$</li></ul>
</li></ul>
</li>
<li>Distills random network onto trained one</li>
<li>Error is higher for novel states, dissimilar to the ones predictor has been trained on</li>
</ul>
</li>
<li>
<strong>Sources of Prediction Errors</strong><ul>
<li>Amount of training data: Higher prediction error where there are few similar examples<ul><li>Desirable error for novelty</li></ul>
</li>
<li>Stochasticity: Target function is stochastic + stochastic transitions in forward dynamics models<ul><li>We can specify model to be determinstic (this is a non-issue)</li></ul>
</li>
<li>Model Misspecification: Missing information or model capacity too low</li>
<li>Learning Dynamics: Predictor fails to approximate target well</li>
</ul>
</li>
<li>
<strong>Relation to Uncertainty Quantification</strong><ul>
<li>Let $\mathcal{F}$ be distribution over functions $g _{\theta} = f _{\theta} + f _{\theta^\ast}$<ul>
<li>$\theta^\ast$ drawn from $p(\theta^\ast)$ a prior over the parameters mapping $f _{\theta^\ast}$</li>
<li>$\theta = argmin _{\theta} \mathbb{E} _{(x_i, y_i) \sim \mathcal{D}}\vert \vert f _{\theta}(x_i) + f _{\theta^\ast}(x_i) - y_i \vert \vert^2 + \mathcal{R}(\theta)$<ul>
<li>$\mathcal{R}(\theta)$: regularization from prior</li>
<li>Minimizes expected prediction error</li>
</ul>
</li>
</ul>
</li>
<li>We are trying to adjust $f _\theta$ to match a random function $f _{\theta^\ast}$ sampled from a prior<ul>
<li>If we view an ensemble of networks as samples from a posterior, then minimizing the loss corresponds to approximating the posterior</li>
<li>Distillation error is just when $y_i = 0$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Combining Intrinsic and Extrinsic Returns</strong><ul>
<li>Treating problem as non-episodic resulted in better exploration (return not truncated at game over)<ul><li>Intrinsic return should be related to all novel states across all episodes<ul>
<li>This could leak information to agents about task</li>
<li>Can be exploited by continuously resetting the game where it finds a reward at the beginning</li>
</ul>
</li></ul>
</li>
<li>Decompose reward into $R = R_E + R_I$<ul><li>Fit two values heads and get combined value function through sum: $V = V_E + V_I$</li></ul>
</li>
</ul>
</li>
<li>
<strong>Reward and Observation Normalization</strong><ul>
<li>Reward Normalization:<ul>
<li>Scale of reward can vary between environments and through time</li>
<li>Noramlize intrinsic reward by dividing by running estimate of standard deviation of intrinsic rewards</li>
</ul>
</li>
<li>Observation Normalization:<ul>
<li>In RND, paramters are frozen + cannot adjust to scale of datasets</li>
<li>No normalization = embedding variance = low + carries little info about inputs</li>
<li>Normalize by whitening each dimension, subtract running mean, divide by standard deviation, and clip observations between -5 and 5</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Experiments</strong><ul>
<li>Experiments run on Montezuma’s Revenge</li>
<li>
<strong>Pure Exploration</strong><ul>
<li>Comparing episodic and non-episodic exploration, non-episodic has better exploration performance</li>
<li>Mean epsiodic return: Agent not optimizing it directly but as it explores more rooms, it goes up anyways</li>
</ul>
</li>
<li>
<strong>Combining Episodic and Non-Episodic Returns</strong><ul>
<li>Non-episodic reward stream increases the number of rooms explored<ul><li>Effect less dramatic than pure exploration because extrinsic reward behaviors preserves useful behaviors</li></ul>
</li>
<li>Two value heads didn’t show benefit over single in episodic setting</li>
</ul>
</li>
<li>
<strong>Discount Factors</strong><ul>
<li>Extrinsic discount factor: Increasing this from $0.99 \rightarrow 0.999$ improves performance</li>
<li>Intrinsic discount factor: Increasing this from $0.99 \rightarrow 0.999$ hurts performance</li>
</ul>
</li>
<li>
<strong>Scaling Up Training</strong><ul>
<li>To hold the rate at which intrinsic reward decreases over time across experimetns with different numbers of parallel environments, downsample batch size to match 32 parallel environments</li>
<li>More environments = larger policy batch size but constant predictor network batch size<ul><li>Policy needs to quickly learn to find and exploit rewards since they disappear</li></ul>
</li>
</ul>
</li>
<li>
<strong>Recurrence</strong><ul>
<li>Montezuma’s Revenge is a partially observable environment</li>
<li>With a larger discount factor, recurrent policies performed better than CNNs<ul><li>Across multiple games, recurrent policies do better more frequently than CNNs</li></ul>
</li>
</ul>
</li>
<li>
<strong>Comparison to Baselines</strong><ul>
<li>Compare RND to PPO on various games<ul>
<li>Gravitar:<ul>
<li>RND does not consistently exceed PPO performance</li>
<li>Both exceed average human performance with RNN policy and SOTA</li>
</ul>
</li>
<li>Montezuma’s Revenge + Venture: RND outperforms PPO, SOTA, and average human performance</li>
<li>Pitfall: Both alogirthms fail to find positive rewards</li>
<li>PrivateEye: RND exceeds PPO</li>
<li>Solaris: RND is comparable to PPO</li>
</ul>
</li>
<li>Exploration bonus based on forward dynamics error:<ul>
<li>Change RND loss so that predictor predicts random features of next observation given current observation and action</li>
<li>Performs signficiantly worse than RND on Montezuma, PrivateEye, Solaris and similarly on Venture, Pitfall, and Gravitar<ul><li>Oscillates between two rooms in montezuma, causing high prediction error (due to non-determinism)<ul><li>Similar behavior in PrivateEye and Pittfall</li></ul>
</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Qualitative Analysis: Dancing with Skulls</strong><ul><li>Once an agent obtains all extrinsic rewards it knows, it keeps interacting with dangerous objects<ul><li>Dangerous states are difficult to achieve and hence rarely in past experience</li></ul>
</li></ul>
</li>
</ul>
</li>
</ul>
<span class="meta"><time datetime="2025-06-01T00:00:00+00:00">June 1, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
