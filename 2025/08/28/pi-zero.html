<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="A paper about pi0">
<meta property="og:description" content="A paper about pi0">
<link rel="canonical" href="https://samratsahoo.com/2025/08/28/pi-zero">
<meta property="og:url" content="https://samratsahoo.com/2025/08/28/pi-zero">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-08-28T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2025-08-28T00:00:00+00:00","datePublished":"2025-08-28T00:00:00+00:00","description":"A paper about pi0","headline":"$\\pi_0$: A Vision-Language-Action Flow Model for General Robot Control","mainEntityOfPage":{"@type":"WebPage","@id":"https://samratsahoo.com/2025/08/28/pi-zero"},"url":"https://samratsahoo.com/2025/08/28/pi-zero"}</script><title> $\pi_0$: A Vision-Language-Action Flow Model for General Robot Control - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.webp">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="https://samratsahoo.com/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global","scale":1.0,"minScale":0.5,"mtextInheritFont":true,"merrorInheritFont":true,"mathmlSpacing":false,"skipAttributes":{},"exFactor":0.5},"chtml":{"scale":1.0,"minScale":0.5,"matchFontHeight":true,"mtextFont":"serif","linebreaks":{"automatic":false}}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">about</a></li>
<li><a href="/reading" class="active">reading</a></li>
<li><a href="/research">research</a></li>
<li><a href="/writing">writing</a></li>
<li><a href="/search">search</a></li>
</ul></nav></header><section class="post"><h2>$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control</h2>
<ul>
<li>
<strong>Resources</strong><ul><li>
<a href="https://arxiv.org/abs/2410.24164v1">Paper</a> <br><br>
</li></ul>
</li>
<li>
<strong>Introduction</strong><ul>
<li>Human intelligence is significantly more versatile than machine intelligence<ul><li>Solve diverse tasks, respond to envrionment, language, etc.</li></ul>
</li>
<li>LLMs / VLMs aren’t situated in the physical world<ul><li>We need to train on physically situated data</li></ul>
</li>
<li>We train on highly diverse datasets and then fine tune to solve data scarcity issues<ul><li>For robotics, train on robot, non-robot, and other sources of data before fine-tuning on robot data</li></ul>
</li>
<li>3 Bottlenecks<ul>
<li>Pre-training benefits only available at scale</li>
<li>Model Architecture needs to injest diverse data and intricate behaviors</li>
<li>Need the right training recipe</li>
</ul>
</li>
<li>$\pi_0$ Architecture<ul>
<li>Uses a pre-trained VLM to inherit internet-scale knowledge</li>
<li>Train it further to incorporate robot actions and turn it into a vision-language-action (VLA) model</li>
<li>Uses cross-embodiment training: data from many robot types combined into 1 model</li>
<li>Use action chunking + flow matching for continuous action distributions</li>
<li>Augment VLM outputs with flow-based action expert</li>
</ul>
</li>
<li>$\pi_0$ Training<ul>
<li>Follows pre-training / post-training paradigm</li>
<li>Trained on high and low quality data to teach it to 1) act robustly and 2) recover from mistakes</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Overview</strong><ul>
<li>Pre-training mixture is a weighted combination of the different dexterous manipulation datasets from different robots<ul>
<li>Also uses language labels (i.e., task name, segment annotation, etc.)</li>
<li>Purpose: Train base model with broad capabilities + generalization but not necessarily high performance</li>
</ul>
</li>
<li>Post training procedure for downstream tasks</li>
<li>Model based on PaliGemma VLM<ul><li>Add action outputs that use flow matching to PaliGemma</li></ul>
</li>
</ul>
</li>
<li>
<strong>The $\pi_0$ Model</strong><ul>
<li>Consists of a language model backbone</li>
<li>Image encoders embed image into language tokens embedding space</li>
<li>Augment backbone with proprioceptive state + robot actions</li>
<li>Uses conditional flow matching to model a continuous action distribution</li>
<li>Architecture based on transfusion<ul>
<li>Trains transformer using multiple objectives</li>
<li>Continuous output tokens supervised via flow matching loss</li>
<li>Discrete output tokens supervised via cross entropy loss</li>
</ul>
</li>
<li>Seperate weights for robotics state + action tokens (action expert)</li>
<li>We want to model $P(A_t \vert o_t)$<ul>
<li>$A_t$ is a future action chunk (sequence of future actions)</li>
<li>$o_t$: observation consisting of images, language tokens, joint angles<ul><li>Images and joint angles encoded and then projected into language space</li></ul>
</li>
<li>For each action in $A_t$, we have an action token that is fed through the action expert<ul>
<li>Supervised by conditional flow matching loss: $L^T(\theta) = \mathbb{E} _{p(A_t \vert o_t), q(A^\tau_t \vert A_t)} \vert \vert v _{\theta} (A^\tau_t, o_t) - u(A^\tau_t \vert A_t)\vert \vert^2$<ul>
<li>$t$: Robot time step</li>
<li>$\tau$: Flow matching timestep</li>
<li>$q_t$: Sampled from a normal distribution $\epsilon \sim \mathcal{N}(0, I)$<ul><li>Noisy Action: $A^\tau_t = \tau A_t + (1 - \tau)\epsilon$</li></ul>
</li>
<li>$v _{\theta} (A^\tau_t, o_t)$ trained to match denoising vector field (negative derivative of the noisy action) $u(A^\tau_t \vert A_t) = \epsilon - A_t$</li>
<li>Action expert uses bidirectional attention mask (all tokens attend to each other)</li>
</ul>
</li>
<li>During inference, we integrate the learned vector field by integrating it from $\tau = 0$ to $\tau = 1$<ul><li>$A^{\tau + \delta}_t = A^{\tau}_t + \delta v _{\theta} (A^\tau_t, o_t)$<ul><li>$\delta$ is the integration step size</li></ul>
</li></ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Use PaliGemma as backbone and add 300M parameters for the action expert<ul><li>They also trained a smaller version that didn’t use a pre-trained VLM</li></ul>
</li>
</ul>
</li>
<li>
<strong>Data Collection and Training Recipe</strong><ul>
<li>
<strong>Pre-Training and Post-Training</strong><ul>
<li>Open source datasets (OXE, Bridge V2, and DROID)<ul><li>Robots + tasks have 1 - 2 cameras</li></ul>
</li>
<li>Custom datasets<ul><li>Consists of 68 tasks but each task has many behaviors</li></ul>
</li>
<li>Datasets are imbalanced so each dataset is weighed by $n^{0.43}$ where $n$ is the number of samples</li>
<li>Configuration and action vectors have dimensionality of largest robot in dataset<ul><li>Lower DoF robots have these zero-padded</li></ul>
</li>
<li>For robots with fewer than 3 images, missing image slots masked</li>
<li>Post-training consists of fine-tuning on task-specific dataset<ul><li>5 hours of data for simple tasks and 100 hours of data for complex tasks</li></ul>
</li>
</ul>
</li>
<li>
<strong>Language and High Level Policies</strong><ul><li>Use high level VLM to decompose task into immediate subtasks</li></ul>
</li>
<li>
<strong>Robot System Details</strong><ul><li>7 Robot Configurations (See paper for details)<ul>
<li>UR5e</li>
<li>Bimanual UR5e</li>
<li>Franka</li>
<li>Bimanual Trossen</li>
<li>Bimanual ARX &amp; bimanual AgileX</li>
<li>Mobile Trossen &amp; mobile ARX</li>
<li>Mobile Fibocom</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>
<strong>Experimental Evaluation</strong><ul>
<li>How well does $\pi_0$ perform after pre-training on tasks in the pre-training data</li>
<li>How well does $\pi_0$ follow language commands</li>
<li>How does $\pi_0$ compare to prior methods for dexterous manipulation</li>
<li>Can $\pi_0$ be adapted for complex multi-stage tasks</li>
<li>
<strong>Evaluating the Base Model</strong><ul>
<li>Model evaluated on shirt folding, bussing, grocery bagging, and toast out of a toaster</li>
<li>Compare OpenVLA (7B Model) trained on full mixture of data, Octo (93M Model), OpenVLA without Cross embodiment training, and smaller version of $\pi_0$</li>
<li>$\pi_0$ obtains best results on out-of-box tasks</li>
<li>$\pi_0$ small also outperforms Octa and OpenVLA</li>
<li>OpenVLA strugglest because it’s autoregressive discretization architecture doesn’t support chunks</li>
<li>Octo doesn’t support chunks and also has limited representational capacity</li>
</ul>
</li>
<li>
<strong>Following Language Commands</strong><ul>
<li>Compare $\pi_0$ to $\pi_0$-small</li>
<li>Measures how much VLM pretraining boosts ability to follow language instructions</li>
<li>Two versions tested:<ul>
<li>Flat: Directly command model with task description without intermediate language commands</li>
<li>Human: Include intermediate commands</li>
</ul>
</li>
<li>$\pi_0$ is significantly better than $\pi_0$-small for both human and flat, indicating improvement from pretrained VLM</li>
</ul>
</li>
<li>
<strong>Learning New Dexterous Tasks</strong><ul>
<li>Tasks:<ul>
<li>UR5e Stack Bowls</li>
<li>Towel Folding</li>
<li>Tupperware in microwave</li>
<li>Paper towel replacement</li>
<li>Franka items in drawer</li>
</ul>
</li>
<li>Compare to OpenVLA and Octo</li>
<li>$\pi_0$ generally outperforms other methods</li>
<li>Strongest prior models are trained completely from scratch on target tasks; leveraging pretraining presents a challenge for prior approaches</li>
<li>Pre-training with $\pi_0$ yields better results than non-pretrained</li>
</ul>
</li>
<li>
<strong>Mastering Complex Multi-Stage Tasks</strong><ul>
<li>Tasks:<ul>
<li>Laundry Folding</li>
<li>Mobile Laundry</li>
<li>Dryer Unloading</li>
<li>Table Bussing</li>
<li>Box Building</li>
<li>To-go Box</li>
<li>Packing Eggs</li>
</ul>
</li>
<li>Because there are very difficult tasks, only $\pi_0$ can solve them</li>
<li>Compare pre-training + fine-tuning, pre-training only, and training on fine-tuning data only</li>
<li>Pre-training + fine-tuning usually yields best performance</li>
<li>Pre-training causes jump in performance</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Discussions, Limitations, and Future Work</strong><ul><li>Limitations:<ul>
<li>No comprehensive understanding of how pre-training datasets should be composed</li>
<li>Not all tasks in evaluation work reliably</li>
<li>Unsure how much positive transfer there is from combining diverse data from different tasks and robots</li>
</ul>
</li></ul>
</li>
</ul>
<span class="meta"><time datetime="2025-08-28T00:00:00+00:00">August 28, 2025</time> · <a href="/tags/research">research</a></span></section></main></body>
</html>
