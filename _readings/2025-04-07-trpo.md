---
layout: post
title: Trust Region Policy Optimization
description: A paper about trust region constrained policy gradients
summary: A paper about trust region constrained policy gradients
category: reading
tags: [research]
---

* **Resources**
    - [Paper](https://arxiv.org/abs/1502.05477)
<br><br/>

* **Introduction**  
  * 3 categories of policy optimization  
    * Policy Iteration  
    * Policy Gradient  
    * Derivative Free  
      * Treat return as black box optimized by policy parameters  
      * Usually preferred because simple to implement \+ good results  
  * Gradient-based optimization has been good for supervised learning but less so for RL  
  * Minimizing surrogate objective \= guarantees policy improvement  
  * TRPO Variants  
    * Single Path: Applied in model-free setting  
    * Vine: Requires restoring system to specific state → only possible in simulation  
  * TRPO is scalable to systems with thousands or millions of parameters  
* **Preliminaries**  
  * Use standard definitions of policy, state and action value functions, and advantage functions.  
  * Expected return of another policy $\\tilde{\\pi}$ in terms of advantage over $\\pi$, accumulated over timesteps   
    * $\\eta(\\tilde{\\pi}) \= \\eta(\\pi) \+ \\mathbb{E}\_{s\_0, a\_0, \\dots \\sim \\tilde{\\pi}}\[\\sum\_{t=0}^\\infty \\gamma^t A\_\\pi(s\_t, a\_t)\]$  
    * Equivalent to $\\eta(\\pi) \+ \\sum\_s \\rho\_{\\tilde{\\pi}}(s) \\sum\_a \\tilde{\\pi}(a\vert s)A\_\\pi(s,a)$  
      * Implies any policy update that has a non-negative expected advantage at every state is guaranteed to increase or leave it constant  
      * Classical tabular RL follows this because it uses argmax  
    * With function approximation, some states have negative expected advantage. Also dependency on $\\rho\_{\\tilde{\\pi}}(s)$ makes it hard to optimize  
      * Instead use: $L\_\\pi(\\tilde{\\pi}) \= \\eta(\\pi) \+ \\sum\_s \\rho\_\\pi(s) \\sum\_a \\tilde{\\pi}(a\vert s)A\_\\pi(s, a)$  
        * Where this uses current instead of next policy visitation frequency (easier to optimize)  
    * If $\\pi\_\\theta(a\vert s)$ is differentiable  
      * $L\_{\\pi\\theta\_{old}} \= \\eta(\\pi\_{\\theta\_0})$   
      * $\\nabla\_{\\theta\_0}L\_{\\pi\\theta\_{old}} \= \\nabla\_{\\theta\_0}\\eta(\\pi\_{\\theta\_0})$   
        * For a small step $\\pi\_{\\theta\_0} \\rightarrow \\tilde{\\pi}$ that improves $L\_{\\pi\\theta\_{old}}$ will also improve $\\eta$  
          * We don’t know how small of a step to take, though  
    * Conservative Policy Iterations  
      * Provides lower bounds on improvement of $\\eta$  
      * Let $\\pi' \= argmax L\_{\\pi\_{old}}(\\pi')$  
      * New Policy: $\\pi\_{new}(a\vert s) \= (1-\\alpha)\\pi\_{old}(a\vert s) \+ \\alpha \\pi'(a\vert s)$  
      * Lower Bound Improvement: $\\eta(\\pi\_{new}) \\geq L\_{\\pi\_{old}}(\\pi\_{new}) \- \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2$  
        * $\\epsilon \= max\_s \vert\\mathbb{E}\_{a \\sim \\pi'(a\vert s)}\[A\_\\pi(s,a)\]\vert$  
* **Monotonic Improvement Guarantee for General Stochastic Policies**  
  * Conservative policy iteration uses a mixture policy (between old and argmax policy)  
    * We can extend this to general stochastic policies with a distance measure between $\\pi$ and $\\tilde{\\pi}$ and changing $\\epsilon$  
    * Uses total variation divergence as distance measure: $D\_{max\_{TV}}(\\pi, \\tilde{\\pi}) \= max\_s D\_{TV}(\\pi(\\cdot \vert s) \vert\vert \\tilde{\\pi}(\\cdot \vert s))$  
      * $D\_{TV}(p \vert\vert q) \= \\frac{1}{2}\\sum\_i \vert p\_i \- q\_i\vert$  
    * Using total variation divergence, we get the following bound:  
      * $\\eta(\\pi\_{new}) \\geq L\_{\\pi\_{old}}(\\pi\_{new}) \- \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2$  
        * $\\epsilon \= max\_{s,a}\vert A\_\\pi(s,a)\vert$  
    * We can also use KL divergence because we know $D\_{TV}(p\vert\vert q)^2 \\leq D\_{KL}(p\vert\vert q)$  
      * $\\eta(\\tilde{\\pi}) \\geq L\_\\pi(\\tilde{\\pi}) \- CD\_{max\_{KL}}(\\pi, \\tilde{\\pi})$ where $C \= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}$  
      * Guaranteed to produce monotonically non-decreasing policies when we maximize $L\_\\pi(\\tilde{\\pi}) \- CD\_{max\_{KL}}(\\pi, \\tilde{\\pi})$  
  * This is a minimization-maximization algorithm  
  * TRPO uses a KL divergence constraint instead of a penalty to allow large updates  
* **Optimization of Parameterized Policies**  
  * If we follow the maximation objective: $maximize\_\\theta \[L\_{\\theta\_{old}}(\\theta) \- CD^{max}\_{KL}(\\theta\_{old}, \\theta)\]$  
    * Guaranteed to improve true objective $\\eta$  
    * However step sizes would be very small  
  * Instead we can use a KL divergence (trust region) constraint between new and old policies  
    * $maximize L\_{\\theta\_{old}}(\\theta)$ subject to $D\_{KL}^{max}(\\theta\_{old}, \\theta) \\leq \\delta$  
    * In practice, we use heuristic approximation: $\\bar{D}^\\rho\_{KL}(\\theta\_1, \\theta\_2) \= \\mathbb{E}\_{s \\sim \\rho}\[D\_{KL}(\\pi\_{\\theta\_1}) (\\cdot \vert s)\vert\vert \\pi\_{\\theta\_1})(\\cdot \vert s))\]$  
* **Sample-Based Estimation of the Objective and Constraint**  
  * We need to approximate the objective and constraint using monte carlo simulations  
  * Original optimization problem: $maximize\_\\theta \\sum\_s \\rho\_{\\theta\_{old}}(s)\\sum\_a \\pi\_\\theta(a\vert s)A\_{\\theta\_{old}}(s,a)$ subject to $\\bar{D}\_{KL}^{\\rho\_{\\theta\_{old}}}(\\theta\_{old}, \\theta) \\leq \\delta$  
    * Replace $\\sum\_s \\rho\_{\\theta\_{old}}(s)$ with expectation   
      * Replace expectation with sample averages  
    * Replace advantages with $Q\_{old}$ values  
      * Replace with empirical estimate  
    * Replace sum over actions with importance sampling ratio $\\frac{\\pi\_theta(a\vert s\_n)}{q(a\vert s\_n)}$ where $q$ is the sampling distribution  
    * Two sampling schemes: single path and vine  
  * **Single Path**  
    * Collect sequence of states by sampling $s\_0 \\sim \\rho\_0$ and then simulating a policy  
    * $q(a\vert s) \= \\pi\_{\\theta\_{old}}$  
    * $Q\_{old}$ computed at each state-action pair by taking the discounted sum of future rewards  
  * **Vine**  
    * Sample $s\_0 \\sim \\rho\_0$ and then simulating a policy, $\\pi\_{\\theta\_i}$   
    * Generate a number of trajectories  
    * Choose a subset N of states known as the rollout set  
    * For each state in the rollout set, sample K actions from$q(\\cdot \vert s\_n)$ → $q(\\cdot \vert s\_n) \= \\pi(\\cdot \vert s\_n)$ works well in practice  
    * Estimate $\\hat{Q}\_{\\theta\_i}(s\_n, a\_{n,k})$ by doing a small rollout  
    * For finite action space  
      * $L\_n(\\theta) \= \\sum\_{k=1}^K \\pi\_\\theta(a\_k \vert s\_n)\\hat{Q}(s\_n, a\_k)$  
    * For continuous action space, we can estimate using importance sampling and normalize it:  
      * $L\_n(\\theta) \= \\frac{\\sum\_{k=1}^K\\frac{\\pi\_\\theta(a\_{n,k}\vert s\_n)}{\\pi\_{\\theta\_{old}}(a\_{n,k}\vert s\_n)}\\hat{Q}(s\_n, a\_k))}{\\sum\_{k=1}^K\\frac{\\pi\_\\theta(a\_{n,k}\vert s\_n)}{\\pi\_{\\theta\_{old}}(a\_{n,k}\vert s\_n)}}$  
      * Removes need for baselines (gradient unchanged)  
    * Vine results in much lower variance than single path given same number of Q-value samples  
      * But requires more calls to simulator  
* **Practical Algorithm**  
  * Use single path or vine to collect set of state-action pairs \+ Q value estimates  
  * Construct estimated objective \+ constraint  
  * Approximately solve to update parameters  
    * Use conjugate gradient with line search   
    * Compute fisher information matrix by analytically computing hessian on KL divergence  
      * Removes need to store dense hessian or policy gradients for batch of trajectories  
* **Connections with Prior Work**  
  * Natural policy gradient sets the step size as a hyperparameter  
    * TRPO forces the constraint size at each step  
      * Improves the performance significantly   
  * We can get the standard policy gradient update using an L2 constraint instead of KL  
  * Relative entropy policy search constrains $p(s,a)$ whereas TRPO constrains $p(s\vert A)$  
  * KL Divergence has been used to ensure policy does not stray away from regions where dynamics model is valid  
* **Experiments**  
  * **Simulated Robotic Locomotion**  
    * Tested on swimmer, hopper, and walker  
    * TRPO had the best solutions on all problems relative to natural policy gradient  
    * KL divergence way better way to choose step sizes instead of a fixed penalty  
    * TRPO learned all policies with simple rewards \+ minimal prior knowledge  
  * **Playing Games from Images**  
    * Only outperformed previous methods on some games but achieved reasonable scores 	  
    * TRPO was not made for these tasks, but its performance demonstrates generalization